{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Experiment with std1pos -- 2 files</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from models import HyperPars as AlignmentHyperPars\n",
    "from discretisation.adduct_cluster import AdductCluster, Peak, Possible\n",
    "from discretisation import utils\n",
    "from discretisation.preprocessing import FileLoader\n",
    "from shared_bin_matching import SharedBinMatching as Aligner\n",
    "from ground_truth import GroundTruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Experiment Parameters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dir = '/home/joewandy/git/metabolomics_tools/alignment/input/std1_csv_full_old'\n",
    "database_file = None\n",
    "transformation_file = '/home/joewandy/git/metabolomics_tools/alignment/pos_transformations_full.yml'\n",
    "gt_file = '/home/joewandy/git/metabolomics_tools/alignment/input/std1_csv_full_old/ground_truth/ground_truth.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters across_file_mass_tol=10, across_file_rt_tol=120, alpha_mass=1.0, beta=0.1, dp_alpha=1000.0, mass_clustering_n_iterations=200, rt_clustering_burnin=200, rt_clustering_nsamps=400, t=0.0, within_file_mass_tol=5, within_file_rt_tol=30\n"
     ]
    }
   ],
   "source": [
    "hp = AlignmentHyperPars()    \n",
    "hp.within_file_mass_tol = 5\n",
    "hp.within_file_rt_tol = 30\n",
    "hp.across_file_mass_tol = 10\n",
    "hp.across_file_rt_tol = 120\n",
    "hp.alpha_mass = 1.0\n",
    "hp.dp_alpha = 1000.0\n",
    "hp.t = 0.0\n",
    "hp.mass_clustering_n_iterations = 200\n",
    "hp.rt_clustering_nsamps = 400\n",
    "hp.rt_clustering_burnin = 200\n",
    "\n",
    "print hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluation_method = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the std1pos files that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999 features read from std1-file1.txt\n",
      "4986 features read from std1-file2.txt\n",
      "6836 features read from std1-file3.txt\n",
      "9752 features read from std1-file4.txt\n",
      "7076 features read from std1-file5.txt\n",
      "4146 features read from std1-file6.txt\n",
      "6319 features read from std1-file7.txt\n",
      "4101 features read from std1-file8.txt\n",
      "5485 features read from std1-file9.txt\n",
      "5034 features read from std1-file10.txt\n",
      "5317 features read from std1-file11.txt\n"
     ]
    }
   ],
   "source": [
    "loader = FileLoader()        \n",
    "data_list = loader.load_model_input(input_dir, synthetic=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_list = []\n",
    "for mass_tol in range(3, 10, 3):\n",
    "    for rt_tol in range(10, 101, 10):\n",
    "        param_list.append((mass_tol, rt_tol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_files = 2\n",
    "n_iter = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create the first-stage clustering -- Gibbs, mh_biggest=True</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters across_file_mass_tol=3.0, across_file_rt_tol=60.0, alpha_mass=1.0, beta=0.1, dp_alpha=1000.0, mass_clustering_n_iterations=200, rt_clustering_burnin=200, rt_clustering_nsamps=400, t=0.0, within_file_mass_tol=5, within_file_rt_tol=30\n",
      "[M+ACN+H, 2M+H, 2M+Na, M+ACN+2H, M+2ACN+2H, M+2H, M+H, M+H+NH4, M+2ACN+H, M+NH4, 2M+ACN+H, M+CH3OH+H, M+ACN+Na, M+Na]\n",
      "First stage clustering -- within_file_mass_tol=5.00, within_file_rt_tol=30.00, alpha=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 out of  11 | elapsed:  2.3min remaining: 22.7min\n",
      "[Parallel(n_jobs=4)]: Done   3 out of  11 | elapsed:  4.3min remaining: 11.4min\n",
      "[Parallel(n_jobs=4)]: Done   5 out of  11 | elapsed:  6.8min remaining:  8.2min\n",
      "[Parallel(n_jobs=4)]: Done   7 out of  11 | elapsed:  8.7min remaining:  4.9min\n",
      "[Parallel(n_jobs=4)]: Done   9 out of  11 | elapsed:  8.7min remaining:  1.9min\n",
      "[Parallel(n_jobs=4)]: Done  11 out of  11 | elapsed:  9.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[M+ACN+H, 2M+H, 2M+Na, M+ACN+2H, M+2ACN+2H, M+2H, M+H, M+H+NH4, M+2ACN+H, M+NH4, 2M+ACN+H, M+CH3OH+H, M+ACN+Na, M+Na]\n",
      "[M+ACN+H, 2M+H, 2M+Na, M+ACN+2H, M+2ACN+2H, M+2H, M+H, M+H+NH4, M+2ACN+H, M+NH4, 2M+ACN+H, M+CH3OH+H, M+ACN+Na, M+Na]\n",
      "[M+ACN+H, 2M+H, 2M+Na, M+ACN+2H, M+2ACN+2H, M+2H, M+H, M+H+NH4, M+2ACN+H, M+NH4, 2M+ACN+H, M+CH3OH+H, M+ACN+Na, M+Na]\n",
      "[M+ACN+H, 2M+H, 2M+Na, M+ACN+2H, M+2ACN+2H, M+2H, M+H, M+H+NH4, M+2ACN+H, M+NH4, 2M+ACN+H, M+CH3OH+H, M+ACN+Na, M+Na]\n",
      "Created 4999 clustersCreated 4986 clustersCreated 6836 clustersCreated 9752 clusters\n",
      "\n",
      "\n",
      "\n",
      "Binning with mh_biggest = True\n",
      "Binning with mh_biggest = True\n",
      "Binning with mh_biggest = True\n",
      "Binning with mh_biggest = True\n",
      "Assigning possible transformations 0/4999\n",
      "Assigning possible transformations 0/4986\n",
      "Assigning possible transformations 0/6836\n",
      "Assigning possible transformations 0/9752\n",
      "Assigning possible transformations 500/4999\n",
      "Assigning possible transformations 500/4986\n",
      "Assigning possible transformations 500/6836\n",
      "Assigning possible transformations 500/9752\n",
      "Assigning possible transformations 1000/4999\n",
      "Assigning possible transformations 1000/4986\n",
      "Assigning possible transformations 1000/6836\n",
      "Assigning possible transformations 1000/9752\n",
      "Assigning possible transformations 1500/4999\n",
      "Assigning possible transformations 1500/4986\n",
      "Assigning possible transformations 1500/6836\n",
      "Assigning possible transformations 1500/9752\n",
      "Assigning possible transformations 2000/4999\n",
      "Assigning possible transformations 2000/4986\n",
      "Assigning possible transformations 2000/6836\n",
      "Assigning possible transformations 2000/9752\n",
      "Assigning possible transformations 2500/4999\n",
      "Assigning possible transformations 2500/4986\n",
      "Assigning possible transformations 2500/6836\n",
      "Assigning possible transformations 2500/9752\n",
      "Assigning possible transformations 3000/4999\n",
      "Assigning possible transformations 3000/4986\n",
      "Assigning possible transformations 3000/6836\n",
      "Assigning possible transformations 3000/9752\n",
      "Assigning possible transformations 3500/4999\n",
      "Assigning possible transformations 3500/4986\n",
      "Assigning possible transformations 3500/6836\n",
      "Assigning possible transformations 3500/9752\n",
      "Assigning possible transformations 4000/4999\n",
      "Assigning possible transformations 4000/4986\n",
      "Assigning possible transformations 4000/6836\n",
      "Assigning possible transformations 4000/9752\n",
      "Assigning possible transformations 4500/4999\n",
      "Assigning possible transformations 4500/4986\n",
      "Assigning possible transformations 4500/6836\n",
      "Assigning possible transformations 4500/9752\n",
      "1096 peaks to be re-sampled in stage 1\n",
      "1083 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 5000/6836\n",
      "Assigning possible transformations 5000/9752\n",
      "[M+ACN+H, 2M+H, 2M+Na, M+ACN+2H, M+2ACN+2H, M+2H, M+H, M+H+NH4, M+2ACN+H, M+NH4, 2M+ACN+H, M+CH3OH+H, M+ACN+Na, M+Na]\n",
      "[M+ACN+H, 2M+H, 2M+Na, M+ACN+2H, M+2ACN+2H, M+2H, M+H, M+H+NH4, M+2ACN+H, M+NH4, 2M+ACN+H, M+CH3OH+H, M+ACN+Na, M+Na]\n",
      "Assigning possible transformations 5500/6836\n",
      "Assigning possible transformations 5500/9752\n",
      "Created 7076 clustersCreated 4146 clusters\n",
      "\n",
      "Assigning possible transformations 6000/6836\n",
      "Assigning possible transformations 6000/9752\n",
      "Binning with mh_biggest = True\n",
      "Binning with mh_biggest = True\n",
      "Assigning possible transformations 0/7076\n",
      "Assigning possible transformations 0/4146\n",
      "Assigning possible transformations 6500/6836\n",
      "Assigning possible transformations 6500/9752\n",
      "Assigning possible transformations 500/7076\n",
      "Assigning possible transformations 500/4146\n",
      "1215 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 7000/9752\n",
      "Assigning possible transformations 1000/7076\n",
      "Assigning possible transformations 1000/4146\n",
      "[M+ACN+H, 2M+H, 2M+Na, M+ACN+2H, M+2ACN+2H, M+2H, M+H, M+H+NH4, M+2ACN+H, M+NH4, 2M+ACN+H, M+CH3OH+H, M+ACN+Na, M+Na]\n",
      "Assigning possible transformations 7500/9752\n",
      "Created 4101 clustersAssigning possible transformations 1500/7076\n",
      "Assigning possible transformations 1500/4146\n",
      "\n",
      "Assigning possible transformations 8000/9752\n",
      "Binning with mh_biggest = True\n",
      "Assigning possible transformations 2000/7076\n",
      "Assigning possible transformations 2000/4146\n",
      "Assigning possible transformations 0/4101\n",
      "Assigning possible transformations 8500/9752\n",
      "Assigning possible transformations 2500/7076\n",
      "Assigning possible transformations 2500/4146\n",
      "Assigning possible transformations 500/4101\n",
      "Assigning possible transformations 9000/9752\n",
      "Assigning possible transformations 3000/7076\n",
      "Assigning possible transformations 3000/4146\n",
      "Assigning possible transformations 1000/4101\n",
      "Assigning possible transformations 9500/9752\n",
      "Assigning possible transformations 3500/7076\n",
      "Assigning possible transformations 3500/4146\n",
      "Assigning possible transformations 1500/4101\n",
      "1875 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 4000/7076\n",
      "Assigning possible transformations 4000/4146\n",
      "Assigning possible transformations 2000/4101\n",
      "[M+ACN+H, 2M+H, 2M+Na, M+ACN+2H, M+2ACN+2H, M+2H, M+H, M+H+NH4, M+2ACN+H, M+NH4, 2M+ACN+H, M+CH3OH+H, M+ACN+Na, M+Na]\n",
      "Created 5317 clustersAssigning possible transformations 4500/7076\n",
      "897 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 2500/4101\n",
      "\n",
      "Binning with mh_biggest = True\n",
      "Assigning possible transformations 5000/7076\n",
      "[M+ACN+H, 2M+H, 2M+Na, M+ACN+2H, M+2ACN+2H, M+2H, M+H, M+H+NH4, M+2ACN+H, M+NH4, 2M+ACN+H, M+CH3OH+H, M+ACN+Na, M+Na]\n",
      "Assigning possible transformations 3000/4101\n",
      "Assigning possible transformations 0/5317\n",
      "Created 6319 clustersAssigning possible transformations 5500/7076\n",
      "\n",
      "Assigning possible transformations 3500/4101\n",
      "Assigning possible transformations 500/5317\n",
      "Binning with mh_biggest = True\n",
      "Assigning possible transformations 6000/7076\n",
      "Assigning possible transformations 0/6319\n",
      "Assigning possible transformations 4000/4101\n",
      "Assigning possible transformations 1000/5317\n",
      "Assigning possible transformations 6500/7076\n",
      "Assigning possible transformations 500/6319\n",
      "701 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 1500/5317\n",
      "Assigning possible transformations 7000/7076\n",
      "Assigning possible transformations 1000/6319\n",
      "[M+ACN+H, 2M+H, 2M+Na, M+ACN+2H, M+2ACN+2H, M+2H, M+H, M+H+NH4, M+2ACN+H, M+NH4, 2M+ACN+H, M+CH3OH+H, M+ACN+Na, M+Na]\n",
      "Assigning possible transformations 2000/5317\n",
      "Created 5485 clusters1407 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 1500/6319\n",
      "\n",
      "Assigning possible transformations 2500/5317\n",
      "Binning with mh_biggest = True\n",
      "[M+ACN+H, 2M+H, 2M+Na, M+ACN+2H, M+2ACN+2H, M+2H, M+H, M+H+NH4, M+2ACN+H, M+NH4, 2M+ACN+H, M+CH3OH+H, M+ACN+Na, M+Na]\n",
      "Assigning possible transformations 2000/6319\n",
      "Assigning possible transformations 0/5485\n",
      "Assigning possible transformations 3000/5317\n",
      "Created 5034 clusters\n",
      "Assigning possible transformations 2500/6319\n",
      "Assigning possible transformations 500/5485\n",
      "Assigning possible transformations 3500/5317\n",
      "Binning with mh_biggest = True\n",
      "Assigning possible transformations 0/5034\n",
      "Assigning possible transformations 3000/6319\n",
      "Assigning possible transformations 1000/5485\n",
      "Assigning possible transformations 4000/5317\n",
      "Assigning possible transformations 500/5034\n",
      "Assigning possible transformations 3500/6319\n",
      "Assigning possible transformations 1500/5485\n",
      "Assigning possible transformations 4500/5317\n",
      "Assigning possible transformations 1000/5034\n",
      "Assigning possible transformations 4000/6319\n",
      "Assigning possible transformations 2000/5485\n",
      "Assigning possible transformations 5000/5317\n",
      "Assigning possible transformations 1500/5034\n",
      "Assigning possible transformations 4500/6319\n",
      "Assigning possible transformations 2500/5485\n",
      "1068 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 2000/5034\n",
      "Assigning possible transformations 5000/6319\n",
      "Assigning possible transformations 3000/5485\n",
      "Assigning possible transformations 2500/5034\n",
      "Assigning possible transformations 5500/6319\n",
      "Assigning possible transformations 3500/5485\n",
      "Assigning possible transformations 3000/5034\n",
      "Assigning possible transformations 6000/6319\n",
      "Assigning possible transformations 4000/5485\n",
      "Assigning possible transformations 3500/5034\n",
      "986 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 4500/5485\n",
      "Assigning possible transformations 4000/5034\n",
      "Assigning possible transformations 5000/5485\n",
      "Assigning possible transformations 4500/5034\n",
      "1167 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 5000/5034\n",
      "1053 peaks to be re-sampled in stage 1\n"
     ]
    }
   ],
   "source": [
    "aligner1 = Aligner(data_list, database_file, transformation_file, \n",
    "                       hp, verbose=False, seed=1234567890, parallel=True, mh_biggest=True, use_vb=False)\n",
    "clustering_results = aligner1._first_stage_clustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_list = zip(data_list, clustering_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Run the Experiment</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(selected_data, param_list, hp, evaluation_method):\n",
    "    \n",
    "    performances = []\n",
    "    for param in param_list:\n",
    "\n",
    "        print \"Parameter mass_tol=%f rt_tol=%f\" % (param)\n",
    "        hp.across_file_mass_tol = param[0]\n",
    "        hp.across_file_rt_tol = param[1]\n",
    "        selected_files = [x[0] for x in selected_data]        \n",
    "        aligner = Aligner(selected_files, database_file, transformation_file, \n",
    "                               hp, verbose=False, seed=1234567890)\n",
    "        match_mode = 0\n",
    "        aligner.run(match_mode)\n",
    "\n",
    "        res = aligner.evaluate_performance(gt_file, verbose=False, print_TP=True, method=evaluation_method)\n",
    "        output = param+res[0]\n",
    "        performances.append(output)\n",
    "    \n",
    "    df = pd.DataFrame(performances, columns=['mass_tol', 'rt_tol', 'TP', 'FP', 'FN', 'Prec', 'Rec', 'F1', 'Threshold'])\n",
    "    \n",
    "    sorted_df = df.sort_values(['F1', 'mass_tol', 'rt_tol'], ascending=[False, True, True])\n",
    "    best_row = sorted_df.iloc[0]\n",
    "    return df, best_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(selected_data, best_row, hp, match_mode, evaluation_method):\n",
    "\n",
    "    param = (best_row['mass_tol'], best_row['rt_tol'])\n",
    "    hp.across_file_mass_tol = param[0]\n",
    "    hp.across_file_rt_tol = param[1]\n",
    "    selected_files = [x[0] for x in selected_data]\n",
    "    selected_clusterings = [x[1] for x in selected_data]    \n",
    "    aligner = Aligner(selected_files, database_file, transformation_file, \n",
    "                           hp, verbose=False, seed=1234567890)\n",
    "    aligner.run(match_mode, first_stage_clustering_results=selected_clusterings)\n",
    "\n",
    "    res = aligner.evaluate_performance(gt_file, verbose=False, print_TP=True, method=evaluation_method)\n",
    "    output = param+res[0]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Training on ['std1-file1.txt', 'std1-file2.txt']\n",
      "Parameter mass_tol=3.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=100.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=100.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=100.000000\n",
      "Testing on ['std1-file6.txt', 'std1-file7.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=40, tp=11, fp=4, fn=17, prec=0.733, rec=0.393, f1=0.512, th_prob=1.000\n",
      "match_mode=1, mass_tol=3, rt_tol=40, tp=15, fp=1, fn=13, prec=0.938, rec=0.536, f1=0.682, th_prob=1.000\n",
      "\n",
      "Iteration 1\n",
      "Training on ['std1-file11.txt', 'std1-file6.txt']\n",
      "Parameter mass_tol=3.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=100.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=100.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=100.000000\n",
      "Testing on ['std1-file4.txt', 'std1-file2.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=80, tp=138, fp=16, fn=13, prec=0.896, rec=0.914, f1=0.905, th_prob=1.000\n",
      "match_mode=1, mass_tol=3, rt_tol=80, tp=139, fp=3, fn=12, prec=0.979, rec=0.921, f1=0.949, th_prob=1.000\n",
      "\n",
      "Iteration 2\n",
      "Training on ['std1-file6.txt', 'std1-file7.txt']\n",
      "Parameter mass_tol=3.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=100.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=100.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=100.000000\n",
      "Testing on ['std1-file7.txt', 'std1-file6.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=70, tp=24, fp=5, fn=4, prec=0.828, rec=0.857, f1=0.842, th_prob=1.000\n",
      "match_mode=1, mass_tol=3, rt_tol=70, tp=27, fp=1, fn=1, prec=0.964, rec=0.964, f1=0.964, th_prob=1.000\n",
      "\n",
      "Iteration 3\n",
      "Training on ['std1-file6.txt', 'std1-file7.txt']\n",
      "Parameter mass_tol=3.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=100.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=100.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=100.000000\n",
      "Testing on ['std1-file2.txt', 'std1-file3.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=70, tp=136, fp=12, fn=12, prec=0.919, rec=0.919, f1=0.919, th_prob=1.000\n",
      "match_mode=1, mass_tol=3, rt_tol=70, tp=139, fp=2, fn=9, prec=0.986, rec=0.939, f1=0.962, th_prob=1.000\n",
      "\n",
      "Iteration 4\n",
      "Training on ['std1-file2.txt', 'std1-file7.txt']\n",
      "Parameter mass_tol=3.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=100.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=100.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=100.000000\n",
      "Testing on ['std1-file3.txt', 'std1-file11.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=70, tp=144, fp=5, fn=5, prec=0.966, rec=0.966, f1=0.966, th_prob=1.000\n",
      "match_mode=1, mass_tol=3, rt_tol=70, tp=141, fp=5, fn=8, prec=0.966, rec=0.946, f1=0.956, th_prob=1.000\n",
      "\n",
      "Iteration 5\n",
      "Training on ['std1-file6.txt', 'std1-file4.txt']\n",
      "Parameter mass_tol=3.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=100.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=100.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=9.000000 rt_tol=100.000000\n",
      "Testing on ['std1-file7.txt', 'std1-file1.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=90, tp=25, fp=5, fn=6, prec=0.833, rec=0.806, f1=0.820, th_prob=1.000\n",
      "match_mode=1, mass_tol=3, rt_tol=90, tp=29, fp=1, fn=2, prec=0.967, rec=0.935, f1=0.951, th_prob=1.000\n",
      "\n",
      "Iteration 6\n",
      "Training on ['std1-file11.txt', 'std1-file3.txt']\n",
      "Parameter mass_tol=3.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=50.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=60.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=70.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=80.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=90.000000\n",
      "Parameter mass_tol=3.000000 rt_tol=100.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=10.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=20.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=30.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=40.000000\n",
      "Parameter mass_tol=6.000000 rt_tol=50.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-cad430b8bc93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Iteration %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Training on %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtraining_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_training_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluation_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtesting_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-5235d5107725>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(selected_data, param_list, hp, evaluation_method)\u001b[0m\n\u001b[0;32m     11\u001b[0m                                hp, verbose=False, seed=1234567890)\n\u001b[0;32m     12\u001b[0m         \u001b[0mmatch_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0maligner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatch_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maligner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_performance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgt_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_TP\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevaluation_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/joewandy/git/metabolomics_tools/alignment/shared_bin_matching.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, match_mode, first_stage_clustering_results)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mmatching_rt_tol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macross_file_rt_tol\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mfile_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0malignment_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_match_peak_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatching_mass_tol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatching_rt_tol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmatch_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# matching based on the MAP of precursor clustering\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/joewandy/git/metabolomics_tools/alignment/shared_bin_matching.py\u001b[0m in \u001b[0;36m_match_peak_features\u001b[1;34m(self, file_data, mass_tol, rt_tol)\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0malignment_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_aligned_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m             \u001b[0mmatcher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaxWeightedMatching\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatched_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malignment_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m             \u001b[0mmatched_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_matching\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m             \u001b[0moutput_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatched_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_all_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0minput_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0moutput_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"input %d output %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/joewandy/git/metabolomics_tools/alignment/matching.pyc\u001b[0m in \u001b[0;36mdo_matching\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Computing score matrix\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mscore_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwomen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdmz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;31m# do approximate or exact matching here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/joewandy/git/metabolomics_tools/alignment/matching.pyc\u001b[0m in \u001b[0;36mcompute_scores\u001b[1;34m(self, men, women, dmz, drt)\u001b[0m\n\u001b[0;32m    125\u001b[0m                     \u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_dist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mman\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwoman\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdmz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                     \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwoman\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m                     \u001b[0mdist_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m                     \u001b[0mweight_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_dist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/joewandy/anaconda/lib/python2.7/site-packages/scipy/sparse/lil.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, index, x)\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[1;31m# assignment for other types is handled below together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[1;31m# with fancy indexing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m             if ((isinstance(i, int) or isinstance(i, np.integer)) and\n\u001b[0m\u001b[0;32m    276\u001b[0m                     (isinstance(j, int) or isinstance(j, np.integer))):\n\u001b[0;32m    277\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iteration_results = []\n",
    "n_iter = 30\n",
    "for i in range(n_iter):\n",
    "\n",
    "    training_data = random.sample(combined_list, n_files)\n",
    "    print \"Iteration %d\" % i\n",
    "    print \"Training on %s\" % [x[0].filename for x in training_data]\n",
    "    training_df, best_training_row = train(training_data, param_list, hp, evaluation_method)\n",
    "    \n",
    "    testing_data = random.sample(combined_list, n_files)\n",
    "    print \"Testing on %s\" % [x[0].filename for x in testing_data]\n",
    "\n",
    "    match_mode = 0\n",
    "    match_feature_res = test(testing_data, best_training_row, hp, match_mode, evaluation_method)\n",
    "    output = (match_mode,) + match_feature_res\n",
    "    print \"match_mode=%d, mass_tol=%d, rt_tol=%d, tp=%d, fp=%d, fn=%d, prec=%.3f, rec=%.3f, f1=%.3f, th_prob=%.3f\" % output\n",
    "\n",
    "    match_mode = 1\n",
    "    match_cluster_res = test(testing_data, best_training_row, hp, match_mode, evaluation_method)\n",
    "    output = (match_mode,) + match_cluster_res\n",
    "    print \"match_mode=%d, mass_tol=%d, rt_tol=%d, tp=%d, fp=%d, fn=%d, prec=%.3f, rec=%.3f, f1=%.3f, th_prob=%.3f\" % output\n",
    "\n",
    "    item = (training_data, training_df, best_training_row, match_feature_res, match_cluster_res)\n",
    "    iteration_results.append(item)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
