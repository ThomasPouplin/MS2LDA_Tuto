{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "basedir = '../'\n",
    "sys.path.append(basedir) # need to do this to be able to import the stuff below\n",
    "\n",
    "import itertools\n",
    "from operator import attrgetter\n",
    "from operator import itemgetter\n",
    "import time\n",
    "\n",
    "from discretisation.discrete_mass_clusterer import DiscreteVB\n",
    "from discretisation.models import HyperPars\n",
    "from discretisation.plotting import ClusterPlotter\n",
    "from discretisation.preprocessing import FileLoader\n",
    "import discretisation.utils as utils\n",
    "from dp_rt_clusterer import DpMixtureGibbs\n",
    "\n",
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a whole bunch of useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_hist(mapping, filename, mass_tol, rt_tol):\n",
    "    no_trans = (mapping > 0).sum(1)\n",
    "    mini_hist = []\n",
    "    for i in np.arange(10) + 1:\n",
    "        mini_hist.append((no_trans == i).sum())\n",
    "    print 'mini_hist ' + str(mini_hist)\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(np.arange(10) + 1, mini_hist)\n",
    "    title = 'Binning -- MASS_TOL ' + str(mass_tol) + ', RT_TOL ' + str(rt_tol)\n",
    "    plt.title(title)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.spy(mapping, markersize=1)\n",
    "    plt.title('possible')\n",
    "    plt.suptitle(filename)\n",
    "    plt.show()     \n",
    "    \n",
    "def annotate(annotations, feature, msg):\n",
    "    if feature in annotations:\n",
    "        current_msg = annotations[feature]\n",
    "        annotations[feature] = current_msg + \" \" + msg\n",
    "    else:\n",
    "        annotations[feature] = msg\n",
    "    \n",
    "def get_transformation_map(transformations):\n",
    "    tmap = {}\n",
    "    t = 1\n",
    "    for trans in transformations:\n",
    "        tmap[t] = trans\n",
    "        t += 1\n",
    "    return tmap\n",
    "\n",
    "def find_same_top_id(key, items):\n",
    "    indices = []\n",
    "    results = []\n",
    "    for a in range(len(items)):\n",
    "        check = items[a]\n",
    "        if key.top_id == check.top_id:\n",
    "            indices.append(a)\n",
    "            results.append(check)\n",
    "    return indices, results\n",
    "\n",
    "def match_features(members):\n",
    "    results = []\n",
    "    if len(members) == 1:\n",
    "        # just singleton things\n",
    "        features = members[0].features\n",
    "        for f in features:\n",
    "            tup = (f, )\n",
    "            results.append(tup)                        \n",
    "    else:\n",
    "        # need to match across the same bins\n",
    "        processed = set()\n",
    "        for bb1 in members:\n",
    "            features1 = bb1.features\n",
    "            for f1 in features1:\n",
    "                if f1 in processed:\n",
    "                    continue\n",
    "                # find features in other bins that are the closest in mass to f1\n",
    "                temp = []\n",
    "                temp.append(f1)\n",
    "                processed.add(f1)\n",
    "                for bb2 in members:\n",
    "                    if bb1.origin == bb2.origin:\n",
    "                        continue\n",
    "                    else:\n",
    "                        features2 = bb2.features\n",
    "                        closest = None\n",
    "                        min_diff = float('inf')\n",
    "                        for f2 in features2:\n",
    "                            if f2 in processed:\n",
    "                                continue\n",
    "                            diff = abs(f1.mass - f2.mass)\n",
    "                            if diff < min_diff:\n",
    "                                min_diff = diff\n",
    "                                closest = f2\n",
    "                        if closest is not None:\n",
    "                            temp.append(closest)\n",
    "                            processed.add(closest)\n",
    "                tup = tuple(temp)\n",
    "                results.append(tup)  \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load 4 std1 pos files and process them with the following parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "database = '../discretisation/database/std1_mols.csv'\n",
    "transformation = '../discretisation/mulsubs/mulsub2.txt'\n",
    "input_file = './input/std1_csv_2'\n",
    "\n",
    "binning_mass_tol = 2.0                 # mass tolerance in ppm when binning\n",
    "binning_rt_tol = 5.0                   # rt tolerance in seconds when binning\n",
    "within_file_rt_sd = 2.5                # standard deviation of each cluster when clustering by precursor masses in a single file\n",
    "across_file_rt_sd = 10.0               # standard deviation of mixture component when clustering by RT across files\n",
    "alpha_mass = 100.0                     # concentration parameter for precursor mass clustering\n",
    "alpha_rt = 100.0                       # concentration parameter for DP mixture on RT\n",
    "t = 0.50                               # threshold for cluster membership for precursor mass clustering\n",
    "limit_n = 1000                         # the number of features to load per file to make debugging easier, -1 to load all\n",
    "\n",
    "mass_clustering_n_iterations = 20       # no. of iterations for VB precursor clustering\n",
    "rt_clustering_nsamps = 200              # no. of total samples for Gibbs RT clustering\n",
    "rt_clustering_burnin = 100              # no. of burn-in samples for Gibbs RT clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First stage clustering. Here we do the usual precursor mass clustering of peak features to some common bins shared across files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5823 features read from 021010_jp32A_15ul_1_000_ld_020.txt\n",
      "4781 features read from 021016_jp32A_10ul_3_000_ld_020.txt\n",
      "Discretising at mass_tol=2.0\n",
      "Making top-level bins shared across files ......................................................\n",
      "Total top bins=10319 total features=10604\n",
      "Instantiating concrete bins for file 0 ....................................................\n",
      "File 0 has 5824 concrete bins instantiated\n",
      "Building matrices for file 0 ..............."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ef3f11057391>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Here we cluster peak features by their precursor masses to the common bins shared across files.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFileLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinning_mass_tol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinning_rt_tol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit_n\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtransformations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_transformation_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m../discretisation/preprocessing.pyc\u001b[0m in \u001b[0;36mload_model_input\u001b[1;34m(self, input_file, database_file, transformation_file, mass_tol, rt_tol, make_bins, synthetic, limit_n)\u001b[0m\n",
      "\u001b[1;32m../discretisation/preprocessing.pyc\u001b[0m in \u001b[0;36mrun_multiple\u001b[1;34m(self, data_list)\u001b[0m\n",
      "\u001b[1;32m/home/joewandy/anaconda/lib/python2.7/site-packages/scipy/sparse/lil.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, index, x)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;31m# General indexing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m         \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unpack_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;31m# shortcut for common case of full matrix assign:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/joewandy/anaconda/lib/python2.7/site-packages/scipy/sparse/sputils.pyc\u001b[0m in \u001b[0;36m_unpack_index\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \"\"\"\n\u001b[0;32m    249\u001b[0m         \u001b[1;31m# First, check if indexing with single boolean matrix.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspmatrix\u001b[0m  \u001b[1;31m# This feels dirty but...\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         if (isinstance(index, (spmatrix, np.ndarray)) and\n\u001b[0;32m    252\u001b[0m            (index.ndim == 2) and index.dtype.kind == 'b'):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# First stage clustering. \n",
    "# Here we cluster peak features by their precursor masses to the common bins shared across files.\n",
    "loader = FileLoader()\n",
    "data_list = loader.load_model_input(input_file, database, transformation, binning_mass_tol, binning_rt_tol, limit_n=limit_n)\n",
    "transformations = data_list[0].transformations\n",
    "tmap = get_transformation_map(transformations)\n",
    "all_bins = []\n",
    "posterior_bin_rts = []    \n",
    "annotations = {}\n",
    "\n",
    "file_bins = []\n",
    "file_post_rts = []\n",
    "\n",
    "for j in range(len(data_list)):\n",
    "\n",
    "    # run precursor mass clustering\n",
    "    peak_data = data_list[j]\n",
    "    plot_hist(peak_data.possible, input_file, binning_mass_tol, binning_rt_tol)\n",
    "    print \"Clustering file \" + str(j) + \" by precursor masses\"\n",
    "    hp = HyperPars()\n",
    "    hp.rt_prec = 1.0/(within_file_rt_sd*within_file_rt_sd)\n",
    "    hp.alpha = alpha_mass\n",
    "    discrete = DiscreteVB(peak_data, hp)\n",
    "    # discrete = ContinuousVB(peak_data, hp)\n",
    "    discrete.n_iterations = mass_clustering_n_iterations\n",
    "    print discrete\n",
    "    discrete.run()\n",
    "\n",
    "    # pick the non-empty bins for the second stage clustering\n",
    "    cluster_membership = (discrete.Z>t)\n",
    "    s = cluster_membership.sum(0)\n",
    "    nnz_idx = s.nonzero()[1]  \n",
    "    nnz_idx = np.squeeze(np.asarray(nnz_idx)) # flatten the thing\n",
    "\n",
    "    # find the non-empty bins\n",
    "    bins = [peak_data.bins[a] for a in nnz_idx]\n",
    "    all_bins.extend(bins)\n",
    "    file_bins.append(bins)\n",
    "\n",
    "    # find the non-empty bins' posterior RT values\n",
    "    bin_rts = discrete.cluster_rt_mean[nnz_idx]\n",
    "    plt.figure()\n",
    "    plt.plot(bin_rts, '.b')\n",
    "    plt.title(\"Posterior RT values for file \" + str(j))\n",
    "    plt.xlabel(\"Non-empty bins\")\n",
    "    plt.ylabel(\"RT\")\n",
    "    plt.show()\n",
    "    bin_rts = bin_rts.ravel().tolist()\n",
    "    posterior_bin_rts.extend(bin_rts)\n",
    "    file_post_rts.append(bin_rts)\n",
    "\n",
    "    # make some plots\n",
    "    cp = ClusterPlotter(peak_data, discrete)\n",
    "    cp.summary(file_idx=j)\n",
    "    # cp.plot_biggest(3)        \n",
    "\n",
    "    # assign peaks into their respective bins, \n",
    "    # this makes it easier when matching peaks across the same bins later\n",
    "    # note: a peak can belong to multiple bins, depending on the choice of threshold t\n",
    "    cx = cluster_membership.tocoo()\n",
    "    for i,j,v in itertools.izip(cx.row, cx.col, cx.data):\n",
    "        f = peak_data.features[i]\n",
    "        bb = peak_data.bins[j] # copy of the common bin specific to file j\n",
    "        bb.add_feature(f)    \n",
    "        # annotate each feature by its precursor mass & adduct type probabilities, for reporting later\n",
    "        bin_prob = discrete.Z[i, j]\n",
    "        trans_idx = discrete.possible[i, j]\n",
    "        tran = tmap[trans_idx]\n",
    "        msg = \"{:s}@{:3.5f} prob={:.2f}\".format(tran.name, bb.mass, bin_prob)            \n",
    "        annotate(annotations, f, msg)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the distribution of values in the Z matrices (peak to cluster assignment) for each file. This kind of suggests that the problem doesn't have to be solved by a probability model at all? i.e. the graph-based approach in CAMERA is good enough. Especially for high precision mass spec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot posterior RTs of the same bins across files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_bins = file_bins[0]\n",
    "first_rts = file_post_rts[0]\n",
    "second_bins = file_bins[1]\n",
    "second_rts = file_post_rts[1]\n",
    "xs = []\n",
    "ys = []\n",
    "for j1 in range(len(first_bins)):\n",
    "    bin1 = first_bins[j1]\n",
    "    j2s, bin2s = find_same_top_id(bin1, second_bins)\n",
    "    for j2 in j2s:        \n",
    "        rt1 = first_rts[j1]\n",
    "        rt2 = second_rts[j2]\n",
    "        xs.append(rt1)\n",
    "        ys.append(rt2)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.array(xs), np.array(ys), '.b')\n",
    "plt.xlabel(\"File 0\")\n",
    "plt.ylabel(\"File 1\")\n",
    "plt.title(\"Bin-vs-bin posterior RTs\")\n",
    "plt.show()\n",
    "\n",
    "sizes = []\n",
    "for bin1 in first_bins:\n",
    "    sizes.append(bin1.get_features_count())\n",
    "plt.figure()\n",
    "plt.plot(np.array(sizes), 'r.')\n",
    "sizes = []\n",
    "for bin2 in second_bins:\n",
    "    sizes.append(bin2.get_features_count())\n",
    "plt.plot(np.array(sizes), 'g.')\n",
    "plt.title(\"Bin sizes in file 0 & 1\")\n",
    "plt.xlabel(\"Bins\")\n",
    "plt.ylabel(\"Sizes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second stage clustering. Here we cluster the bins across all the fils using a DP mixture on the posterior RT values of the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Second-stage clustering\n",
    "N = len(all_bins)\n",
    "assert N == len(posterior_bin_rts)\n",
    "\n",
    "# Here we cluster the 'concrete' common bins across files by their posterior RT values\n",
    "hp = HyperPars()\n",
    "hp.rt_prec = 1.0/(across_file_rt_sd*across_file_rt_sd)\n",
    "hp.rt_prior_prec = 5E-3\n",
    "hp.alpha = alpha_rt\n",
    "data = (posterior_bin_rts, all_bins)\n",
    "dp = DpMixtureGibbs(data, hp)\n",
    "dp.nsamps = rt_clustering_nsamps\n",
    "dp.burn_in = rt_clustering_burnin\n",
    "dp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot distribution of values in ZZ_all\n",
    "ZZ_all = dp.ZZ_all\n",
    "x = []\n",
    "cx = ZZ_all.tocoo()    \n",
    "for i,j,v in itertools.izip(cx.row, cx.col, cx.data):\n",
    "    x.append(v)       \n",
    "x = np.array(x)\n",
    "plt.figure() \n",
    "plt.hist(x, 10)\n",
    "plt.title(\"DP RT clustering -- ZZ_all\")\n",
    "plt.xlabel(\"Probabilities\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now need to construct the alignment results from all the clustering output above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count frequencies of aligned bins produced across the Gibbs samples\n",
    "print \"Counting frequencies of aligned peaksets\"\n",
    "matching_results = dp.matching_results\n",
    "counter = dict()\n",
    "for bins in matching_results:\n",
    "    if len(bins) > 1:\n",
    "        bins = sorted(bins, key = attrgetter('origin'))\n",
    "        bins = tuple(bins)\n",
    "    if bins not in counter:\n",
    "        counter[bins] = 1\n",
    "    else:\n",
    "        counter[bins] += 1\n",
    "\n",
    "# normalise the counts\n",
    "print \"Normalising counts\"\n",
    "S = dp.samples_obtained\n",
    "for key, value in counter.items():\n",
    "    new_value = float(value)/S\n",
    "    counter[key] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print report of aligned peaksets in descending order of probabilities\n",
    "print \n",
    "print \"==========================================================================\"\n",
    "print \"REPORT\"\n",
    "print \"==========================================================================\"\n",
    "sorted_list = sorted(counter.items(), key=itemgetter(1), reverse=True)\n",
    "probs = []\n",
    "i = 0\n",
    "for item in sorted_list:\n",
    "    members = item[0]\n",
    "    if len(members)==1:\n",
    "        continue # skip all the singleton stuff\n",
    "    prob = item[1]\n",
    "    matched_list = match_features(members)\n",
    "    for features in matched_list:\n",
    "        if len(features)==1:\n",
    "            continue\n",
    "        mzs = np.array([f.mass for f in features])\n",
    "        rts = np.array([f.rt for f in features])\n",
    "        avg_mz = np.mean(mzs)\n",
    "        avg_rt = np.mean(rts)\n",
    "        print str(i+1) + \". avg m/z=\" + str(avg_mz) + \" avg RT=\" + str(avg_rt) + \" prob=\" + str(prob)\n",
    "        for f in features:\n",
    "            msg = annotations[f]            \n",
    "            output = \"\\tfeature_id {:5d} file_id {:d} mz {:3.5f} RT {:5.2f} intensity {:.4e}\\t{:s}\".format(\n",
    "                        f.feature_id, f.file_id, f.mass, f.rt, f.intensity, msg)\n",
    "            print(output) \n",
    "        probs.append(prob)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report above shows the list of numbered aligned peaksets (excluding the singleton peaksets), alongside their probabilit values, and the transformed mass and adduct type annotations for each peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = np.array(probs) \n",
    "plt.figure()\n",
    "plt.hist(probs, 10)\n",
    "plt.title(\"Aligned peaksets probabilities\")\n",
    "plt.xlabel(\"Probabilities\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "utils.timer(\"TOTAL ELAPSED TIME\", start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate performance .."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
