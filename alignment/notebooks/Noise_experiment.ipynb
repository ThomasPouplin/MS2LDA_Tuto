{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Experiment with adding noise on the std1pos data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "basedir = '/home/joewandy/git/metabolomics_tools'\n",
    "sys.path.append(basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import cPickle\n",
    "import random\n",
    "import copy\n",
    "import glob\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from alignment.models import HyperPars as AlignmentHyperPars\n",
    "from discretisation.adduct_cluster import AdductCluster, Peak, Possible\n",
    "from discretisation import utils\n",
    "from discretisation.preprocessing import FileLoader\n",
    "from alignment.shared_bin_matching import SharedBinMatching as Aligner\n",
    "from alignment.ground_truth import GroundTruth\n",
    "from discretisation.models import Feature, PeakData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Experiment Parameters</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up all the experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dir = '/home/joewandy/git/metabolomics_tools/alignment/input/std1_csv_full_old'\n",
    "transformation_file = '/home/joewandy/git/metabolomics_tools/alignment/pos_transformations_full.yml'\n",
    "gt_file = '/home/joewandy/git/metabolomics_tools/alignment/input/std1_csv_full_old/ground_truth/ground_truth.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters across_file_mass_tol=10, across_file_rt_tol=120, alpha_mass=1.0, beta=0.1, dp_alpha=1000.0, mass_clustering_n_iterations=200, matching_alpha=0.3, rt_clustering_burnin=0, rt_clustering_nsamps=100, t=0.0, within_file_mass_tol=5, within_file_rt_tol=30\n"
     ]
    }
   ],
   "source": [
    "hp = AlignmentHyperPars()    \n",
    "hp.within_file_mass_tol = 5\n",
    "hp.within_file_rt_tol = 30\n",
    "hp.across_file_mass_tol = 10\n",
    "hp.across_file_rt_tol = 120\n",
    "hp.alpha_mass = 1.0\n",
    "hp.dp_alpha = 1000.0\n",
    "hp.beta = 0.1\n",
    "hp.t = 0.0\n",
    "hp.mass_clustering_n_iterations = 200\n",
    "hp.rt_clustering_nsamps = 100\n",
    "hp.rt_clustering_burnin = 0\n",
    "\n",
    "print hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluation_method = 2\n",
    "n_iter = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_list = []\n",
    "for mass_tol in range(2, 11, 2):\n",
    "    for rt_tol in range(5, 101, 5):\n",
    "        param_list.append((mass_tol, rt_tol))\n",
    "\n",
    "# overwrite\n",
    "param_list = [(hp.across_file_mass_tol, hp.across_file_rt_tol)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create the first-stage clustering for all input files -- Gibbs, mh_biggest=True</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_or_create_clustering(filename, input_dir, transformation_file, hp):\n",
    "    try:\n",
    "        with gzip.GzipFile(filename, 'rb') as f:\n",
    "            combined_list = cPickle.load(f)\n",
    "            print \"Loaded from %s\" % filename\n",
    "            return combined_list\n",
    "    except (IOError, EOFError):\n",
    "        loader = FileLoader()        \n",
    "        data_list = loader.load_model_input(input_dir, synthetic=True)\n",
    "        aligner = Aligner(data_list, None, transformation_file, \n",
    "                               hp, verbose=False, seed=1234567890, parallel=True, mh_biggest=True, use_vb=False)\n",
    "        clustering_results = aligner._first_stage_clustering()\n",
    "        combined_list = zip(data_list, clustering_results)\n",
    "        with gzip.GzipFile(filename, 'wb') as f:\n",
    "            cPickle.dump(combined_list, f, protocol=cPickle.HIGHEST_PROTOCOL)        \n",
    "        print \"Saved to %s\" % filename\n",
    "        return combined_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from pickles/std1_pos_clustering.p\n"
     ]
    }
   ],
   "source": [
    "combined_list = load_or_create_clustering('pickles/std1_pos_clustering.p', input_dir, transformation_file, hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define Experimental Methods</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(selected_data, param_list, hp, match_mode, evaluation_method):\n",
    "    \n",
    "    performances = []\n",
    "    for param in param_list:\n",
    "\n",
    "        # print \"Parameter mass_tol=%f rt_tol=%f\" % (param)\n",
    "        hp.across_file_mass_tol = param[0]\n",
    "        hp.across_file_rt_tol = param[1]\n",
    "        selected_files = [x[0] for x in selected_data]  \n",
    "        selected_clusterings = [x[1] for x in selected_data]            \n",
    "        aligner = Aligner(selected_files, None, transformation_file, \n",
    "                               hp, verbose=False, seed=1234567890)\n",
    "        aligner.run(match_mode, first_stage_clustering_results=selected_clusterings)\n",
    "\n",
    "        res = aligner.evaluate_performance(gt_file, verbose=False, print_TP=True, method=evaluation_method)\n",
    "        output = param+res[0]\n",
    "        performances.append(output)\n",
    "    \n",
    "    df = pd.DataFrame(performances, columns=['mass_tol', 'rt_tol', 'TP', 'FP', 'FN', 'Prec', 'Rec', 'F1', 'Threshold'])\n",
    "    \n",
    "    sorted_df = df.sort_values(['F1', 'mass_tol', 'rt_tol'], ascending=[False, True, True])\n",
    "    best_row = sorted_df.iloc[0]\n",
    "    return df, best_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(selected_data, best_row, hp, match_mode, evaluation_method):\n",
    "\n",
    "    param = (best_row['mass_tol'], best_row['rt_tol'])\n",
    "    hp.across_file_mass_tol = param[0]\n",
    "    hp.across_file_rt_tol = param[1]\n",
    "    selected_files = [x[0] for x in selected_data]\n",
    "    selected_clusterings = [x[1] for x in selected_data]    \n",
    "    aligner = Aligner(selected_files, None, transformation_file, \n",
    "                           hp, verbose=False, seed=1234567890)\n",
    "    aligner.run(match_mode, first_stage_clustering_results=selected_clusterings)\n",
    "\n",
    "    res = aligner.evaluate_performance(gt_file, verbose=False, print_TP=True, method=evaluation_method)\n",
    "    output = param+res[0]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_single(match_mode, training_data, testing_data, i):\n",
    "    \n",
    "    for n in range(len(training_data)):\n",
    "        print training_data[n][0].num_peaks\n",
    "    for n in range(len(testing_data)):\n",
    "        print testing_data[n][0].num_peaks\n",
    "\n",
    "    print \"Iteration %d\" % i\n",
    "    print \"Training on %s\" % [x[0].filename for x in training_data]\n",
    "    training_df, best_training_row = train(training_data, param_list, hp, match_mode, evaluation_method)\n",
    "\n",
    "    testing_data = testing_list[i]\n",
    "    print \"Testing on %s\" % [x[0].filename for x in testing_data]\n",
    "    match_res = test(testing_data, best_training_row, hp, match_mode, evaluation_method)\n",
    "    output = (match_mode,) + match_res\n",
    "    print \"match_mode=%d, mass_tol=%d, rt_tol=%d, tp=%d, fp=%d, fn=%d, prec=%.3f, rec=%.3f, f1=%.3f, th_prob=%.3f\" % output\n",
    "\n",
    "    item = (training_data, training_df, best_training_row, match_res)\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_test(match_mode, training_list, testing_list, idx=None):\n",
    "    assert len(training_list) == len(testing_list)\n",
    "    n_iter = len(training_list)\n",
    "    exp_results = []\n",
    "    if idx is not None:\n",
    "        for i in range(n_iter):\n",
    "            training_data = training_list[i]\n",
    "            testing_data = testing_list[i]\n",
    "            item = train_test_single(match_mode, training_data, testing_data, i)\n",
    "            exp_results.append(item)\n",
    "            print\n",
    "    else:\n",
    "        training_data = training_list[idx]\n",
    "        testing_data = testing_list[idx]\n",
    "        item = train_test_single(match_mode, training_data, testing_data, idx)\n",
    "        exp_results.append(item)\n",
    "        print\n",
    "        \n",
    "    return exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_experiment(match_mode, training_list, testing_list, filename):\n",
    "    try:\n",
    "        with gzip.GzipFile(filename, 'rb') as f:        \n",
    "            exp_results = cPickle.load(f)\n",
    "            print \"Loaded from %s\" % filename\n",
    "            return exp_results\n",
    "    except (IOError, EOFError):\n",
    "        exp_results = train_test(match_mode, training_list, testing_list)\n",
    "        with gzip.GzipFile(filename, 'wb') as f:\n",
    "            cPickle.dump(exp_results, f, protocol=cPickle.HIGHEST_PROTOCOL)                        \n",
    "        print \"Saved to %s\" % filename\n",
    "    return exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_or_create_filelist(filename, combined_list, n_iter, n_files):\n",
    "    try:\n",
    "        with gzip.GzipFile(filename, 'rb') as f:        \n",
    "            item_list = cPickle.load(f)\n",
    "            print \"Loaded from %s\" % filename\n",
    "            for item in item_list:\n",
    "                print \"%s\" % [x[0].filename for x in item]\n",
    "            return item_list\n",
    "    except (IOError, EOFError):\n",
    "        item_list = []\n",
    "        for i in range(n_iter):\n",
    "            item = random.sample(combined_list, n_files)\n",
    "            print \"%s\" % [x[0].filename for x in item]\n",
    "            item_list.append(item)\n",
    "        with gzip.GzipFile(filename, 'wb') as f:\n",
    "            cPickle.dump(item_list, f, protocol=cPickle.HIGHEST_PROTOCOL)                    \n",
    "        print \"Saved to %s\" % filename\n",
    "        return item_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Methods to add noisy peaks</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_noisy_peaks(peakdata, num_add):\n",
    "\n",
    "    next_id = peakdata.features[-1].feature_id\n",
    "    file_id = peakdata.features[-1].file_id\n",
    "    min_mz, max_mz = np.min(peakdata.mass), np.max(peakdata.mass)\n",
    "    min_rt, max_rt = np.min(peakdata.rt), np.max(peakdata.rt)\n",
    "    min_intensity, max_intensity = np.min(peakdata.intensity), np.max(peakdata.intensity)\n",
    "    \n",
    "#     if num_add > peakdata.num_peaks:\n",
    "#         num_add = peakdata.num_peaks\n",
    "        \n",
    "    # new_peaks = random.sample(peakdata.features, num_add)\n",
    "    to_add = list(peakdata.features)\n",
    "    for i in range(num_add):\n",
    "        next_id = next_id + 1\n",
    "        # mz = new_peaks[i].mass\n",
    "        # rt = new_peaks[i].rt\n",
    "        mz = np.random.uniform(low=min_mz, high=max_mz)\n",
    "        rt = np.random.uniform(low=min_rt, high=max_rt)\n",
    "        intensity = np.random.uniform(low=min_intensity, high=max_intensity)\n",
    "        new_feature = Feature(next_id, mz, rt, intensity, file_id)\n",
    "        to_add.append(new_feature)\n",
    "        \n",
    "    new_peakdata = PeakData(to_add, peakdata.filename)\n",
    "    return new_peakdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_noisy_peaks_to_set(training_data, set_idx, num_add):\n",
    "    for i in range(len(training_data)):\n",
    "        single_set = training_data[i]\n",
    "        peakdata = single_set[0]\n",
    "        new_peakdata = add_noisy_peaks(peakdata, num_add)\n",
    "        training_data[i] = (new_peakdata, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Run experiment with 2 random files</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_files = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from pickles/training_list_2.p\n",
      "['std1-file5.txt', 'std1-file7.txt']\n",
      "['std1-file5.txt', 'std1-file11.txt']\n",
      "['std1-file10.txt', 'std1-file5.txt']\n",
      "['std1-file9.txt', 'std1-file2.txt']\n",
      "['std1-file8.txt', 'std1-file5.txt']\n",
      "['std1-file7.txt', 'std1-file6.txt']\n",
      "['std1-file10.txt', 'std1-file3.txt']\n",
      "['std1-file8.txt', 'std1-file2.txt']\n",
      "['std1-file11.txt', 'std1-file7.txt']\n",
      "['std1-file1.txt', 'std1-file2.txt']\n",
      "['std1-file8.txt', 'std1-file1.txt']\n",
      "['std1-file6.txt', 'std1-file9.txt']\n",
      "['std1-file11.txt', 'std1-file7.txt']\n",
      "['std1-file7.txt', 'std1-file8.txt']\n",
      "['std1-file2.txt', 'std1-file7.txt']\n",
      "['std1-file2.txt', 'std1-file8.txt']\n",
      "['std1-file10.txt', 'std1-file7.txt']\n",
      "['std1-file9.txt', 'std1-file4.txt']\n",
      "['std1-file9.txt', 'std1-file4.txt']\n",
      "['std1-file7.txt', 'std1-file2.txt']\n",
      "['std1-file1.txt', 'std1-file10.txt']\n",
      "['std1-file2.txt', 'std1-file9.txt']\n",
      "['std1-file11.txt', 'std1-file1.txt']\n",
      "['std1-file1.txt', 'std1-file10.txt']\n",
      "['std1-file9.txt', 'std1-file8.txt']\n",
      "['std1-file3.txt', 'std1-file1.txt']\n",
      "['std1-file3.txt', 'std1-file9.txt']\n",
      "['std1-file8.txt', 'std1-file7.txt']\n",
      "['std1-file8.txt', 'std1-file11.txt']\n",
      "['std1-file7.txt', 'std1-file3.txt']\n"
     ]
    }
   ],
   "source": [
    "training_list = load_or_create_filelist('pickles/training_list_2.p', combined_list, n_iter, n_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from pickles/testing_list_2.p\n",
      "['std1-file6.txt', 'std1-file8.txt']\n",
      "['std1-file7.txt', 'std1-file11.txt']\n",
      "['std1-file6.txt', 'std1-file2.txt']\n",
      "['std1-file4.txt', 'std1-file8.txt']\n",
      "['std1-file4.txt', 'std1-file7.txt']\n",
      "['std1-file7.txt', 'std1-file11.txt']\n",
      "['std1-file10.txt', 'std1-file1.txt']\n",
      "['std1-file3.txt', 'std1-file10.txt']\n",
      "['std1-file8.txt', 'std1-file11.txt']\n",
      "['std1-file5.txt', 'std1-file3.txt']\n",
      "['std1-file7.txt', 'std1-file5.txt']\n",
      "['std1-file7.txt', 'std1-file10.txt']\n",
      "['std1-file11.txt', 'std1-file9.txt']\n",
      "['std1-file2.txt', 'std1-file10.txt']\n",
      "['std1-file3.txt', 'std1-file7.txt']\n",
      "['std1-file11.txt', 'std1-file10.txt']\n",
      "['std1-file6.txt', 'std1-file9.txt']\n",
      "['std1-file11.txt', 'std1-file2.txt']\n",
      "['std1-file4.txt', 'std1-file10.txt']\n",
      "['std1-file10.txt', 'std1-file4.txt']\n",
      "['std1-file8.txt', 'std1-file3.txt']\n",
      "['std1-file8.txt', 'std1-file4.txt']\n",
      "['std1-file3.txt', 'std1-file11.txt']\n",
      "['std1-file9.txt', 'std1-file5.txt']\n",
      "['std1-file7.txt', 'std1-file4.txt']\n",
      "['std1-file3.txt', 'std1-file2.txt']\n",
      "['std1-file9.txt', 'std1-file1.txt']\n",
      "['std1-file6.txt', 'std1-file7.txt']\n",
      "['std1-file9.txt', 'std1-file11.txt']\n",
      "['std1-file10.txt', 'std1-file9.txt']\n"
     ]
    }
   ],
   "source": [
    "testing_list = load_or_create_filelist('pickles/testing_list_2.p', combined_list, n_iter, n_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test performance before adding noise</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "training_data = training_list[idx]\n",
    "testing_data = testing_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7076\n",
      "6319\n",
      "4146\n",
      "4101\n",
      "Iteration 0\n",
      "Training on ['std1-file5.txt', 'std1-file7.txt']\n",
      "Testing on ['std1-file6.txt', 'std1-file8.txt']\n",
      "match_mode=0, mass_tol=10, rt_tol=60, tp=117, fp=7, fn=13, prec=0.944, rec=0.900, f1=0.921, th_prob=1.000\n"
     ]
    }
   ],
   "source": [
    "exp_results = train_test_single(0,  training_data, testing_data, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result\n",
      "mass_tol     10.000000\n",
      "rt_tol       60.000000\n",
      "TP           25.000000\n",
      "FP            3.000000\n",
      "FN            8.000000\n",
      "Prec          0.892857\n",
      "Rec           0.757576\n",
      "F1            0.819672\n",
      "Threshold     1.000000\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print \"Training result\"\n",
    "print exp_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing result\n",
      "mass_tol      10.000000\n",
      "rt_tol        60.000000\n",
      "TP           117.000000\n",
      "FP             7.000000\n",
      "FN            13.000000\n",
      "Prec           0.943548\n",
      "Rec            0.900000\n",
      "F1             0.921260\n",
      "Threshold      1.000000\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print \"Testing result\"\n",
    "df = pd.DataFrame([exp_results[3]], columns=['mass_tol', 'rt_tol', 'TP', 'FP', 'FN', 'Prec', 'Rec', 'F1', 'Threshold'])\n",
    "print df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Add some noise</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_add = 10000\n",
    "add_noisy_peaks_to_set(training_data, idx, num_add)\n",
    "add_noisy_peaks_to_set(testing_data, idx, num_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Add a single noisy peak</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# single_set = training_data[1]\n",
    "# peakdata = single_set[0]\n",
    "# to_add = list(peakdata.features)\n",
    "# next_id = peakdata.features[-1].feature_id\n",
    "# next_id = next_id + 1\n",
    "# file_id = peakdata.features[-1].file_id\n",
    "# mz = 126.0219\n",
    "# rt = 1000\n",
    "# intensity = 475694\n",
    "# new_feature = Feature(next_id, mz, rt, intensity, file_id)\n",
    "# to_add.append(new_feature)\n",
    "# new_peakdata = PeakData(to_add, peakdata.filename)\n",
    "# training_data[1] = (new_peakdata, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# single_set = training_data[1]\n",
    "# peakdata = single_set[0]\n",
    "# to_add = list(peakdata.features)\n",
    "# next_id = peakdata.features[-1].feature_id\n",
    "# next_id = next_id + 1\n",
    "# file_id = peakdata.features[-1].file_id\n",
    "# mz = 251.0361\n",
    "# rt = 2000\n",
    "# intensity = 475694\n",
    "# new_feature = Feature(next_id, mz, rt, intensity, file_id)\n",
    "# to_add.append(new_feature)\n",
    "# new_peakdata = PeakData(to_add, peakdata.filename)\n",
    "# training_data[1] = (new_peakdata, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print new_feature\n",
    "# print new_peakdata.features[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test performance after adding noise</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57076\n",
      "56319\n",
      "54146\n",
      "54101\n",
      "Iteration 0\n",
      "Training on ['std1-file5.txt', 'std1-file7.txt']\n",
      "Testing on ['std1-file6.txt', 'std1-file8.txt']\n",
      "match_mode=0, mass_tol=10, rt_tol=60, tp=117, fp=7, fn=13, prec=0.944, rec=0.900, f1=0.921, th_prob=1.000\n"
     ]
    }
   ],
   "source": [
    "match_mode = 0\n",
    "exp_results = train_test_single(match_mode,  training_data, testing_data, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result\n",
      "mass_tol     10.000000\n",
      "rt_tol       60.000000\n",
      "TP           25.000000\n",
      "FP            3.000000\n",
      "FN            8.000000\n",
      "Prec          0.892857\n",
      "Rec           0.757576\n",
      "F1            0.819672\n",
      "Threshold     1.000000\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print \"Training result\"\n",
    "print exp_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing result\n",
      "mass_tol      10.000000\n",
      "rt_tol        60.000000\n",
      "TP           117.000000\n",
      "FP             7.000000\n",
      "FN            13.000000\n",
      "Prec           0.943548\n",
      "Rec            0.900000\n",
      "F1             0.921260\n",
      "Threshold      1.000000\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print \"Testing result\"\n",
    "df = pd.DataFrame([exp_results[3]], columns=['mass_tol', 'rt_tol', 'TP', 'FP', 'FN', 'Prec', 'Rec', 'F1', 'Threshold'])\n",
    "print df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
