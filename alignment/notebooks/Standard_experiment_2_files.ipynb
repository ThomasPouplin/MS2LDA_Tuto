{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Experiment with std1pos</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import cPickle\n",
    "import random\n",
    "import copy\n",
    "import glob\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from models import HyperPars as AlignmentHyperPars\n",
    "from discretisation.adduct_cluster import AdductCluster, Peak, Possible\n",
    "from discretisation import utils\n",
    "from discretisation.preprocessing import FileLoader\n",
    "from shared_bin_matching import SharedBinMatching as Aligner\n",
    "from ground_truth import GroundTruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Experiment Parameters</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up all the experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dir = '/home/joewandy/git/metabolomics_tools/alignment/input/std1_csv_full_old'\n",
    "transformation_file = '/home/joewandy/git/metabolomics_tools/alignment/pos_transformations_full.yml'\n",
    "gt_file = '/home/joewandy/git/metabolomics_tools/alignment/input/std1_csv_full_old/ground_truth/ground_truth.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters across_file_mass_tol=10, across_file_rt_tol=120, alpha_mass=1.0, beta=0.1, dp_alpha=1000.0, mass_clustering_n_iterations=200, rt_clustering_burnin=200, rt_clustering_nsamps=400, t=0.0, within_file_mass_tol=5, within_file_rt_tol=30\n"
     ]
    }
   ],
   "source": [
    "hp = AlignmentHyperPars()    \n",
    "hp.within_file_mass_tol = 5\n",
    "hp.within_file_rt_tol = 30\n",
    "hp.across_file_mass_tol = 10\n",
    "hp.across_file_rt_tol = 120\n",
    "hp.alpha_mass = 1.0\n",
    "hp.dp_alpha = 1000.0\n",
    "hp.t = 0.0\n",
    "hp.mass_clustering_n_iterations = 200\n",
    "hp.rt_clustering_nsamps = 400\n",
    "hp.rt_clustering_burnin = 200\n",
    "\n",
    "print hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluation_method = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_list = []\n",
    "for mass_tol in range(3, 10, 3):\n",
    "    for rt_tol in range(10, 101, 10):\n",
    "        param_list.append((mass_tol, rt_tol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create the first-stage clustering for all input files -- Gibbs, mh_biggest=True</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_or_create_clustering(filename, input_dir, transformation_file, hp):\n",
    "    try:\n",
    "        with gzip.GzipFile(filename, 'rb') as f:\n",
    "            combined_list = cPickle.load(f)\n",
    "            print \"Loaded from %s\" % filename\n",
    "    except (IOError, EOFError):\n",
    "        loader = FileLoader()        \n",
    "        data_list = loader.load_model_input(input_dir, synthetic=True)\n",
    "        aligner = Aligner(data_list, None, transformation_file, \n",
    "                               hp, verbose=False, seed=1234567890, parallel=True, mh_biggest=True, use_vb=False)\n",
    "        clustering_results = aligner._first_stage_clustering()\n",
    "        combined_list = zip(data_list, clustering_results)\n",
    "        with gzip.GzipFile(filename, 'wb') as f:\n",
    "            cPickle.dump(combined_list, f, protocol=cPickle.HIGHEST_PROTOCOL)        \n",
    "        print \"Saved to %s\" % filename\n",
    "        return combined_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999 features read from std1-file1.txt\n",
      "4986 features read from std1-file2.txt\n",
      "6836 features read from std1-file3.txt\n",
      "9752 features read from std1-file4.txt\n",
      "7076 features read from std1-file5.txt\n",
      "4146 features read from std1-file6.txt\n",
      "6319 features read from std1-file7.txt\n",
      "4101 features read from std1-file8.txt\n",
      "5485 features read from std1-file9.txt\n",
      "5034 features read from std1-file10.txt\n",
      "5317 features read from std1-file11.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 out of  11 | elapsed:  2.3min remaining: 23.1min\n",
      "[Parallel(n_jobs=4)]: Done   3 out of  11 | elapsed:  4.1min remaining: 10.9min\n",
      "[Parallel(n_jobs=4)]: Done   5 out of  11 | elapsed:  6.6min remaining:  8.0min\n",
      "[Parallel(n_jobs=4)]: Done   7 out of  11 | elapsed:  8.4min remaining:  4.8min\n",
      "[Parallel(n_jobs=4)]: Done   9 out of  11 | elapsed:  8.4min remaining:  1.9min\n",
      "[Parallel(n_jobs=4)]: Done  11 out of  11 | elapsed:  9.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to std1_pos_clustering.p\n",
      "Created 4999 clusters\n",
      "Created 4986 clusters\n",
      "Created 6836 clusters\n",
      "Created 9752 clusters\n",
      "Binning with mh_biggest = True\n",
      "Binning with mh_biggest = True\n",
      "Binning with mh_biggest = True\n",
      "Binning with mh_biggest = True\n",
      "Assigning possible transformations 0/4999\n",
      "Assigning possible transformations 0/4986\n",
      "Assigning possible transformations 0/6836\n",
      "Assigning possible transformations 0/9752\n",
      "Assigning possible transformations 500/4999\n",
      "Assigning possible transformations 500/4986\n",
      "Assigning possible transformations 500/6836\n",
      "Assigning possible transformations 500/9752\n",
      "Assigning possible transformations 1000/4999\n",
      "Assigning possible transformations 1000/4986\n",
      "Assigning possible transformations 1000/6836\n",
      "Assigning possible transformations 1000/9752\n",
      "Assigning possible transformations 1500/4999\n",
      "Assigning possible transformations 1500/4986\n",
      "Assigning possible transformations 1500/6836\n",
      "Assigning possible transformations 1500/9752\n",
      "Assigning possible transformations 2000/4999\n",
      "Assigning possible transformations 2000/4986\n",
      "Assigning possible transformations 2000/6836\n",
      "Assigning possible transformations 2000/9752\n",
      "Assigning possible transformations 2500/4999\n",
      "Assigning possible transformations 2500/4986\n",
      "Assigning possible transformations 2500/6836\n",
      "Assigning possible transformations 2500/9752\n",
      "Assigning possible transformations 3000/4999\n",
      "Assigning possible transformations 3000/4986\n",
      "Assigning possible transformations 3000/6836\n",
      "Assigning possible transformations 3000/9752\n",
      "Assigning possible transformations 3500/4999\n",
      "Assigning possible transformations 3500/4986\n",
      "Assigning possible transformations 3500/6836\n",
      "Assigning possible transformations 3500/9752\n",
      "Assigning possible transformations 4000/4999\n",
      "Assigning possible transformations 4000/4986\n",
      "Assigning possible transformations 4000/6836\n",
      "Assigning possible transformations 4000/9752\n",
      "Assigning possible transformations 4500/4999\n",
      "Assigning possible transformations 4500/4986\n",
      "Assigning possible transformations 4500/6836\n",
      "Assigning possible transformations 4500/9752\n",
      "1096 peaks to be re-sampled in stage 1\n",
      "1083 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 5000/6836\n",
      "Assigning possible transformations 5000/9752\n",
      "Created 4146 clusters\n",
      "Created 7076 clusters\n",
      "Assigning possible transformations 5500/6836\n",
      "Assigning possible transformations 5500/9752\n",
      "Binning with mh_biggest = True\n",
      "Binning with mh_biggest = True\n",
      "Assigning possible transformations 0/4146\n",
      "Assigning possible transformations 0/7076\n",
      "Assigning possible transformations 6000/6836\n",
      "Assigning possible transformations 6000/9752\n",
      "Assigning possible transformations 500/4146\n",
      "Assigning possible transformations 500/7076\n",
      "Assigning possible transformations 6500/6836\n",
      "Assigning possible transformations 6500/9752\n",
      "Assigning possible transformations 1000/4146\n",
      "Assigning possible transformations 1000/7076\n",
      "1215 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 7000/9752\n",
      "Assigning possible transformations 1500/4146\n",
      "Assigning possible transformations 1500/7076\n",
      "Created 4101 clusters\n",
      "Assigning possible transformations 7500/9752\n",
      "Binning with mh_biggest = True\n",
      "Assigning possible transformations 2000/4146\n",
      "Assigning possible transformations 2000/7076\n",
      "Assigning possible transformations 0/4101\n",
      "Assigning possible transformations 8000/9752\n",
      "Assigning possible transformations 2500/4146\n",
      "Assigning possible transformations 2500/7076\n",
      "Assigning possible transformations 500/4101\n",
      "Assigning possible transformations 8500/9752\n",
      "Assigning possible transformations 3000/4146\n",
      "Assigning possible transformations 3000/7076\n",
      "Assigning possible transformations 1000/4101\n",
      "Assigning possible transformations 9000/9752\n",
      "Assigning possible transformations 3500/4146\n",
      "Assigning possible transformations 3500/7076\n",
      "Assigning possible transformations 1500/4101\n",
      "Assigning possible transformations 9500/9752\n",
      "Assigning possible transformations 4000/4146\n",
      "Assigning possible transformations 4000/7076\n",
      "Assigning possible transformations 2000/4101\n",
      "1875 peaks to be re-sampled in stage 1\n",
      "897 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 4500/7076\n",
      "Assigning possible transformations 2500/4101\n",
      "Created 5317 clusters\n",
      "Binning with mh_biggest = True\n",
      "Created 6319 clusters\n",
      "Assigning possible transformations 5000/7076\n",
      "Assigning possible transformations 3000/4101\n",
      "Assigning possible transformations 0/5317\n",
      "Binning with mh_biggest = True\n",
      "Assigning possible transformations 0/6319\n",
      "Assigning possible transformations 5500/7076\n",
      "Assigning possible transformations 3500/4101\n",
      "Assigning possible transformations 500/5317\n",
      "Assigning possible transformations 500/6319\n",
      "Assigning possible transformations 6000/7076\n",
      "Assigning possible transformations 4000/4101\n",
      "Assigning possible transformations 1000/5317\n",
      "Assigning possible transformations 1000/6319\n",
      "Assigning possible transformations 6500/7076\n",
      "701 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 1500/5317\n",
      "Assigning possible transformations 1500/6319\n",
      "Assigning possible transformations 7000/7076\n",
      "Created 5485 clusters\n",
      "Assigning possible transformations 2000/5317\n",
      "Binning with mh_biggest = True\n",
      "Assigning possible transformations 2000/6319\n",
      "1407 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 0/5485\n",
      "Assigning possible transformations 2500/5317\n",
      "Assigning possible transformations 2500/6319\n",
      "Created 5034 clusters\n",
      "Assigning possible transformations 500/5485\n",
      "Assigning possible transformations 3000/5317\n",
      "Binning with mh_biggest = True\n",
      "Assigning possible transformations 3000/6319\n",
      "Assigning possible transformations 0/5034\n",
      "Assigning possible transformations 1000/5485\n",
      "Assigning possible transformations 3500/5317\n",
      "Assigning possible transformations 3500/6319\n",
      "Assigning possible transformations 500/5034\n",
      "Assigning possible transformations 1500/5485\n",
      "Assigning possible transformations 4000/5317\n",
      "Assigning possible transformations 4000/6319\n",
      "Assigning possible transformations 1000/5034\n",
      "Assigning possible transformations 2000/5485\n",
      "Assigning possible transformations 4500/5317\n",
      "Assigning possible transformations 4500/6319\n",
      "Assigning possible transformations 1500/5034\n",
      "Assigning possible transformations 2500/5485\n",
      "Assigning possible transformations 5000/5317\n",
      "Assigning possible transformations 5000/6319\n",
      "Assigning possible transformations 2000/5034\n",
      "Assigning possible transformations 3000/5485\n",
      "1068 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 5500/6319\n",
      "Assigning possible transformations 2500/5034\n",
      "Assigning possible transformations 3500/5485\n",
      "Assigning possible transformations 6000/6319\n",
      "Assigning possible transformations 3000/5034\n",
      "Assigning possible transformations 4000/5485\n",
      "986 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 3500/5034\n",
      "Assigning possible transformations 4500/5485\n",
      "Assigning possible transformations 5000/5485\n",
      "Assigning possible transformations 4000/5034\n",
      "1167 peaks to be re-sampled in stage 1\n",
      "Assigning possible transformations 4500/5034\n",
      "Assigning possible transformations 5000/5034\n",
      "1053 peaks to be re-sampled in stage 1\n"
     ]
    }
   ],
   "source": [
    "combined_list = load_or_create_clustering('std1_pos_clustering.p', input_dir, transformation_file, hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define Experimental Methods</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(selected_data, param_list, hp, match_mode, evaluation_method):\n",
    "    \n",
    "    performances = []\n",
    "    for param in param_list:\n",
    "\n",
    "        # print \"Parameter mass_tol=%f rt_tol=%f\" % (param)\n",
    "        hp.across_file_mass_tol = param[0]\n",
    "        hp.across_file_rt_tol = param[1]\n",
    "        selected_files = [x[0] for x in selected_data]  \n",
    "        selected_clusterings = [x[1] for x in selected_data]            \n",
    "        aligner = Aligner(selected_files, None, transformation_file, \n",
    "                               hp, verbose=False, seed=1234567890)\n",
    "        aligner.run(match_mode, first_stage_clustering_results=selected_clusterings)\n",
    "\n",
    "        res = aligner.evaluate_performance(gt_file, verbose=False, print_TP=True, method=evaluation_method)\n",
    "        output = param+res[0]\n",
    "        performances.append(output)\n",
    "    \n",
    "    df = pd.DataFrame(performances, columns=['mass_tol', 'rt_tol', 'TP', 'FP', 'FN', 'Prec', 'Rec', 'F1', 'Threshold'])\n",
    "    \n",
    "    sorted_df = df.sort_values(['F1', 'mass_tol', 'rt_tol'], ascending=[False, True, True])\n",
    "    best_row = sorted_df.iloc[0]\n",
    "    return df, best_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(selected_data, best_row, hp, match_mode, evaluation_method):\n",
    "\n",
    "    param = (best_row['mass_tol'], best_row['rt_tol'])\n",
    "    hp.across_file_mass_tol = param[0]\n",
    "    hp.across_file_rt_tol = param[1]\n",
    "    selected_files = [x[0] for x in selected_data]\n",
    "    selected_clusterings = [x[1] for x in selected_data]    \n",
    "    aligner = Aligner(selected_files, None, transformation_file, \n",
    "                           hp, verbose=False, seed=1234567890)\n",
    "    aligner.run(match_mode, first_stage_clustering_results=selected_clusterings)\n",
    "\n",
    "    res = aligner.evaluate_performance(gt_file, verbose=False, print_TP=True, method=evaluation_method)\n",
    "    output = param+res[0]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_test(match_mode, training_list, testing_list):\n",
    "    assert len(training_list) == len(testing_list)\n",
    "    n_iter = len(training_list)\n",
    "    exp_results = []\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        training_data = training_list[i]\n",
    "        print \"Iteration %d\" % i\n",
    "        print \"Training on %s\" % [x[0].filename for x in training_data]\n",
    "        training_df, best_training_row = train(training_data, param_list, hp, match_mode, evaluation_method)\n",
    "\n",
    "        testing_data = testing_list[i]\n",
    "        print \"Testing on %s\" % [x[0].filename for x in testing_data]\n",
    "        match_res = test(testing_data, best_training_row, hp, match_mode, evaluation_method)\n",
    "        output = (match_mode,) + match_res\n",
    "        print \"match_mode=%d, mass_tol=%d, rt_tol=%d, tp=%d, fp=%d, fn=%d, prec=%.3f, rec=%.3f, f1=%.3f, th_prob=%.3f\" % output\n",
    "\n",
    "        item = (training_data, training_df, best_training_row, match_res)\n",
    "        exp_results.append(item)\n",
    "        print\n",
    "    return exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_experiment(match_mode, training_list, testing_list, filename):\n",
    "    try:\n",
    "        with gzip.GzipFile(filename, 'rb') as f:        \n",
    "            exp_results = cPickle.load(f)\n",
    "            print \"Loaded from %s\" % filename\n",
    "            return exp_results\n",
    "    except (IOError, EOFError):\n",
    "        exp_results = train_test(match_mode, training_list, testing_list)\n",
    "        with gzip.GzipFile(filename, 'wb') as f:\n",
    "            cPickle.dump(exp_results, f, protocol=cPickle.HIGHEST_PROTOCOL)                        \n",
    "        print \"Saved to %s\" % filename\n",
    "    return exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_or_create_filelist(filename, combined_list, n_iter, n_files):\n",
    "    try:\n",
    "        with gzip.GzipFile(filename, 'rb') as f:        \n",
    "            item_list = cPickle.load(f)\n",
    "            print \"Loaded from %s\" % filename\n",
    "            for item in item_list:\n",
    "                print \"%s\" % [x[0].filename for x in item]\n",
    "            return item_list\n",
    "    except (IOError, EOFError):\n",
    "        item_list = []\n",
    "        for i in range(n_iter):\n",
    "            item = random.sample(combined_list, n_files)\n",
    "            print \"%s\" % [x[0].filename for x in item]\n",
    "            item_list.append(item)\n",
    "        with gzip.GzipFile(filename, 'wb') as f:\n",
    "            cPickle.dump(item_list, f, protocol=cPickle.HIGHEST_PROTOCOL)                    \n",
    "        print \"Saved to %s\" % filename\n",
    "        return item_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Run experiment with 2 random files</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from training_list_2.p\n",
      "['std1-file9.txt', 'std1-file6.txt']\n",
      "['std1-file1.txt', 'std1-file3.txt']\n",
      "['std1-file11.txt', 'std1-file6.txt']\n",
      "['std1-file5.txt', 'std1-file6.txt']\n",
      "['std1-file2.txt', 'std1-file8.txt']\n",
      "['std1-file3.txt', 'std1-file4.txt']\n",
      "['std1-file10.txt', 'std1-file11.txt']\n",
      "['std1-file1.txt', 'std1-file7.txt']\n",
      "['std1-file10.txt', 'std1-file1.txt']\n",
      "['std1-file9.txt', 'std1-file6.txt']\n",
      "['std1-file1.txt', 'std1-file5.txt']\n",
      "['std1-file3.txt', 'std1-file1.txt']\n",
      "['std1-file7.txt', 'std1-file8.txt']\n",
      "['std1-file4.txt', 'std1-file10.txt']\n",
      "['std1-file9.txt', 'std1-file11.txt']\n",
      "['std1-file10.txt', 'std1-file8.txt']\n",
      "['std1-file11.txt', 'std1-file3.txt']\n",
      "['std1-file4.txt', 'std1-file7.txt']\n",
      "['std1-file2.txt', 'std1-file3.txt']\n",
      "['std1-file1.txt', 'std1-file5.txt']\n",
      "['std1-file3.txt', 'std1-file2.txt']\n",
      "['std1-file1.txt', 'std1-file4.txt']\n",
      "['std1-file5.txt', 'std1-file11.txt']\n",
      "['std1-file3.txt', 'std1-file4.txt']\n",
      "['std1-file11.txt', 'std1-file9.txt']\n",
      "['std1-file10.txt', 'std1-file5.txt']\n",
      "['std1-file6.txt', 'std1-file9.txt']\n",
      "['std1-file9.txt', 'std1-file10.txt']\n",
      "['std1-file4.txt', 'std1-file3.txt']\n",
      "['std1-file11.txt', 'std1-file4.txt']\n"
     ]
    }
   ],
   "source": [
    "training_list = load_or_create_filelist('training_list_2.p', combined_list, 30, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['std1-file8.txt', 'std1-file6.txt']\n",
      "['std1-file6.txt', 'std1-file7.txt']\n",
      "['std1-file8.txt', 'std1-file2.txt']\n",
      "['std1-file8.txt', 'std1-file1.txt']\n",
      "['std1-file4.txt', 'std1-file5.txt']\n",
      "['std1-file2.txt', 'std1-file10.txt']\n",
      "['std1-file9.txt', 'std1-file7.txt']\n",
      "['std1-file2.txt', 'std1-file10.txt']\n",
      "['std1-file2.txt', 'std1-file6.txt']\n",
      "['std1-file6.txt', 'std1-file11.txt']\n",
      "['std1-file6.txt', 'std1-file7.txt']\n",
      "['std1-file7.txt', 'std1-file3.txt']\n",
      "['std1-file4.txt', 'std1-file8.txt']\n",
      "['std1-file9.txt', 'std1-file4.txt']\n",
      "['std1-file8.txt', 'std1-file3.txt']\n",
      "['std1-file2.txt', 'std1-file10.txt']\n",
      "['std1-file11.txt', 'std1-file2.txt']\n",
      "['std1-file6.txt', 'std1-file8.txt']\n",
      "['std1-file10.txt', 'std1-file4.txt']\n",
      "['std1-file2.txt', 'std1-file10.txt']\n",
      "['std1-file11.txt', 'std1-file10.txt']\n",
      "['std1-file2.txt', 'std1-file6.txt']\n",
      "['std1-file3.txt', 'std1-file10.txt']\n",
      "['std1-file2.txt', 'std1-file1.txt']\n",
      "['std1-file2.txt', 'std1-file8.txt']\n",
      "['std1-file1.txt', 'std1-file9.txt']\n",
      "['std1-file10.txt', 'std1-file3.txt']\n",
      "['std1-file7.txt', 'std1-file5.txt']\n",
      "['std1-file6.txt', 'std1-file7.txt']\n",
      "['std1-file6.txt', 'std1-file3.txt']\n",
      "Saved to testing_list_2.p\n"
     ]
    }
   ],
   "source": [
    "testing_list = load_or_create_filelist('testing_list_2.p', combined_list, 30, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Training on ['std1-file9.txt', 'std1-file6.txt']\n",
      "Testing on ['std1-file8.txt', 'std1-file6.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=40, tp=114, fp=7, fn=16, prec=0.942, rec=0.877, f1=0.908, th_prob=1.000\n",
      "\n",
      "Iteration 1\n",
      "Training on ['std1-file1.txt', 'std1-file3.txt']\n",
      "Testing on ['std1-file6.txt', 'std1-file7.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=80, tp=24, fp=5, fn=4, prec=0.828, rec=0.857, f1=0.842, th_prob=1.000\n",
      "\n",
      "Iteration 2\n",
      "Training on ['std1-file11.txt', 'std1-file6.txt']\n",
      "Testing on ['std1-file8.txt', 'std1-file2.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=80, tp=119, fp=6, fn=8, prec=0.952, rec=0.937, f1=0.944, th_prob=1.000\n",
      "\n",
      "Iteration 3\n",
      "Training on ['std1-file5.txt', 'std1-file6.txt']\n",
      "Testing on ['std1-file8.txt', 'std1-file1.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=70, tp=120, fp=9, fn=12, prec=0.930, rec=0.909, f1=0.920, th_prob=1.000\n",
      "\n",
      "Iteration 4\n",
      "Training on ['std1-file2.txt', 'std1-file8.txt']\n",
      "Testing on ['std1-file4.txt', 'std1-file5.txt']\n",
      "match_mode=0, mass_tol=6, rt_tol=70, tp=170, fp=6, fn=8, prec=0.966, rec=0.955, f1=0.960, th_prob=1.000\n",
      "\n",
      "Iteration 5\n",
      "Training on ['std1-file3.txt', 'std1-file4.txt']\n",
      "Testing on ['std1-file2.txt', 'std1-file10.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=50, tp=116, fp=10, fn=12, prec=0.921, rec=0.906, f1=0.913, th_prob=1.000\n",
      "\n",
      "Iteration 6\n",
      "Training on ['std1-file10.txt', 'std1-file11.txt']\n",
      "Testing on ['std1-file9.txt', 'std1-file7.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=60, tp=22, fp=6, fn=10, prec=0.786, rec=0.688, f1=0.733, th_prob=1.000\n",
      "\n",
      "Iteration 7\n",
      "Training on ['std1-file1.txt', 'std1-file7.txt']\n",
      "Testing on ['std1-file2.txt', 'std1-file10.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=70, tp=117, fp=10, fn=11, prec=0.921, rec=0.914, f1=0.918, th_prob=1.000\n",
      "\n",
      "Iteration 8\n",
      "Training on ['std1-file10.txt', 'std1-file1.txt']\n",
      "Testing on ['std1-file2.txt', 'std1-file6.txt']\n",
      "match_mode=0, mass_tol=6, rt_tol=60, tp=120, fp=11, fn=10, prec=0.916, rec=0.923, f1=0.920, th_prob=1.000\n",
      "\n",
      "Iteration 9\n",
      "Training on ['std1-file9.txt', 'std1-file6.txt']\n",
      "Testing on ['std1-file6.txt', 'std1-file11.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=40, tp=108, fp=13, fn=18, prec=0.893, rec=0.857, f1=0.874, th_prob=1.000\n",
      "\n",
      "Iteration 10\n",
      "Training on ['std1-file1.txt', 'std1-file5.txt']\n",
      "Testing on ['std1-file6.txt', 'std1-file7.txt']\n",
      "match_mode=0, mass_tol=6, rt_tol=60, tp=21, fp=4, fn=7, prec=0.840, rec=0.750, f1=0.792, th_prob=1.000\n",
      "\n",
      "Iteration 11\n",
      "Training on ['std1-file3.txt', 'std1-file1.txt']\n",
      "Testing on ['std1-file7.txt', 'std1-file3.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=80, tp=27, fp=2, fn=2, prec=0.931, rec=0.931, f1=0.931, th_prob=1.000\n",
      "\n",
      "Iteration 12\n",
      "Training on ['std1-file7.txt', 'std1-file8.txt']\n",
      "Testing on ['std1-file4.txt', 'std1-file8.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=60, tp=144, fp=4, fn=12, prec=0.973, rec=0.923, f1=0.947, th_prob=1.000\n",
      "\n",
      "Iteration 13\n",
      "Training on ['std1-file4.txt', 'std1-file10.txt']\n",
      "Testing on ['std1-file9.txt', 'std1-file4.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=60, tp=150, fp=10, fn=10, prec=0.938, rec=0.938, f1=0.938, th_prob=1.000\n",
      "\n",
      "Iteration 14\n",
      "Training on ['std1-file9.txt', 'std1-file11.txt']\n",
      "Testing on ['std1-file8.txt', 'std1-file3.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=100, tp=138, fp=6, fn=6, prec=0.958, rec=0.958, f1=0.958, th_prob=1.000\n",
      "\n",
      "Iteration 15\n",
      "Training on ['std1-file10.txt', 'std1-file8.txt']\n",
      "Testing on ['std1-file2.txt', 'std1-file10.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=80, tp=117, fp=10, fn=11, prec=0.921, rec=0.914, f1=0.918, th_prob=1.000\n",
      "\n",
      "Iteration 16\n",
      "Training on ['std1-file11.txt', 'std1-file3.txt']\n",
      "Testing on ['std1-file11.txt', 'std1-file2.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=60, tp=116, fp=15, fn=14, prec=0.885, rec=0.892, f1=0.889, th_prob=1.000\n",
      "\n",
      "Iteration 17\n",
      "Training on ['std1-file4.txt', 'std1-file7.txt']\n",
      "Testing on ['std1-file6.txt', 'std1-file8.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=80, tp=121, fp=7, fn=9, prec=0.945, rec=0.931, f1=0.938, th_prob=1.000\n",
      "\n",
      "Iteration 18\n",
      "Training on ['std1-file2.txt', 'std1-file3.txt']\n",
      "Testing on ['std1-file10.txt', 'std1-file4.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=80, tp=158, fp=3, fn=3, prec=0.981, rec=0.981, f1=0.981, th_prob=1.000\n",
      "\n",
      "Iteration 19\n",
      "Training on ['std1-file1.txt', 'std1-file5.txt']\n",
      "Testing on ['std1-file2.txt', 'std1-file10.txt']\n",
      "match_mode=0, mass_tol=6, rt_tol=60, tp=118, fp=10, fn=10, prec=0.922, rec=0.922, f1=0.922, th_prob=1.000\n",
      "\n",
      "Iteration 20\n",
      "Training on ['std1-file3.txt', 'std1-file2.txt']\n",
      "Testing on ['std1-file11.txt', 'std1-file10.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=80, tp=128, fp=8, fn=6, prec=0.941, rec=0.955, f1=0.948, th_prob=1.000\n",
      "\n",
      "Iteration 21\n",
      "Training on ['std1-file1.txt', 'std1-file4.txt']\n",
      "Testing on ['std1-file2.txt', 'std1-file6.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=90, tp=120, fp=11, fn=10, prec=0.916, rec=0.923, f1=0.920, th_prob=1.000\n",
      "\n",
      "Iteration 22\n",
      "Training on ['std1-file5.txt', 'std1-file11.txt']\n",
      "Testing on ['std1-file3.txt', 'std1-file10.txt']\n",
      "match_mode=0, mass_tol=6, rt_tol=60, tp=144, fp=4, fn=4, prec=0.973, rec=0.973, f1=0.973, th_prob=1.000\n",
      "\n",
      "Iteration 23\n",
      "Training on ['std1-file3.txt', 'std1-file4.txt']\n",
      "Testing on ['std1-file2.txt', 'std1-file1.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=50, tp=147, fp=7, fn=6, prec=0.955, rec=0.961, f1=0.958, th_prob=1.000\n",
      "\n",
      "Iteration 24\n",
      "Training on ['std1-file11.txt', 'std1-file9.txt']\n",
      "Testing on ['std1-file2.txt', 'std1-file8.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=100, tp=119, fp=6, fn=8, prec=0.952, rec=0.937, f1=0.944, th_prob=1.000\n",
      "\n",
      "Iteration 25\n",
      "Training on ['std1-file10.txt', 'std1-file5.txt']\n",
      "Testing on ['std1-file1.txt', 'std1-file9.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=80, tp=147, fp=4, fn=4, prec=0.974, rec=0.974, f1=0.974, th_prob=1.000\n",
      "\n",
      "Iteration 26\n",
      "Training on ['std1-file6.txt', 'std1-file9.txt']\n",
      "Testing on ['std1-file10.txt', 'std1-file3.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=40, tp=144, fp=4, fn=4, prec=0.973, rec=0.973, f1=0.973, th_prob=1.000\n",
      "\n",
      "Iteration 27\n",
      "Training on ['std1-file9.txt', 'std1-file10.txt']\n",
      "Testing on ['std1-file7.txt', 'std1-file5.txt']\n",
      "match_mode=0, mass_tol=6, rt_tol=60, tp=25, fp=3, fn=8, prec=0.893, rec=0.758, f1=0.820, th_prob=1.000\n",
      "\n",
      "Iteration 28\n",
      "Training on ['std1-file4.txt', 'std1-file3.txt']\n",
      "Testing on ['std1-file6.txt', 'std1-file7.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=50, tp=15, fp=4, fn=13, prec=0.789, rec=0.536, f1=0.638, th_prob=1.000\n",
      "\n",
      "Iteration 29\n",
      "Training on ['std1-file11.txt', 'std1-file4.txt']\n",
      "Testing on ['std1-file6.txt', 'std1-file3.txt']\n",
      "match_mode=0, mass_tol=3, rt_tol=60, tp=125, fp=14, fn=14, prec=0.899, rec=0.899, f1=0.899, th_prob=1.000\n",
      "\n",
      "Saved to res_match_feature_2.p\n"
     ]
    }
   ],
   "source": [
    "exp_results_1a = run_experiment(0, training_list, testing_list, 'res_match_feature_2.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Training on ['std1-file9.txt', 'std1-file6.txt']\n",
      "Testing on ['std1-file8.txt', 'std1-file6.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=40, tp=112, fp=2, fn=18, prec=0.982, rec=0.862, f1=0.918, th_prob=1.000\n",
      "\n",
      "Iteration 1\n",
      "Training on ['std1-file1.txt', 'std1-file3.txt']\n",
      "Testing on ['std1-file6.txt', 'std1-file7.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=80, tp=27, fp=1, fn=1, prec=0.964, rec=0.964, f1=0.964, th_prob=1.000\n",
      "\n",
      "Iteration 2\n",
      "Training on ['std1-file11.txt', 'std1-file6.txt']\n",
      "Testing on ['std1-file8.txt', 'std1-file2.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=80, tp=120, fp=3, fn=7, prec=0.976, rec=0.945, f1=0.960, th_prob=1.000\n",
      "\n",
      "Iteration 3\n",
      "Training on ['std1-file5.txt', 'std1-file6.txt']\n",
      "Testing on ['std1-file8.txt', 'std1-file1.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=60, tp=119, fp=7, fn=13, prec=0.944, rec=0.902, f1=0.922, th_prob=1.000\n",
      "\n",
      "Iteration 4\n",
      "Training on ['std1-file2.txt', 'std1-file8.txt']\n",
      "Testing on ['std1-file4.txt', 'std1-file5.txt']\n",
      "match_mode=1, mass_tol=6, rt_tol=70, tp=157, fp=6, fn=21, prec=0.963, rec=0.882, f1=0.921, th_prob=1.000\n",
      "\n",
      "Iteration 5\n",
      "Training on ['std1-file3.txt', 'std1-file4.txt']\n",
      "Testing on ['std1-file2.txt', 'std1-file10.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=50, tp=120, fp=3, fn=8, prec=0.976, rec=0.938, f1=0.956, th_prob=1.000\n",
      "\n",
      "Iteration 6\n",
      "Training on ['std1-file10.txt', 'std1-file11.txt']\n",
      "Testing on ['std1-file9.txt', 'std1-file7.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=50, tp=21, fp=1, fn=11, prec=0.955, rec=0.656, f1=0.778, th_prob=1.000\n",
      "\n",
      "Iteration 7\n",
      "Training on ['std1-file1.txt', 'std1-file7.txt']\n",
      "Testing on ['std1-file2.txt', 'std1-file10.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=70, tp=120, fp=3, fn=8, prec=0.976, rec=0.938, f1=0.956, th_prob=1.000\n",
      "\n",
      "Iteration 8\n",
      "Training on ['std1-file10.txt', 'std1-file1.txt']\n",
      "Testing on ['std1-file2.txt', 'std1-file6.txt']\n",
      "match_mode=1, mass_tol=6, rt_tol=60, tp=125, fp=4, fn=5, prec=0.969, rec=0.962, f1=0.965, th_prob=1.000\n",
      "\n",
      "Iteration 9\n",
      "Training on ['std1-file9.txt', 'std1-file6.txt']\n",
      "Testing on ['std1-file6.txt', 'std1-file11.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=40, tp=111, fp=6, fn=15, prec=0.949, rec=0.881, f1=0.914, th_prob=1.000\n",
      "\n",
      "Iteration 10\n",
      "Training on ['std1-file1.txt', 'std1-file5.txt']\n",
      "Testing on ['std1-file6.txt', 'std1-file7.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=60, tp=24, fp=1, fn=4, prec=0.960, rec=0.857, f1=0.906, th_prob=1.000\n",
      "\n",
      "Iteration 11\n",
      "Training on ['std1-file3.txt', 'std1-file1.txt']\n",
      "Testing on ['std1-file7.txt', 'std1-file3.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=80, tp=26, fp=2, fn=3, prec=0.929, rec=0.897, f1=0.912, th_prob=1.000\n",
      "\n",
      "Iteration 12\n",
      "Training on ['std1-file7.txt', 'std1-file8.txt']\n",
      "Testing on ['std1-file4.txt', 'std1-file8.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=70, tp=140, fp=4, fn=16, prec=0.972, rec=0.897, f1=0.933, th_prob=1.000\n",
      "\n",
      "Iteration 13\n",
      "Training on ['std1-file4.txt', 'std1-file10.txt']\n",
      "Testing on ['std1-file9.txt', 'std1-file4.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=60, tp=143, fp=3, fn=17, prec=0.979, rec=0.894, f1=0.935, th_prob=1.000\n",
      "\n",
      "Iteration 14\n",
      "Training on ['std1-file9.txt', 'std1-file11.txt']\n",
      "Testing on ['std1-file8.txt', 'std1-file3.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=100, tp=139, fp=4, fn=5, prec=0.972, rec=0.965, f1=0.969, th_prob=1.000\n",
      "\n",
      "Iteration 15\n",
      "Training on ['std1-file10.txt', 'std1-file8.txt']\n",
      "Testing on ['std1-file2.txt', 'std1-file10.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=80, tp=120, fp=3, fn=8, prec=0.976, rec=0.938, f1=0.956, th_prob=1.000\n",
      "\n",
      "Iteration 16\n",
      "Training on ['std1-file11.txt', 'std1-file3.txt']\n",
      "Testing on ['std1-file11.txt', 'std1-file2.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=50, tp=124, fp=3, fn=6, prec=0.976, rec=0.954, f1=0.965, th_prob=1.000\n",
      "\n",
      "Iteration 17\n",
      "Training on ['std1-file4.txt', 'std1-file7.txt']\n",
      "Testing on ['std1-file6.txt', 'std1-file8.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=80, tp=120, fp=2, fn=10, prec=0.984, rec=0.923, f1=0.952, th_prob=1.000\n",
      "\n",
      "Iteration 18\n",
      "Training on ['std1-file2.txt', 'std1-file3.txt']\n",
      "Testing on ['std1-file10.txt', 'std1-file4.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=80, tp=147, fp=1, fn=14, prec=0.993, rec=0.913, f1=0.951, th_prob=1.000\n",
      "\n",
      "Iteration 19\n",
      "Training on ['std1-file1.txt', 'std1-file5.txt']\n",
      "Testing on ['std1-file2.txt', 'std1-file10.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=60, tp=120, fp=3, fn=8, prec=0.976, rec=0.938, f1=0.956, th_prob=1.000\n",
      "\n",
      "Iteration 20\n",
      "Training on ['std1-file3.txt', 'std1-file2.txt']\n",
      "Testing on ['std1-file11.txt', 'std1-file10.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=80, tp=126, fp=5, fn=8, prec=0.962, rec=0.940, f1=0.951, th_prob=1.000\n",
      "\n",
      "Iteration 21\n",
      "Training on ['std1-file1.txt', 'std1-file4.txt']\n",
      "Testing on ['std1-file2.txt', 'std1-file6.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=90, tp=125, fp=4, fn=5, prec=0.969, rec=0.962, f1=0.965, th_prob=1.000\n",
      "\n",
      "Iteration 22\n",
      "Training on ['std1-file5.txt', 'std1-file11.txt']\n",
      "Testing on ['std1-file3.txt', 'std1-file10.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=60, tp=138, fp=1, fn=10, prec=0.993, rec=0.932, f1=0.962, th_prob=1.000\n",
      "\n",
      "Iteration 23\n",
      "Training on ['std1-file3.txt', 'std1-file4.txt']\n",
      "Testing on ['std1-file2.txt', 'std1-file1.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=50, tp=150, fp=3, fn=3, prec=0.980, rec=0.980, f1=0.980, th_prob=1.000\n",
      "\n",
      "Iteration 24\n",
      "Training on ['std1-file11.txt', 'std1-file9.txt']\n",
      "Testing on ['std1-file2.txt', 'std1-file8.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=100, tp=120, fp=3, fn=7, prec=0.976, rec=0.945, f1=0.960, th_prob=1.000\n",
      "\n",
      "Iteration 25\n",
      "Training on ['std1-file10.txt', 'std1-file5.txt']\n",
      "Testing on ['std1-file1.txt', 'std1-file9.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=70, tp=144, fp=4, fn=7, prec=0.973, rec=0.954, f1=0.963, th_prob=1.000\n",
      "\n",
      "Iteration 26\n",
      "Training on ['std1-file6.txt', 'std1-file9.txt']\n",
      "Testing on ['std1-file10.txt', 'std1-file3.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=40, tp=138, fp=1, fn=10, prec=0.993, rec=0.932, f1=0.962, th_prob=1.000\n",
      "\n",
      "Iteration 27\n",
      "Training on ['std1-file9.txt', 'std1-file10.txt']\n",
      "Testing on ['std1-file7.txt', 'std1-file5.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=60, tp=27, fp=1, fn=6, prec=0.964, rec=0.818, f1=0.885, th_prob=1.000\n",
      "\n",
      "Iteration 28\n",
      "Training on ['std1-file4.txt', 'std1-file3.txt']\n",
      "Testing on ['std1-file6.txt', 'std1-file7.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=50, tp=18, fp=1, fn=10, prec=0.947, rec=0.643, f1=0.766, th_prob=1.000\n",
      "\n",
      "Iteration 29\n",
      "Training on ['std1-file11.txt', 'std1-file4.txt']\n",
      "Testing on ['std1-file6.txt', 'std1-file3.txt']\n",
      "match_mode=1, mass_tol=3, rt_tol=60, tp=127, fp=5, fn=12, prec=0.962, rec=0.914, f1=0.937, th_prob=1.000\n",
      "\n",
      "Saved to res_match_cluster_2.p\n"
     ]
    }
   ],
   "source": [
    "exp_results_1b = run_experiment(1, training_list, testing_list, 'res_match_cluster_2.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(<discretisation.models.PeakData object at 0x7f23cf22a750>, <adduct_cluster.AdductCluster object at 0x7f23cec61690>), (<discretisation.models.PeakData object at 0x7f23d5210e90>, <adduct_cluster.AdductCluster object at 0x7f23ca66e490>)],     mass_tol  rt_tol   TP  FP  FN      Prec       Rec        F1  Threshold\n",
      "0          3      10  117   3  18  0.975000  0.866667  0.917647          1\n",
      "1          3      20  127   3   8  0.976923  0.940741  0.958491          1\n",
      "2          3      30  129   4   6  0.969925  0.955556  0.962687          1\n",
      "3          3      40  131   5   4  0.963235  0.970370  0.966790          1\n",
      "4          3      50  131   5   4  0.963235  0.970370  0.966790          1\n",
      "5          3      60  131   5   4  0.963235  0.970370  0.966790          1\n",
      "6          3      70  131   5   4  0.963235  0.970370  0.966790          1\n",
      "7          3      80  131   5   4  0.963235  0.970370  0.966790          1\n",
      "8          3      90  131   5   4  0.963235  0.970370  0.966790          1\n",
      "9          3     100  131   5   4  0.963235  0.970370  0.966790          1\n",
      "10         6      10  117   3  18  0.975000  0.866667  0.917647          1\n",
      "11         6      20  127   3   8  0.976923  0.940741  0.958491          1\n",
      "12         6      30  129   4   6  0.969925  0.955556  0.962687          1\n",
      "13         6      40  131   5   4  0.963235  0.970370  0.966790          1\n",
      "14         6      50  131   5   4  0.963235  0.970370  0.966790          1\n",
      "15         6      60  131   5   4  0.963235  0.970370  0.966790          1\n",
      "16         6      70  131   5   4  0.963235  0.970370  0.966790          1\n",
      "17         6      80  131   5   4  0.963235  0.970370  0.966790          1\n",
      "18         6      90  131   5   4  0.963235  0.970370  0.966790          1\n",
      "19         6     100  131   5   4  0.963235  0.970370  0.966790          1\n",
      "20         9      10  117   3  18  0.975000  0.866667  0.917647          1\n",
      "21         9      20  127   3   8  0.976923  0.940741  0.958491          1\n",
      "22         9      30  129   4   6  0.969925  0.955556  0.962687          1\n",
      "23         9      40  131   5   4  0.963235  0.970370  0.966790          1\n",
      "24         9      50  131   5   4  0.963235  0.970370  0.966790          1\n",
      "25         9      60  131   5   4  0.963235  0.970370  0.966790          1\n",
      "26         9      70  131   5   4  0.963235  0.970370  0.966790          1\n",
      "27         9      80  131   5   4  0.963235  0.970370  0.966790          1\n",
      "28         9      90  131   5   4  0.963235  0.970370  0.966790          1\n",
      "29         9     100  131   5   4  0.963235  0.970370  0.966790          1, mass_tol       3.000000\n",
      "rt_tol        40.000000\n",
      "TP           131.000000\n",
      "FP             5.000000\n",
      "FN             4.000000\n",
      "Prec           0.963235\n",
      "Rec            0.970370\n",
      "F1             0.966790\n",
      "Threshold      1.000000\n",
      "Name: 3, dtype: float64, (3.0, 40.0, 114.0, 7.0, 16.0, 0.9421487603305785, 0.8769230769230769, 0.9083665338645418, 1.0))\n"
     ]
    }
   ],
   "source": [
    "print exp_results_1a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-56-d1e8b4854525>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-56-d1e8b4854525>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    <h2>Run experiment with 4 random files</h2>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<h2>Run experiment with 4 random files</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_list = load_or_create_filelist('training_list_4.p', combined_list, 30, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_list = load_or_create_filelist('testing_list_4.p', combined_list, 30, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_results_2a = run_experiment(0, training_list, testing_list, 'res_match_feature_4.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_results_2b = run_experiment(1, training_list, testing_list, 'res_match_cluster_4.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<h2>Run experiment with 6 random files</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_list = load_or_create_filelist('training_list_6.p', combined_list, 30, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_list = load_or_create_filelist('testing_list_6.p', combined_list, 30, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_results_3a = run_experiment(0, training_list, testing_list, 'res_match_feature_6.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_results_3b = run_experiment(1, training_list, testing_list, 'res_match_cluster_6.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
