{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beer3 Positive Results\n",
    "============"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. LDA\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "basedir = '../'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "from lda_for_fragments import Ms2Lda\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_topics = 100\n",
    "n_samples = 100\n",
    "\n",
    "fragment_filename = basedir + 'input/Beer_3_T10_POS_fragments.csv'\n",
    "neutral_loss_filename = basedir + 'input/Beer_3_T10_POS_losses.csv'\n",
    "mzdiff_filename = None\n",
    "\n",
    "ms1_filename = basedir + 'input/Beer_3_T10_POS_ms1.csv'\n",
    "ms2_filename = basedir + 'input/Beer_3_T10_POS_ms2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape (856, 1664)\n",
      "Fitting model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5cf36149820d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mms2lda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m ms2lda.run_lda(df, 100, 200, 100, 1, \n\u001b[1;32m----> 5\u001b[1;33m                0.1, 0.01, use_own_model=False, use_inline=False)\n\u001b[0m",
      "\u001b[1;32m/home/joewandy/git/metabolomics_tools/justin/lda_for_fragments.py\u001b[0m in \u001b[0;36mrun_lda\u001b[1;34m(self, df, n_topics, n_samples, n_burn, n_thin, alpha, beta, use_own_model, use_inline)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m\"DONE. Time=\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/lda/lda.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \"\"\"\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/lda/lda.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    221\u001b[0m                 \u001b[1;31m# keep track of loglikelihoods for monitoring convergence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloglikelihoods_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sample_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrands\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m         \u001b[0mll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloglikelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"<{}> log likelihood: {:.0f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/lda/lda.pyc\u001b[0m in \u001b[0;36m_sample_topics\u001b[1;34m(self, rands)\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[0meta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         lda._lda._sample_topics(self.WS, self.DS, self.ZS, self.nzw_, self.ndz_, self.nz_,\n\u001b[1;32m--> 282\u001b[1;33m                                 alpha, eta, rands)\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ms2lda.write_results('beer3_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_fragments = ms2lda.model.topic_word_\n",
    "n_top_frags = 20\n",
    "for i,topic_dist in enumerate(topic_fragments):\n",
    "    topic_f = np.array(ms2lda.data.columns.values)[np.argsort(topic_dist)][:-n_top_frags:-1]\n",
    "    out_string = 'Topic {}: {}'.format(i, ', '.join(topic_f.astype('str')))\n",
    "    print(out_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(ms2lda.model.loglikelihoods_)\n",
    "plt.plot(ms2lda.model.loglikelihoods_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. PCA\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use PCA to project the vector of topics for each parent peak to lower-dimensional space for visualisation purposes. First ensure the variables are scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = ms2lda.docdf.transpose() # topics x documents matrix\n",
    "# df = fragments_topicdf.transpose() # topics x words matrix\n",
    "print df.shape\n",
    "\n",
    "# normalise and scale the variables\n",
    "scaled_mat = preprocessing.scale(df, axis=0)\n",
    "# print scaled_mat.mean(axis=0)\n",
    "# print scaled_mat.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(scaled_mat)\n",
    "X_r = pca.transform(scaled_mat)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(X_r[:, 0], bins=30)\n",
    "plt.title('First transformed variable')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(X_r[:, 1], bins=30)\n",
    "plt.title('Second transformed variable')\n",
    "plt.show()\n",
    "\n",
    "print np.argmax(np.abs(pca.components_[0, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the explained variances by the first few principal components are too low ..??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_r.shape\n",
    "print np.sum(pca.explained_variance_ratio_)\n",
    "print('explained variance by the principal components: %s' % str(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_r[:, 0], X_r[:, 1])\n",
    "plt.xlabel('1st princomp')\n",
    "plt.ylabel('2nd princomp')\n",
    "plt.title('Projected parent peaks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Network\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to put the parent peaks on a network too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = topicdf.transpose() # topic x terms matrix\n",
    "df = ms2lda.docdf.transpose() # documents x topic matrix\n",
    "print df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create adjacency matrix A\n",
    "\n",
    "# first compute euclidean distance between the topics\n",
    "from scipy.spatial.distance import cdist\n",
    "A = cdist(df, df, 'euclidean')\n",
    "print A.shape\n",
    "\n",
    "# crudely convert to similarities\n",
    "maxval = A.max()\n",
    "A = 1-(A/maxval)\n",
    "plt.figure()\n",
    "plt.hist(A)\n",
    "plt.title('Histogram of values in the adjacency matrix')\n",
    "plt.show()\n",
    "\n",
    "# set a threshold for the similarity values for the network graph\n",
    "for i in xrange(A.shape[0]):\n",
    "    for j in xrange(A.shape[1]):\n",
    "        if A[i, j] < 0.75:\n",
    "            A[i, j] = 0\n",
    "\n",
    "plt.figure()\n",
    "plt.matshow(A)\n",
    "plt.colorbar()\n",
    "plt.title('Adjacency matrix after thresholding', y=1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = [('len', float)]\n",
    "A = A.view(dt)\n",
    "G = nx.from_numpy_matrix(A)\n",
    "pos = nx.spring_layout(G, k=0.01, iterations=20)\n",
    "nx.draw(G, pos, node_size=10, with_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see some connected components in the network graph. Below we print the largest top-20 components. \n",
    "\n",
    "Parent peaks in the same component are connected in the graph above, i.e. they form some sort of clusters, suggesting they share topics in common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = sorted(nx.connected_components(G), key = len, reverse=True)\n",
    "counter = 1\n",
    "for comp in components:\n",
    "    if counter > 20:\n",
    "        break\n",
    "    print \"Component \" + str(counter)\n",
    "    print \"==============\"\n",
    "    idx = np.array(comp)-1 # nodes are indexed from 1 .. N\n",
    "    ms1_rows = ms2lda.ms1.iloc[idx]\n",
    "    print ms1_rows[['peakID', 'mz', 'rt', 'intensity']].to_string(index=False, justify='left')\n",
    "    counter += 1\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Document-Topics Distribution\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the document-topic distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = ms2lda.docdf.transpose()\n",
    "print df.shape\n",
    "plt.pcolor(df, norm=None, cmap='Blues')\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Topics')\n",
    "plt.ylabel('Parent peaks')\n",
    "plt.title('Documents-topics distributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing useful here ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
