{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial run\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "basedir = '../'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "from lda_for_fragments import Ms2Lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Feature Extraction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to generate the input matrices, described below in option (a) and (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>a. Loading Existing Input Matrices</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either you separately run the feature extraction pipeline in R, producing the input matrices below .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input files\n",
      "Data shape (61, 2985)\n"
     ]
    }
   ],
   "source": [
    "fragment_filename = '/home/joewandy/isabel/isabelpos_fragments.csv'\n",
    "neutral_loss_filename = '/home/joewandy/isabel/isabelpos_losses.csv'\n",
    "mzdiff_filename = None\n",
    "ms1_filename = '/home/joewandy/isabel/isabelpos_ms1.csv'\n",
    "ms2_filename = '/home/joewandy/isabel/isabelpos_ms2.csv'\n",
    "ms2lda = Ms2Lda.lcms_data_from_R(fragment_filename, neutral_loss_filename, mzdiff_filename, \n",
    "                             ms1_filename, ms2_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>b. Running the Feature Extraction Pipeline</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can call the run_feature_extraction in MS2-LDA. This is a wrapper method to the feature extraction pipeline, written in R. It takes as input the full scan and fragmentation files (defined in config_filename) and produces the various count matrices used as input to LDA.\n",
    "\n",
    "Note: On the R side, the main entry point for the feature extraction pipeline is the <i>script_folder/startFeatureExtraction.R</i>, which loads RMassBank as a requirement. RMassBank relies on rJava, which unfortunately can be really rather fiddly to configure. The following is a common problem when configuring rJava : http://stackoverflow.com/questions/12872699/error-unable-to-load-installed-packages-just-now. \n",
    "\n",
    "Note2: This also steps depends on http://rpy.sourceforge.net/, which doesn't seem to be well-supported in Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path to the folder containing the R scripts used for feature extraction \n",
    "script_folder = '/Users/simon/git/metabolomics_tools/justin/R'\n",
    "\n",
    "# path to the configuration file for feature extraction\n",
    "# config_filename = os.path.join(script_folder, 'config.yml')\n",
    "config_filename = '/Users/simon/git/metabolomics_tools/justin/isabel/config.yml'\n",
    "\n",
    "# too many warning messages printed from R\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# run the feature extraction pipeline, this will run for a long time!!\n",
    "ms2lda = Ms2Lda.run_feature_extraction(script_folder, config_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Analysis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>a. Run LDA</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been loaded by performing either step 1(a) or 1(b), we're now ready to run LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "CGS LDA initialising\n",
      ".......\n",
      "Using Numba for LDA sampling\n",
      "Preparing words\n",
      "Preparing Z matrix\n",
      "DONE\n",
      "Sample 1   Log likelihood = -72493.065 \n",
      "Sample 2   Log likelihood = -66294.165 \n",
      "Sample 3   Log likelihood = -60491.404 \n",
      "Sample 4   Log likelihood = -56909.086 \n",
      "Sample 5   Log likelihood = -54987.119 \n",
      "Sample 6   Log likelihood = -53772.525 \n",
      "Sample 7   Log likelihood = -53165.675 \n",
      "Sample 8   Log likelihood = -52743.725 \n",
      "Sample 9   Log likelihood = -52427.084 \n",
      "Sample 10   Log likelihood = -52038.428 \n",
      "Sample 11   Log likelihood = -51671.552 \n",
      "Sample 12   Log likelihood = -51538.521 \n",
      "Sample 13   Log likelihood = -51306.650 \n",
      "Sample 14   Log likelihood = -51106.953 \n",
      "Sample 15   Log likelihood = -50816.168 \n",
      "Sample 16   Log likelihood = -50754.881 \n",
      "Sample 17   Log likelihood = -50690.395 \n",
      "Sample 18   Log likelihood = -50584.156 \n",
      "Sample 19   Log likelihood = -50363.528 \n",
      "Sample 20   Log likelihood = -50257.572 \n",
      "Sample 21   Log likelihood = -50412.248 \n",
      "Sample 22   Log likelihood = -50178.189 \n",
      "Sample 23   Log likelihood = -50168.890 \n",
      "Sample 24   Log likelihood = -49995.902 \n",
      "Sample 25   Log likelihood = -49979.130 \n",
      "Sample 26   Log likelihood = -49768.149 \n",
      "Sample 27   Log likelihood = -49880.029 \n",
      "Sample 28   Log likelihood = -49925.814 \n",
      "Sample 29   Log likelihood = -49768.714 \n",
      "Sample 30   Log likelihood = -49750.005 \n",
      "Sample 31   Log likelihood = -49802.721 \n",
      "Sample 32   Log likelihood = -49976.142 \n",
      "Sample 33   Log likelihood = -49878.604 \n",
      "Sample 34   Log likelihood = -49867.084 \n",
      "Sample 35   Log likelihood = -49924.991 \n",
      "Sample 36   Log likelihood = -49908.035 \n",
      "Sample 37   Log likelihood = -49761.933 \n",
      "Sample 38   Log likelihood = -49685.293 \n",
      "Sample 39   Log likelihood = -49714.472 \n",
      "Sample 40   Log likelihood = -49814.013 \n",
      "Sample 41   Log likelihood = -49757.940 \n",
      "Sample 42   Log likelihood = -49753.850 \n",
      "Sample 43   Log likelihood = -49604.015 \n",
      "Sample 44   Log likelihood = -49615.822 \n",
      "Sample 45   Log likelihood = -49564.848 \n",
      "Sample 46   Log likelihood = -49512.691 \n",
      "Sample 47   Log likelihood = -49832.807 \n",
      "Sample 48   Log likelihood = -49678.924 \n",
      "Sample 49   Log likelihood = -49665.447 \n",
      "Sample 50   Log likelihood = -49655.676 \n",
      "Sample 51   Log likelihood = -49597.644 \n",
      "Sample 52   Log likelihood = -49625.082 \n",
      "Sample 53   Log likelihood = -49615.127 \n",
      "Sample 54   Log likelihood = -49704.942 \n",
      "Sample 55   Log likelihood = -49640.361 \n",
      "Sample 56   Log likelihood = -49705.604 \n",
      "Sample 57   Log likelihood = -49633.416 \n",
      "Sample 58   Log likelihood = -49721.304 \n",
      "Sample 59   Log likelihood = -49584.739 \n",
      "Sample 60   Log likelihood = -49559.392 \n",
      "Sample 61   Log likelihood = -49619.238 \n",
      "Sample 62   Log likelihood = -49637.332 \n",
      "Sample 63   Log likelihood = -49538.889 \n",
      "Sample 64   Log likelihood = -49539.059 \n",
      "Sample 65   Log likelihood = -49554.356 \n",
      "Sample 66   Log likelihood = -49645.317 \n",
      "Sample 67   Log likelihood = -49740.150 \n",
      "Sample 68   Log likelihood = -49595.582 \n",
      "Sample 69   Log likelihood = -49670.331 \n",
      "Sample 70   Log likelihood = -49686.418 \n",
      "Sample 71   Log likelihood = -49563.358 \n",
      "Sample 72   Log likelihood = -49626.249 \n",
      "Sample 73   Log likelihood = -49648.034 \n",
      "Sample 74   Log likelihood = -49697.964 \n",
      "Sample 75   Log likelihood = -49623.723 \n",
      "Sample 76   Log likelihood = -49763.946 \n",
      "Sample 77   Log likelihood = -49703.566 \n",
      "Sample 78   Log likelihood = -49717.204 \n",
      "Sample 79   Log likelihood = -49733.137 \n",
      "Sample 80   Log likelihood = -49812.012 \n",
      "Sample 81   Log likelihood = -49773.518 \n",
      "Sample 82   Log likelihood = -49657.157 \n",
      "Sample 83   Log likelihood = -49488.637 \n",
      "Sample 84   Log likelihood = -49635.802 \n",
      "Sample 85   Log likelihood = -49598.886 \n",
      "Sample 86   Log likelihood = -49706.979 \n",
      "Sample 87   Log likelihood = -49656.822 \n",
      "Sample 88   Log likelihood = -49791.479 \n",
      "Sample 89   Log likelihood = -49722.613 \n",
      "Sample 90   Log likelihood = -49699.958 \n",
      "Sample 91   Log likelihood = -49594.055 \n",
      "Sample 92   Log likelihood = -49727.716 \n",
      "Sample 93   Log likelihood = -49723.236 \n",
      "Sample 94   Log likelihood = -49632.422 \n",
      "Sample 95   Log likelihood = -49745.675 \n",
      "Sample 96   Log likelihood = -49603.732 \n",
      "Sample 97   Log likelihood = -49601.902 \n",
      "Sample 98   Log likelihood = -49631.741 \n",
      "Sample 99   Log likelihood = -49696.280 \n",
      "Sample 100   Log likelihood = -49683.870 \n",
      "Sample 101   Log likelihood = -49699.798 \n",
      "Sample 102   Log likelihood = -49662.272 \n",
      "Sample 103   Log likelihood = -49670.678 \n",
      "Sample 104   Log likelihood = -49683.037 \n",
      "Sample 105   Log likelihood = -49663.440 \n",
      "Sample 106   Log likelihood = -49630.113 \n",
      "Sample 107   Log likelihood = -49602.307 \n",
      "Sample 108   Log likelihood = -49648.743 \n",
      "Sample 109   Log likelihood = -49628.559 \n",
      "Sample 110   Log likelihood = -49582.666 \n",
      "Sample 111   Log likelihood = -49616.904 \n",
      "Sample 112   Log likelihood = -49633.711 \n",
      "Sample 113   Log likelihood = -49515.150 \n",
      "Sample 114   Log likelihood = -49559.692 \n",
      "Sample 115   Log likelihood = -49755.581 \n",
      "Sample 116   Log likelihood = -49700.937 \n",
      "Sample 117   Log likelihood = -49632.413 \n",
      "Sample 118   Log likelihood = -49884.414 \n",
      "Sample 119   Log likelihood = -49609.118 \n",
      "Sample 120   Log likelihood = -49626.965 \n",
      "Sample 121   Log likelihood = -49582.996 \n",
      "Sample 122   Log likelihood = -49594.797 \n",
      "Sample 123   Log likelihood = -49657.972 \n",
      "Sample 124   Log likelihood = -49596.552 \n",
      "Sample 125   Log likelihood = -49581.480 \n",
      "Sample 126   Log likelihood = -49625.004 \n",
      "Sample 127   Log likelihood = -49481.180 \n",
      "Sample 128   Log likelihood = -49549.622 \n",
      "Sample 129   Log likelihood = -49550.339 \n",
      "Sample 130   Log likelihood = -49564.165 \n",
      "Sample 131   Log likelihood = -49715.933 \n",
      "Sample 132   Log likelihood = -49739.991 \n",
      "Sample 133   Log likelihood = -49753.173 \n",
      "Sample 134   Log likelihood = -49553.206 \n",
      "Sample 135   Log likelihood = -49649.018 \n",
      "Sample 136   Log likelihood = -49645.591 \n",
      "Sample 137   Log likelihood = -49804.131 \n",
      "Sample 138   Log likelihood = -49661.680 \n",
      "Sample 139   Log likelihood = -49566.620 \n",
      "Sample 140   Log likelihood = -49605.015 \n",
      "Sample 141   Log likelihood = -49581.028 \n",
      "Sample 142   Log likelihood = -49672.462 \n",
      "Sample 143   Log likelihood = -49622.972 \n",
      "Sample 144   Log likelihood = -49606.455 \n",
      "Sample 145   Log likelihood = -49656.054 \n",
      "Sample 146   Log likelihood = -49635.672 \n",
      "Sample 147   Log likelihood = -49666.868 \n",
      "Sample 148   Log likelihood = -49611.063 \n",
      "Sample 149   Log likelihood = -49679.879 \n",
      "Sample 150   Log likelihood = -49766.267 \n",
      "Sample 151   Log likelihood = -49700.885 \n",
      "Sample 152   Log likelihood = -49725.778 \n",
      "Sample 153   Log likelihood = -49648.320 \n",
      "Sample 154   Log likelihood = -49501.865 \n",
      "Sample 155   Log likelihood = -49562.304 \n",
      "Sample 156   Log likelihood = -49475.343 \n",
      "Sample 157   Log likelihood = -49457.068 \n",
      "Sample 158   Log likelihood = -49565.057 \n",
      "Sample 159   Log likelihood = -49547.197 \n",
      "Sample 160   Log likelihood = -49736.034 \n",
      "Sample 161   Log likelihood = -49570.154 \n",
      "Sample 162   Log likelihood = -49592.315 \n",
      "Sample 163   Log likelihood = -49612.369 \n",
      "Sample 164   Log likelihood = -49522.157 \n",
      "Sample 165   Log likelihood = -49639.379 \n",
      "Sample 166   Log likelihood = -49682.443 \n",
      "Sample 167   Log likelihood = -49572.500 \n",
      "Sample 168   Log likelihood = -49534.248 \n",
      "Sample 169   Log likelihood = -49782.060 \n",
      "Sample 170   Log likelihood = -49581.254 \n",
      "Sample 171   Log likelihood = -49548.602 \n",
      "Sample 172   Log likelihood = -49670.610 \n",
      "Sample 173   Log likelihood = -49643.213 \n",
      "Sample 174   Log likelihood = -49686.969 \n",
      "Sample 175   Log likelihood = -49614.486 \n",
      "Sample 176   Log likelihood = -49513.538 \n",
      "Sample 177   Log likelihood = -49594.305 \n",
      "Sample 178   Log likelihood = -49675.464 \n",
      "Sample 179   Log likelihood = -49740.380 \n",
      "Sample 180   Log likelihood = -49727.291 \n",
      "Sample 181   Log likelihood = -49585.875 \n",
      "Sample 182   Log likelihood = -49616.377 \n",
      "Sample 183   Log likelihood = -49643.519 \n",
      "Sample 184   Log likelihood = -49512.819 \n",
      "Sample 185   Log likelihood = -49500.687 \n",
      "Sample 186   Log likelihood = -49562.653 \n",
      "Sample 187   Log likelihood = -49518.361 \n",
      "Sample 188   Log likelihood = -49550.135 \n",
      "Sample 189   Log likelihood = -49606.866 \n",
      "Sample 190   Log likelihood = -49480.585 \n",
      "Sample 191   Log likelihood = -49482.971 \n",
      "Sample 192   Log likelihood = -49648.890 \n",
      "Sample 193   Log likelihood = -49713.288 \n",
      "Sample 194   Log likelihood = -49545.380 \n",
      "Sample 195   Log likelihood = -49597.799 \n",
      "Sample 196   Log likelihood = -49611.395 \n",
      "Sample 197   Log likelihood = -49679.582 \n",
      "Sample 198   Log likelihood = -49593.477 \n",
      "Sample 199   Log likelihood = -49700.184 \n",
      "Sample 200   Log likelihood = -49690.398 \n",
      "Sample 201   Log likelihood = -49535.679 \n",
      "Sample 202   Log likelihood = -49605.182 \n",
      "Sample 203   Log likelihood = -49585.409 \n",
      "Sample 204   Log likelihood = -49741.992 \n",
      "Sample 205   Log likelihood = -49819.627 \n",
      "Sample 206   Log likelihood = -49627.397 \n",
      "Sample 207   Log likelihood = -49735.278 \n",
      "Sample 208   Log likelihood = -49521.741 \n",
      "Sample 209   Log likelihood = -49652.518 \n",
      "Sample 210   Log likelihood = -49582.801 \n",
      "Sample 211   Log likelihood = -49671.714 \n",
      "Sample 212   Log likelihood = -49654.533 \n",
      "Sample 213   Log likelihood = -49526.703 \n",
      "Sample 214   Log likelihood = -49523.461 \n",
      "Sample 215   Log likelihood = -49655.456 \n",
      "Sample 216   Log likelihood = -49703.292 \n",
      "Sample 217   Log likelihood = -49678.703 \n",
      "Sample 218   Log likelihood = -49590.754 \n",
      "Sample 219   Log likelihood = -49593.860 \n",
      "Sample 220   Log likelihood = -49538.189 \n",
      "Sample 221   Log likelihood = -49630.509 \n",
      "Sample 222   Log likelihood = -49666.368 \n",
      "Sample 223   Log likelihood = -49525.442 \n",
      "Sample 224   Log likelihood = -49567.980 \n",
      "Sample 225   Log likelihood = -49680.735 \n",
      "Sample 226   Log likelihood = -49666.717 \n",
      "Sample 227   Log likelihood = -49575.125 \n",
      "Sample 228   Log likelihood = -49581.170 \n",
      "Sample 229   Log likelihood = -49547.222 \n",
      "Sample 230   Log likelihood = -49720.787 \n",
      "Sample 231   Log likelihood = -49614.052 \n",
      "Sample 232   Log likelihood = -49716.895 \n",
      "Sample 233   Log likelihood = -49766.404 \n",
      "Sample 234   Log likelihood = -49772.042 \n",
      "Sample 235   Log likelihood = -49698.805 \n",
      "Sample 236   Log likelihood = -49649.592 \n",
      "Sample 237   Log likelihood = -49693.957 \n",
      "Sample 238   Log likelihood = -49741.182 \n",
      "Sample 239   Log likelihood = -49722.855 \n",
      "Sample 240   Log likelihood = -49578.007 \n",
      "Sample 241   Log likelihood = -49560.241 \n",
      "Sample 242   Log likelihood = -49633.199 \n",
      "Sample 243   Log likelihood = -49595.702 \n",
      "Sample 244   Log likelihood = -49665.344 \n",
      "Sample 245   Log likelihood = -49709.684 \n",
      "Sample 246   Log likelihood = -49696.527 \n",
      "Sample 247   Log likelihood = -49776.423 \n",
      "Sample 248   Log likelihood = -49717.742 \n",
      "Sample 249   Log likelihood = -49704.095 \n",
      "Sample 250   Log likelihood = -49645.454 \n",
      "Sample 251   Log likelihood = -49733.034 \n",
      "Sample 252   Log likelihood = -49604.580 \n",
      "Sample 253   Log likelihood = -49628.102 \n",
      "Sample 254   Log likelihood = -49658.393 \n",
      "Sample 255   Log likelihood = -49589.839 \n",
      "Sample 256   Log likelihood = -49644.088 \n",
      "Sample 257   Log likelihood = -49611.338 \n",
      "Sample 258   Log likelihood = -49594.877 \n",
      "Sample 259   Log likelihood = -49613.374 \n",
      "Sample 260   Log likelihood = -49558.509 \n",
      "Sample 261   Log likelihood = -49627.049 \n",
      "Sample 262   Log likelihood = -49664.200 \n",
      "Sample 263   Log likelihood = -49731.868 \n",
      "Sample 264   Log likelihood = -49562.077 \n",
      "Sample 265   Log likelihood = -49655.849 \n",
      "Sample 266   Log likelihood = -49727.326 \n",
      "Sample 267   Log likelihood = -49730.096 \n",
      "Sample 268   Log likelihood = -49516.004 \n",
      "Sample 269   Log likelihood = -49550.414 \n",
      "Sample 270   Log likelihood = -49672.109 \n",
      "Sample 271   Log likelihood = -49512.070 \n",
      "Sample 272   Log likelihood = -49516.911 \n",
      "Sample 273   Log likelihood = -49676.103 \n",
      "Sample 274   Log likelihood = -49538.295 \n",
      "Sample 275   Log likelihood = -49601.252 \n",
      "Sample 276   Log likelihood = -49657.846 \n",
      "Sample 277   Log likelihood = -49608.932 \n",
      "Sample 278   Log likelihood = -49642.887 \n",
      "Sample 279   Log likelihood = -49663.649 \n",
      "Sample 280   Log likelihood = -49679.548 \n",
      "Sample 281   Log likelihood = -49499.808 \n",
      "Sample 282   Log likelihood = -49480.721 \n",
      "Sample 283   Log likelihood = -49662.821 \n",
      "Sample 284   Log likelihood = -49614.584 \n",
      "Sample 285   Log likelihood = -49667.136 \n",
      "Sample 286   Log likelihood = -49495.274 \n",
      "Sample 287   Log likelihood = -49587.351 \n",
      "Sample 288   Log likelihood = -49605.174 \n",
      "Sample 289   Log likelihood = -49632.204 \n",
      "Sample 290   Log likelihood = -49715.430 \n",
      "Sample 291   Log likelihood = -49537.976 \n",
      "Sample 292   Log likelihood = -49558.840 \n",
      "Sample 293   Log likelihood = -49583.869 \n",
      "Sample 294   Log likelihood = -49509.300 \n",
      "Sample 295   Log likelihood = -49477.273 \n",
      "Sample 296   Log likelihood = -49530.139 \n",
      "Sample 297   Log likelihood = -49556.201 \n",
      "Sample 298   Log likelihood = -49502.358 \n",
      "Sample 299   Log likelihood = -49627.901 \n",
      "Sample 300   Log likelihood = -49647.396 \n",
      "Sample 301   Log likelihood = -49681.291 \n",
      "Sample 302   Log likelihood = -49605.921 \n",
      "Sample 303   Log likelihood = -49720.762 \n",
      "Sample 304   Log likelihood = -49614.016 \n",
      "Sample 305   Log likelihood = -49610.057 \n",
      "Sample 306   Log likelihood = -49549.687 \n",
      "Sample 307   Log likelihood = -49541.923 \n",
      "Sample 308   Log likelihood = -49625.090 \n",
      "Sample 309   Log likelihood = -49675.116 \n",
      "Sample 310   Log likelihood = -49649.738 \n",
      "Sample 311   Log likelihood = -49557.599 \n",
      "Sample 312   Log likelihood = -49609.063 \n",
      "Sample 313   Log likelihood = -49664.249 \n",
      "Sample 314   Log likelihood = -49610.156 \n",
      "Sample 315   Log likelihood = -49599.526 \n",
      "Sample 316   Log likelihood = -49653.590 \n",
      "Sample 317   Log likelihood = -49655.822 \n",
      "Sample 318   Log likelihood = -49578.707 \n",
      "Sample 319   Log likelihood = -49597.576 \n",
      "Sample 320   Log likelihood = -49553.558 \n",
      "Sample 321   Log likelihood = -49583.468 \n",
      "Sample 322   Log likelihood = -49637.305 \n",
      "Sample 323   Log likelihood = -49695.733 \n",
      "Sample 324   Log likelihood = -49642.484 \n",
      "Sample 325   Log likelihood = -49517.678 \n",
      "Sample 326   Log likelihood = -49545.536 \n",
      "Sample 327   Log likelihood = -49561.852 \n",
      "Sample 328   Log likelihood = -49602.677 \n",
      "Sample 329   Log likelihood = -49633.249 \n",
      "Sample 330   Log likelihood = -49647.007 \n",
      "Sample 331   Log likelihood = -49641.574 \n",
      "Sample 332   Log likelihood = -49545.284 \n",
      "Sample 333   Log likelihood = -49507.135 \n",
      "Sample 334   Log likelihood = -49522.900 \n",
      "Sample 335   Log likelihood = -49612.650 \n",
      "Sample 336   Log likelihood = -49654.749 \n",
      "Sample 337   Log likelihood = -49503.192 \n",
      "Sample 338   Log likelihood = -49652.224 \n",
      "Sample 339   Log likelihood = -49681.072 \n",
      "Sample 340   Log likelihood = -49723.184 \n",
      "Sample 341   Log likelihood = -49461.407 \n",
      "Sample 342   Log likelihood = -49584.315 \n",
      "Sample 343   Log likelihood = -49674.350 \n",
      "Sample 344   Log likelihood = -49708.903 \n",
      "Sample 345   Log likelihood = -49784.170 \n",
      "Sample 346   Log likelihood = -49551.604 \n",
      "Sample 347   Log likelihood = -49610.748 \n",
      "Sample 348   Log likelihood = -49595.194 \n",
      "Sample 349   Log likelihood = -49657.089 \n",
      "Sample 350   Log likelihood = -49627.438 \n",
      "Sample 351   Log likelihood = -49561.492 \n",
      "Sample 352   Log likelihood = -49567.776 \n",
      "Sample 353   Log likelihood = -49529.477 \n",
      "Sample 354   Log likelihood = -49612.404 \n",
      "Sample 355   Log likelihood = -49759.783 \n",
      "Sample 356   Log likelihood = -49570.989 \n",
      "Sample 357   Log likelihood = -49629.132 \n",
      "Sample 358   Log likelihood = -49560.635 \n",
      "Sample 359   Log likelihood = -49560.707 \n",
      "Sample 360   Log likelihood = -49541.734 \n",
      "Sample 361   Log likelihood = -49502.575 \n",
      "Sample 362   Log likelihood = -49590.951 \n",
      "Sample 363   Log likelihood = -49510.258 \n",
      "Sample 364   Log likelihood = -49511.069 \n",
      "Sample 365   Log likelihood = -49745.015 \n",
      "Sample 366   Log likelihood = -49747.874 \n",
      "Sample 367   Log likelihood = -49653.393 \n",
      "Sample 368   Log likelihood = -49502.135 \n",
      "Sample 369   Log likelihood = -49577.135 \n",
      "Sample 370   Log likelihood = -49648.242 \n",
      "Sample 371   Log likelihood = -49630.768 \n",
      "Sample 372   Log likelihood = -49656.642 \n",
      "Sample 373   Log likelihood = -49606.015 \n",
      "Sample 374   Log likelihood = -49541.604 \n",
      "Sample 375   Log likelihood = -49538.393 \n",
      "Sample 376   Log likelihood = -49533.902 \n",
      "Sample 377   Log likelihood = -49636.764 \n",
      "Sample 378   Log likelihood = -49663.439 \n",
      "Sample 379   Log likelihood = -49677.557 \n",
      "Sample 380   Log likelihood = -49643.780 \n",
      "Sample 381   Log likelihood = -49631.484 \n",
      "Sample 382   Log likelihood = -49523.418 \n",
      "Sample 383   Log likelihood = -49644.102 \n",
      "Sample 384   Log likelihood = -49489.034 \n",
      "Sample 385   Log likelihood = -49548.566 \n",
      "Sample 386   Log likelihood = -49469.706 \n",
      "Sample 387   Log likelihood = -49544.904 \n",
      "Sample 388   Log likelihood = -49517.435 \n",
      "Sample 389   Log likelihood = -49601.587 \n",
      "Sample 390   Log likelihood = -49525.437 \n",
      "Sample 391   Log likelihood = -49564.571 \n",
      "Sample 392   Log likelihood = -49658.730 \n",
      "Sample 393   Log likelihood = -49681.631 \n",
      "Sample 394   Log likelihood = -49654.331 \n",
      "Sample 395   Log likelihood = -49638.673 \n",
      "Sample 396   Log likelihood = -49574.074 \n",
      "Sample 397   Log likelihood = -49483.909 \n",
      "Sample 398   Log likelihood = -49530.000 \n",
      "Sample 399   Log likelihood = -49628.638 \n",
      "Sample 400   Log likelihood = -49698.887 \n",
      "Sample 401   Log likelihood = -49771.478 \n",
      "Sample 402   Log likelihood = -49606.147 \n",
      "Sample 403   Log likelihood = -49529.595 \n",
      "Sample 404   Log likelihood = -49519.987 \n",
      "Sample 405   Log likelihood = -49487.666 \n",
      "Sample 406   Log likelihood = -49531.521 \n",
      "Sample 407   Log likelihood = -49618.174 \n",
      "Sample 408   Log likelihood = -49710.209 \n",
      "Sample 409   Log likelihood = -49636.075 \n",
      "Sample 410   Log likelihood = -49414.949 \n",
      "Sample 411   Log likelihood = -49491.880 \n",
      "Sample 412   Log likelihood = -49504.058 \n",
      "Sample 413   Log likelihood = -49659.234 \n",
      "Sample 414   Log likelihood = -49657.610 \n",
      "Sample 415   Log likelihood = -49497.795 \n",
      "Sample 416   Log likelihood = -49562.184 \n",
      "Sample 417   Log likelihood = -49535.507 \n",
      "Sample 418   Log likelihood = -49496.372 \n",
      "Sample 419   Log likelihood = -49506.556 \n",
      "Sample 420   Log likelihood = -49581.700 \n",
      "Sample 421   Log likelihood = -49504.744 \n",
      "Sample 422   Log likelihood = -49579.640 \n",
      "Sample 423   Log likelihood = -49653.970 \n",
      "Sample 424   Log likelihood = -49625.872 \n",
      "Sample 425   Log likelihood = -49597.858 \n",
      "Sample 426   Log likelihood = -49615.574 \n",
      "Sample 427   Log likelihood = -49614.556 \n",
      "Sample 428   Log likelihood = -49637.619 \n",
      "Sample 429   Log likelihood = -49465.324 \n",
      "Sample 430   Log likelihood = -49618.870 \n",
      "Sample 431   Log likelihood = -49445.216 \n",
      "Sample 432   Log likelihood = -49512.611 \n",
      "Sample 433   Log likelihood = -49543.229 \n",
      "Sample 434   Log likelihood = -49577.767 \n",
      "Sample 435   Log likelihood = -49669.990 \n",
      "Sample 436   Log likelihood = -49598.883 \n",
      "Sample 437   Log likelihood = -49606.362 \n",
      "Sample 438   Log likelihood = -49738.715 \n",
      "Sample 439   Log likelihood = -49708.397 \n",
      "Sample 440   Log likelihood = -49581.510 \n",
      "Sample 441   Log likelihood = -49744.987 \n",
      "Sample 442   Log likelihood = -49668.727 \n",
      "Sample 443   Log likelihood = -49545.121 \n",
      "Sample 444   Log likelihood = -49681.455 \n",
      "Sample 445   Log likelihood = -49631.886 \n",
      "Sample 446   Log likelihood = -49614.729 \n",
      "Sample 447   Log likelihood = -49523.706 \n",
      "Sample 448   Log likelihood = -49554.832 \n",
      "Sample 449   Log likelihood = -49596.949 \n",
      "Sample 450   Log likelihood = -49592.938 \n",
      "Sample 451   Log likelihood = -49730.573 \n",
      "Sample 452   Log likelihood = -49523.588 \n",
      "Sample 453   Log likelihood = -49512.488 \n",
      "Sample 454   Log likelihood = -49446.800 \n",
      "Sample 455   Log likelihood = -49577.860 \n",
      "Sample 456   Log likelihood = -49524.780 \n",
      "Sample 457   Log likelihood = -49501.485 \n",
      "Sample 458   Log likelihood = -49570.170 \n",
      "Sample 459   Log likelihood = -49624.633 \n",
      "Sample 460   Log likelihood = -49587.805 \n",
      "Sample 461   Log likelihood = -49625.033 \n",
      "Sample 462   Log likelihood = -49623.768 \n",
      "Sample 463   Log likelihood = -49599.450 \n",
      "Sample 464   Log likelihood = -49580.137 \n",
      "Sample 465   Log likelihood = -49535.308 \n",
      "Sample 466   Log likelihood = -49650.382 \n",
      "Sample 467   Log likelihood = -49563.836 \n",
      "Sample 468   Log likelihood = -49616.867 \n",
      "Sample 469   Log likelihood = -49504.952 \n",
      "Sample 470   Log likelihood = -49665.882 \n",
      "Sample 471   Log likelihood = -49695.094 \n",
      "Sample 472   Log likelihood = -49494.269 \n",
      "Sample 473   Log likelihood = -49533.728 \n",
      "Sample 474   Log likelihood = -49597.284 \n",
      "Sample 475   Log likelihood = -49638.175 \n",
      "Sample 476   Log likelihood = -49624.669 \n",
      "Sample 477   Log likelihood = -49643.957 \n",
      "Sample 478   Log likelihood = -49613.308 \n",
      "Sample 479   Log likelihood = -49610.462 \n",
      "Sample 480   Log likelihood = -49568.623 \n",
      "Sample 481   Log likelihood = -49504.701 \n",
      "Sample 482   Log likelihood = -49488.519 \n",
      "Sample 483   Log likelihood = -49371.871 \n",
      "Sample 484   Log likelihood = -49512.793 \n",
      "Sample 485   Log likelihood = -49540.359 \n",
      "Sample 486   Log likelihood = -49591.659 \n",
      "Sample 487   Log likelihood = -49591.750 \n",
      "Sample 488   Log likelihood = -49488.736 \n",
      "Sample 489   Log likelihood = -49564.840 \n",
      "Sample 490   Log likelihood = -49663.921 \n",
      "Sample 491   Log likelihood = -49554.576 \n",
      "Sample 492   Log likelihood = -49542.828 \n",
      "Sample 493   Log likelihood = -49602.053 \n",
      "Sample 494   Log likelihood = -49679.842 \n",
      "Sample 495   Log likelihood = -49641.456 \n",
      "Sample 496   Log likelihood = -49580.434 \n",
      "Sample 497   Log likelihood = -49443.167 \n",
      "Sample 498   Log likelihood = -49618.319 \n",
      "Sample 499   Log likelihood = -49454.309 \n",
      "Sample 500   Log likelihood = -49519.660 \n",
      "S=1, using only the last sample.\n",
      "DONE. Time=4.96714282036\n"
     ]
    }
   ],
   "source": [
    "### all the parameters you need to specify to run LDA ###\n",
    "\n",
    "n_topics = 10 # 300 - 400 topics from cross-validation\n",
    "n_samples = 500 # 100 is probably okay for testing. For manuscript, use > 500-1000.\n",
    "n_burn = 0 # if 0 then we only use the last sample\n",
    "n_thin = 1 # every n-th sample to use for averaging after burn-in. Ignored if n_burn = 0\n",
    "alpha = 50.0/n_topics # hyper-parameter for document-topic distributions\n",
    "beta = 0.1 # hyper-parameter for topic-word distributions\n",
    "\n",
    "ms2lda.run_lda(n_topics, n_samples, n_burn, n_thin, alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>b. (Optional) In-silico Annotation using SIRIUS</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of visualisation in step 3(c), we can annotate the MS1 and MS2 peaks using [SIRIUS](http://bio.informatik.uni-jena.de/software/sirius/), an in-silico fragmentation tool written in Java. At the moment, each parent MS1 peak and its associated MS2 spectra are run through SIRIUS separately. Isotopic information, which can be used to improve annotation, is not used yet.\n",
    "\n",
    "If you run this annotation step before saving the project in step (c) below, the annotation information will be saved into the ms1 and ms2 peak info too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sirius_platform = 'orbitrap'\n",
    "ms2lda.annotate_with_sirius(sirius_platform, mode='pos') # mode is either 'pos' or 'neg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>c. (Optional) Saving Project</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the whole project so we don't have to re-run everything the next time .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# leave the message parameter out if nothing to say\n",
    "ms2lda.save_project('/home/joewandy/isabel/isabel.project', message=\"Initial run on Isabel data by Joe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Results</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>(Optional) Resuming Project</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you saved the project in step (2c), you can resume from here the next time you load this notebook .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "basedir = '../'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "from lda_for_fragments import Ms2Lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ms2lda = Ms2Lda.resume_from('/home/joewandy/isabel/isabel.project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>a. Thresholding</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of visualisation only, we threshold the document-topic and topic-word distributions produced by LDA, so we can say which topics are used in which documents, and which words 'belongs' to a topic. This needs to be done before step (b) and (c) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Thresholding the doc_topic and topic_word matrices\n",
    "ms2lda.do_thresholding(th_doc_topic=0.05, th_topic_word=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>b. Print Results</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print which fragment/loss words occur with probability above the threshold in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: fragment_124.99979 (0.159640203363), fragment_86.09653 (0.0509190457567), fragment_124.9977 (0.0485725459523), fragment_98.98431 (0.038404380133), fragment_104.10691 (0.03684004693), fragment_86.09506 (0.0344935471255), fragment_125.00208 (0.0321470473211), fragment_184.07376 (0.0313648807196), loss_58.06617 (0.0305827141181), fragment_104.10883 (0.0196323816973), fragment_60.08105 (0.0157215486899), fragment_98.98251 (0.0157215486899), fragment_71.07309 (0.0118107156824), \n",
      "\n",
      "Topic 1: loss_16.9656 (0.0677408959072), loss_1.98299 (0.0664518208186), loss_18.97982 (0.0361585562359), loss_17.99744 (0.0335804060587), loss_1.99306 (0.0284241057042), loss_0.93298 (0.0258459555269), fragment_126.05476 (0.021978730261), loss_19.96557 (0.0213341927167), fragment_72.08081 (0.0174669674509), fragment_126.05678 (0.0168224299065), loss_45.9923 (0.0161778923622), fragment_80.04946 (0.0148888172736), loss_0.97097 (0.013599742185), loss_45.9938 (0.0129552046407), loss_1.95394 (0.0129552046407), fragment_72.08197 (0.0123106670964), fragment_82.06521 (0.0123106670964), fragment_58.06542 (0.011666129552), loss_43.97767 (0.0110215920077), loss_44.99798 (0.0103770544634), \n",
      "\n",
      "Topic 2: loss_0.99303 (0.0956844547564), fragment_136.06219 (0.0789791183295), fragment_84.04431 (0.0576334106729), fragment_84.04569 (0.0455684454756), fragment_114.06633 (0.0446403712297), loss_17.00203 (0.0270069605568), fragment_136.06444 (0.0223665893271), fragment_74.02368 (0.0186542923434), loss_41.99837 (0.0149419953596), fragment_136.0596 (0.0149419953596), loss_45.98195 (0.0149419953596), fragment_114.06458 (0.0140139211137), fragment_114.06805 (0.0140139211137), loss_59.9977 (0.0121577726218), fragment_119.03561 (0.0103016241299), \n",
      "\n",
      "Topic 3: fragment_118.08595 (0.0298743601675), fragment_58.06542 (0.0270823638902), fragment_134.09654 (0.0242903676128), fragment_56.04981 (0.0224290367613), fragment_74.06006 (0.0214983713355), fragment_74.05889 (0.0177757096324), loss_46.9773 (0.014983713355), fragment_58.99529 (0.0140530479293), fragment_59.04947 (0.0140530479293), loss_144.99284 (0.0131223825035), fragment_107.08553 (0.0131223825035), fragment_145.08858 (0.0131223825035), loss_131.01444 (0.0121917170777), \n",
      "\n",
      "Topic 4: fragment_113.10729 (0.0502816180236), loss_46.02984 (0.0400409626216), fragment_56.96512 (0.0359447004608), loss_0.90061 (0.0277521761393), loss_41.0119 (0.0216077828981), fragment_97.07607 (0.0216077828981), loss_1.93251 (0.0175115207373), fragment_99.05541 (0.0175115207373), fragment_96.04439 (0.0175115207373), fragment_113.10921 (0.0154633896569), loss_45.99967 (0.0134152585765), loss_46.03145 (0.0134152585765), loss_17.98647 (0.0134152585765), fragment_58.99529 (0.0123911930364), fragment_100.0217 (0.0123911930364), fragment_109.03956 (0.0113671274962), fragment_72.04442 (0.0113671274962), fragment_86.99001 (0.0113671274962), loss_78.00831 (0.010343061956), loss_44.01419 (0.010343061956), \n",
      "\n",
      "Topic 5: fragment_70.06523 (0.0795297932712), loss_17.95006 (0.0616943656263), loss_15.93447 (0.0600729631131), fragment_90.05504 (0.0535873530604), loss_1.91843 (0.0438589379814), loss_60.04532 (0.0349412241589), fragment_72.08081 (0.0300770166194), fragment_114.05513 (0.0260235103364), fragment_88.07568 (0.0244021078233), loss_104.03472 (0.0227807053101), fragment_132.07686 (0.0154843940008), fragment_70.06412 (0.0146736927442), loss_1.95394 (0.013052290231), loss_41.03956 (0.0122415889745), loss_133.05039 (0.0114308877179), loss_41.01431 (0.0106201864613), \n",
      "\n",
      "Topic 6: loss_14.99471 (0.0851211072664), loss_44.00241 (0.0653484923381), loss_45.9923 (0.0426099851705), fragment_152.05643 (0.03173504696), loss_43.99965 (0.0297577854671), fragment_181.05983 (0.0267918932279), fragment_150.06651 (0.0228373702422), loss_45.98983 (0.0228373702422), fragment_120.01158 (0.019871478003), loss_42.99373 (0.0178942165101), fragment_74.00591 (0.0149283242709), loss_45.9938 (0.0119624320316), loss_30.99102 (0.0119624320316), \n",
      "\n",
      "Topic 7: fragment_84.08079 (0.0413905486149), loss_24.05377 (0.0413905486149), fragment_121.96614 (0.0305268875611), loss_22.05458 (0.0261814231396), fragment_72.08081 (0.0240086909288), loss_0.88484 (0.0174904942966), fragment_123.96438 (0.0174904942966), loss_65.99751 (0.0142313959804), fragment_123.05536 (0.0142313959804), fragment_89.0599 (0.0131450298751), loss_33.99179 (0.0120586637697), fragment_56.04981 (0.0109722976643), \n",
      "\n",
      "Topic 8: fragment_131.11824 (0.0523133302794), loss_45.16538 (0.0495648190563), loss_45.16362 (0.0394869445717), loss_45.16713 (0.0376546037563), loss_45.16187 (0.0229958772332), fragment_239.1059 (0.0220797068255), loss_45.16888 (0.0220797068255), fragment_152.03784 (0.0211635364178), fragment_128.09478 (0.0138341731562), fragment_131.11581 (0.0129180027485), fragment_99.09196 (0.0129180027485), fragment_112.09964 (0.0120018323408), fragment_88.07568 (0.0120018323408), fragment_131.12055 (0.0110856619331), fragment_132.12151 (0.0110856619331), fragment_128.09279 (0.0101694915254), \n",
      "\n",
      "Topic 9: loss_38.98422 (0.0679330619629), fragment_137.04591 (0.0480325644505), fragment_120.08086 (0.0398914518318), loss_43.96674 (0.038986883763), loss_46.01285 (0.0208955223881), fragment_118.03447 (0.0208955223881), fragment_137.04816 (0.0163726820443), fragment_85.02843 (0.0163726820443), fragment_120.08258 (0.0145635459068), loss_43.96516 (0.0145635459068), fragment_137.04359 (0.0136589778381), loss_119.06152 (0.0127544097693), fragment_120.07884 (0.0127544097693), loss_43.96845 (0.0127544097693), fragment_103.05438 (0.0100407055631), \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ms2lda.print_topic_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the output to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ms2lda.write_results('beer3_test_method3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>c. Visualisation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visualisation module is provided to explore the results. This can be run in either interactive (in the browser) or non-interactive (directly plotting all results in this notebook, which can be a lot output!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Set Visualisation Parameters</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If True, an interactive visualisation is shown in a separate tab. \n",
    "# You need to interrupt the kernel to stop it once you're done with it (from the menu above, Kernel > Interrupt).\n",
    "interactive=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used for highlighting 'consistent' topic fragments/losses during plotting. \n",
    "# A value of 0.50 means the fragment (or loss) word occurs at least half of the plotted documents of the topic.\n",
    "consistency=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used for graph visualisation in the interactive mode only. \n",
    "# Specifies the 'special' nodes to be coloured differently.\n",
    "# special_nodes = [\n",
    "#     ('doc_372.18877_540.996', '#CC0000'), # maroon\n",
    "#     ('doc_291.66504_547_239', 'gold'),\n",
    "#     ('doc_308.17029_289.13', 'green'),\n",
    "#     ('topic_244', '#CC0000'), # maroon\n",
    "#     ('topic_98', 'aqua'),\n",
    "#     ('topic_293', '#ff1493') # deep pink\n",
    "# ]\n",
    "special_nodes = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Run Visualisation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking topics ...\n",
      " - topic 0 h-index = 7\n",
      " - topic 1 h-index = 5\n",
      " - topic 2 h-index = 6\n",
      " - topic 3 h-index = 5\n",
      " - topic 4 h-index = 4\n",
      " - topic 5 h-index = 5\n",
      " - topic 6 h-index = 2\n",
      " - topic 7 h-index = 4\n",
      " - topic 8 h-index = 4\n",
      " - topic 9 h-index = 2\n",
      "DONE!\n",
      "\n",
      "Generating plots for topic 0 h-index=7, degree=26\n",
      "Generating plots for topic 2 h-index=6, degree=29\n",
      "Generating plots for topic 1 h-index=5, degree=31\n",
      "Generating plots for topic 3 h-index=5, degree=30\n",
      "Generating plots for topic 5 h-index=5, degree=30\n",
      "Generating plots for topic 4 h-index=4, degree=22\n",
      "Generating plots for topic 7 h-index=4, degree=27\n",
      "Generating plots for topic 8 h-index=4, degree=21\n",
      "Generating plots for topic 6 h-index=2, degree=23\n",
      "Generating plots for topic 9 h-index=2, degree=26\n",
      "Using visualisation script defined in /LDAvis.js\n",
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8889/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [01/Oct/2015 17:26:48] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2015 17:26:48] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2015 17:26:48] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2015 17:26:48] \"GET /LDAvis.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2015 17:26:48] \"GET /images/graph_example.jpg HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2015 17:26:48] \"GET /images/default_logo.png HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2015 17:26:50] \"GET /topic?circle_id=ldavis_el124141400914198526481705957299-topic2&action=load HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2015 17:26:52] \"GET /topic?circle_id=ldavis_el124141400914198526481705957299-topic2&action=set HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2015 17:26:53] \"GET /topic?circle_id=ldavis_el124141400914198526481705957299-topic2&action=load HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2015 17:26:53] \"GET /images/default_logo.png HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2015 17:26:54] \"GET /topic?circle_id=ldavis_el124141400914198526481705957299-topic3&action=load HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2015 17:26:54] \"GET /images/default_logo.png HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2015 17:26:55] \"GET /topic?circle_id=ldavis_el124141400914198526481705957299-topic3&action=set HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2015 17:26:57] \"GET /topic?circle_id=ldavis_el124141400914198526481705957299-topic3&action=load HTTP/1.1\" 200 -\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/joewandy/anaconda/lib/python2.7/SocketServer.py\", line 295, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/joewandy/anaconda/lib/python2.7/SocketServer.py\", line 321, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/joewandy/anaconda/lib/python2.7/SocketServer.py\", line 334, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/joewandy/anaconda/lib/python2.7/SocketServer.py\", line 657, in __init__\n",
      "    self.finish()\n",
      "  File \"/home/joewandy/anaconda/lib/python2.7/SocketServer.py\", line 716, in finish\n",
      "    self.wfile.close()\n",
      "  File \"/home/joewandy/anaconda/lib/python2.7/socket.py\", line 283, in close\n",
      "    self.flush()\n",
      "  File \"/home/joewandy/anaconda/lib/python2.7/socket.py\", line 307, in flush\n",
      "    self._sock.sendall(view[write_offset:write_offset+buffer_size])\n",
      "error: [Errno 32] Broken pipe\n",
      "127.0.0.1 - - [01/Oct/2015 17:26:57] \"GET /images/default_logo.png HTTP/1.1\" 200 -\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/joewandy/anaconda/lib/python2.7/SocketServer.py\", line 295, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/joewandy/anaconda/lib/python2.7/SocketServer.py\", line 321, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/joewandy/anaconda/lib/python2.7/SocketServer.py\", line 334, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/joewandy/anaconda/lib/python2.7/SocketServer.py\", line 657, in __init__\n",
      "    self.finish()\n",
      "  File \"/home/joewandy/anaconda/lib/python2.7/SocketServer.py\", line 716, in finish\n",
      "    self.wfile.close()\n",
      "  File \"/home/joewandy/anaconda/lib/python2.7/socket.py\", line 283, in close\n",
      "    self.flush()\n",
      "  File \"/home/joewandy/anaconda/lib/python2.7/socket.py\", line 307, in flush\n",
      "    self._sock.sendall(view[write_offset:write_offset+buffer_size])\n",
      "error: [Errno 32] Broken pipe\n",
      "127.0.0.1 - - [01/Oct/2015 17:26:58] \"GET /topic?circle_id=ldavis_el124141400914198526481705957299-topic1&action=load HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "ms2lda.plot_lda_fragments(consistency=consistency, interactive=interactive, to_highlight=special_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
