{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Notebook for Persistent Topics\n",
    "===================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how we can **(1)** run LDA on one data, **(2)** save some of the topics from the first LDA run and **(3)** use the saved topics when running LDA again on a new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "basedir = '../'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "from lda_for_fragments import Ms2Lda\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initial LDA on Beer 3\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_topics = 125 # number of topics\n",
    "n_samples = 400 # how many samples to get during Gibbs sampling\n",
    "n_burn = 200 # no. of burn-in samples to discard\n",
    "n_thin = 5 # thinning parameter\n",
    "alpha = 0.1 # Dirichlet parameter for document-topic distributions\n",
    "beta = 0.01 # Dirichlet parameter for topic-word distributions\n",
    "\n",
    "fragment_filename = basedir + 'input/relative_intensities/Beer_3_T10_POS_fragments_rel.csv'\n",
    "neutral_loss_filename = basedir + 'input/relative_intensities/Beer_3_T10_POS_losses_rel.csv'\n",
    "mzdiff_filename = None\n",
    "\n",
    "ms1_filename = basedir + 'input/relative_intensities/Beer_3_T10_POS_ms1_rel.csv'\n",
    "ms2_filename = basedir + 'input/relative_intensities/Beer_3_T10_POS_ms2_rel.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape (856, 1664)\n"
     ]
    }
   ],
   "source": [
    "ms2lda = Ms2Lda(fragment_filename, neutral_loss_filename, mzdiff_filename, \n",
    "                ms1_filename, ms2_filename, relative_intensity=True)\n",
    "df, vocab = ms2lda.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "CGS LDA initialising\n",
      "......................................................................................\n",
      "Using Numba for LDA sampling\n",
      "Preparing words\n",
      "Preparing Z matrix\n",
      "DONE\n",
      "Burn-in 1 \n",
      "Burn-in 2 \n",
      "Burn-in 3 \n",
      "Burn-in 4 \n",
      "Burn-in 5 \n",
      "Burn-in 6 \n",
      "Burn-in 7 \n",
      "Burn-in 8 \n",
      "Burn-in 9 \n",
      "Burn-in 10 \n",
      "Burn-in 11 \n",
      "Burn-in 12 \n",
      "Burn-in 13 \n",
      "Burn-in 14 \n",
      "Burn-in 15 \n",
      "Burn-in 16 \n",
      "Burn-in 17 \n",
      "Burn-in 18 \n",
      "Burn-in 19 \n",
      "Burn-in 20 \n",
      "Burn-in 21 \n",
      "Burn-in 22 \n",
      "Burn-in 23 \n",
      "Burn-in 24 \n",
      "Burn-in 25 \n",
      "Burn-in 26 \n",
      "Burn-in 27 \n",
      "Burn-in 28 \n",
      "Burn-in 29 \n",
      "Burn-in 30 \n",
      "Burn-in 31 \n",
      "Burn-in 32 \n",
      "Burn-in 33 \n",
      "Burn-in 34 \n",
      "Burn-in 35 \n",
      "Burn-in 36 \n",
      "Burn-in 37 \n",
      "Burn-in 38 \n",
      "Burn-in 39 \n",
      "Burn-in 40 \n",
      "Burn-in 41 \n",
      "Burn-in 42 \n",
      "Burn-in 43 \n",
      "Burn-in 44 \n",
      "Burn-in 45 \n",
      "Burn-in 46 \n",
      "Burn-in 47 \n",
      "Burn-in 48 \n",
      "Burn-in 49 \n",
      "Burn-in 50 \n",
      "Burn-in 51 \n",
      "Burn-in 52 \n",
      "Burn-in 53 \n",
      "Burn-in 54 \n",
      "Burn-in 55 \n",
      "Burn-in 56 \n",
      "Burn-in 57 \n",
      "Burn-in 58 \n",
      "Burn-in 59 \n",
      "Burn-in 60 \n",
      "Burn-in 61 \n",
      "Burn-in 62 \n",
      "Burn-in 63 \n",
      "Burn-in 64 \n",
      "Burn-in 65 \n",
      "Burn-in 66 \n",
      "Burn-in 67 \n",
      "Burn-in 68 \n",
      "Burn-in 69 \n",
      "Burn-in 70 \n",
      "Burn-in 71 \n",
      "Burn-in 72 \n",
      "Burn-in 73 \n",
      "Burn-in 74 \n",
      "Burn-in 75 \n",
      "Burn-in 76 \n",
      "Burn-in 77 \n",
      "Burn-in 78 \n",
      "Burn-in 79 \n",
      "Burn-in 80 \n",
      "Burn-in 81 \n",
      "Burn-in 82 \n",
      "Burn-in 83 \n",
      "Burn-in 84 \n",
      "Burn-in 85 \n",
      "Burn-in 86 \n",
      "Burn-in 87 \n",
      "Burn-in 88 \n",
      "Burn-in 89 \n",
      "Burn-in 90 \n",
      "Burn-in 91 \n",
      "Burn-in 92 \n",
      "Burn-in 93 \n",
      "Burn-in 94 \n",
      "Burn-in 95 \n",
      "Burn-in 96 \n",
      "Burn-in 97 \n",
      "Burn-in 98 \n",
      "Burn-in 99 \n",
      "Burn-in 100 \n",
      "Burn-in 101 \n",
      "Burn-in 102 \n",
      "Burn-in 103 \n",
      "Burn-in 104 \n",
      "Burn-in 105 \n",
      "Burn-in 106 \n",
      "Burn-in 107 \n",
      "Burn-in 108 \n",
      "Burn-in 109 \n",
      "Burn-in 110 \n",
      "Burn-in 111 \n",
      "Burn-in 112 \n",
      "Burn-in 113 \n",
      "Burn-in 114 \n",
      "Burn-in 115 \n",
      "Burn-in 116 \n",
      "Burn-in 117 \n",
      "Burn-in 118 \n",
      "Burn-in 119 \n",
      "Burn-in 120 \n",
      "Burn-in 121 \n",
      "Burn-in 122 \n",
      "Burn-in 123 \n",
      "Burn-in 124 \n",
      "Burn-in 125 \n",
      "Burn-in 126 \n",
      "Burn-in 127 \n",
      "Burn-in 128 \n",
      "Burn-in 129 \n",
      "Burn-in 130 \n",
      "Burn-in 131 \n",
      "Burn-in 132 \n",
      "Burn-in 133 \n",
      "Burn-in 134 \n",
      "Burn-in 135 \n",
      "Burn-in 136 \n",
      "Burn-in 137 \n",
      "Burn-in 138 \n",
      "Burn-in 139 \n",
      "Burn-in 140 \n",
      "Burn-in 141 \n",
      "Burn-in 142 \n",
      "Burn-in 143 \n",
      "Burn-in 144 \n",
      "Burn-in 145 \n",
      "Burn-in 146 \n",
      "Burn-in 147 \n",
      "Burn-in 148 \n",
      "Burn-in 149 \n",
      "Burn-in 150 \n",
      "Burn-in 151 \n",
      "Burn-in 152 \n",
      "Burn-in 153 \n",
      "Burn-in 154 \n",
      "Burn-in 155 \n",
      "Burn-in 156 \n",
      "Burn-in 157 \n",
      "Burn-in 158 \n",
      "Burn-in 159 \n",
      "Burn-in 160 \n",
      "Burn-in 161 \n",
      "Burn-in 162 \n",
      "Burn-in 163 \n",
      "Burn-in 164 \n",
      "Burn-in 165 \n"
     ]
    }
   ],
   "source": [
    "ms2lda.run_lda(df, vocab, n_topics, n_samples, n_burn, n_thin, \n",
    "               alpha, beta, use_own_model=True, use_native=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we save the results of this LDA on beer3 and produce the output matrices etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ms2lda.write_results('beer3_pos_rel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we show the ranking of the top-10 topics by their h-indices. Change the *sort_by* parameter to rank by either h-index or in-degree and remove the *top_N* parameter to show the ranking of all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# topic_ranking, sorted_topic_counts = ms2lda.rank_topics(sort_by='in_degree')\n",
    "topic_ranking, sorted_topic_counts = ms2lda.rank_topics(sort_by='h_index', top_N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fragments of these topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "persisted_topics  = [57, 87, 16, 50, 101, 0, 5, 20, 21, 27]\n",
    "\n",
    "# remove the selected_topics parameter to plot for all topics\n",
    "ms2lda.plot_lda_fragments(consistency=0.50, sort_by=\"h_index\", selected_topics=persisted_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to set all the top-10 topics from beer3pos above (or whatever defined in the *persisted_topics* variable) and run the LDA using them again on beer2.\n",
    "\n",
    "First we need to save the state of the LDA model that we just ran on  beer3pos. Below line will create two more files: the dumped model state (*beer3pos.model*) and the list of vocabularies of the 'words' used for the persisted topics (*beer3pos.vocab*). These files are written to the same location as the output matrices, i.e. in the *results/beer3_pos_rel* folder relative to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_filename = 'results/beer3_pos_rel/beer3pos.model'\n",
    "vocab_filename = 'results/beer3_pos_rel/beer3pos.vocab'\n",
    "ms2lda.save_model(persisted_topics, model_filename, vocab_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. LDA on Beer2 with persistent topics from Beer3\n",
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the previously saved model of beer3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lda_cgs import CollapseGibbsLda\n",
    "beer3_model = CollapseGibbsLda.load(model_filename)\n",
    "if hasattr(beer3_model, 'selected_topics'):\n",
    "    print \"Persistent topics = \" + str(beer3_model.selected_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to go to R and run the feature extraction script (*MS1MS2_MatrixGeneration_default_7ppm_specPeaks.R*) on the Beer2pos data. <font color='red'>**This step has to be manually done for now .. although we should automate it as part of the pipeline later.**</font>\n",
    "\n",
    "Specifically in the R script, set the following parameter (that specifies the vocabulary list of the persistent topics)\n",
    "\n",
    "    prev_words_file <- '/home/joewandy/git/metabolomics_tools/justin/notebooks/results/beer3_pos_rel/beer3pos.vocab'\n",
    "\n",
    "and re-run sections in the R-script that does feature extractions .. from the \"Data filtering\" part onwards.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the LDA on beer2pos, there's now the additional parameter *previous_model* that needs to be passed in. Also, the total number of topics are now 135. The persistent topics (10) come first, and the remaining new topics (125) are appended after them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fragment_filename = basedir + 'input/relative_intensities/Beer_2_T10_POS_fragments_rel.csv'\n",
    "neutral_loss_filename = basedir + 'input/relative_intensities/Beer_2_T10_POS_losses_rel.csv'\n",
    "mzdiff_filename = None\n",
    "ms1_filename = basedir + 'input/relative_intensities/Beer_2_T10_POS_ms1_rel.csv'\n",
    "ms2_filename = basedir + 'input/relative_intensities/Beer_2_T10_POS_ms2_rel.csv'\n",
    "\n",
    "ms2lda = Ms2Lda(fragment_filename, neutral_loss_filename, mzdiff_filename, \n",
    "                ms1_filename, ms2_filename, relative_intensity=True)\n",
    "df, vocab = ms2lda.preprocess()\n",
    "ms2lda.run_lda(df, vocab, n_topics, n_samples, n_burn, n_thin, \n",
    "               alpha, beta, use_own_model=True, use_native=True, previous_model=beer3_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ms2lda.write_results('beer2_pos_rel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Beer2 Results\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The persisted topics from previous LDA run are placed first in list of topics of the new LDA run, so so old topic 57 becomes new topic 0, old topic 87 is new topic 1, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_persisted_index = beer3_model.selected_topics\n",
    "new_persisted_index = range(len(beer3_model.selected_topics))\n",
    "\n",
    "print \"in beer3pos = \" + str(old_persisted_index)\n",
    "print \"in beer2pos = \" + str(new_persisted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we show the ranking of the top-10 topics in beer2pos by the H-index, we see that the persisted topics from beer3 aren't very high up the list, i.e. we don't see topics 0 - 9 there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_ranking, sorted_topic_counts = ms2lda.rank_topics(sort_by='h_index', top_N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the fragments in topics 0 - 9 in beer2pos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ms2lda.plot_lda_fragments(consistency=0.0, sort_by=\"h_index\", selected_topics=new_persisted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot of the predictive distribution of the persisted topics (theta) in the old and new LDA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_old = np.sum(beer3_model.doc_topic_, axis=0)\n",
    "pred_old = pred_old / np.sum(pred_old)\n",
    "pred_old = pred_old[old_persisted_index]\n",
    "\n",
    "beer2_model = ms2lda.model\n",
    "pred_new = np.sum(beer2_model.doc_topic_, axis=0)\n",
    "pred_new = pred_new / np.sum(pred_new)\n",
    "pred_new = pred_new[new_persisted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = len(old_persisted_index)\n",
    "ind = np.arange(K)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, pred_old, width, color='g')\n",
    "rects2 = ax.bar(ind+width, pred_new, width, color='b')\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Predictive Probability')\n",
    "ax.set_xlabel('Persistent Topic')\n",
    "ax.set_title('Predictive distributions of persistent topics')\n",
    "ax.set_xticks(ind+width)\n",
    "ax.set_xticklabels(old_persisted_index)\n",
    "\n",
    "ax.legend( (rects1[0], rects2[0]), ('Beer3pos', 'Beer2pos') )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
