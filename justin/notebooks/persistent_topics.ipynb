{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Notebook for Persistent Topics\n",
    "===================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how we can **(1)** run LDA on one data, **(2)** save some of the topics from the first LDA run and **(3)** use the saved topics when running LDA again on a new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "basedir = '../'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "from lda_for_fragments import Ms2Lda\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initial LDA on Beer 3\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of topics, around 300-400 seems to be good for the beer/urine data from cross-validation results\n",
    "n_topics = 300 \n",
    "\n",
    "# How many samples to get during Gibbs sampling, recommended >500 for analysis\n",
    "n_samples = 500\n",
    "\n",
    "# No. of burn-in samples to discard before we start averaging over the samples. \n",
    "# If 0, then we'll use only the last sample for the results.\n",
    "n_burn = 0 \n",
    "\n",
    "# Thinning parameter when averaging over the samples. \n",
    "# If n_burn is 0 then this doesn't matter.\n",
    "n_thin = 1 \n",
    "\n",
    "# Follow the recommendation from Griffith & Styver\n",
    "alpha = 50.0/n_topics # hyper-parameter for document-topic distributions\n",
    "beta = 0.1 # hyper-parameter for topic-word distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The input files\n",
    "fragment_filename = basedir + 'input/relative_intensities/Beer_3_T10_POS_fragments_rel.csv'\n",
    "neutral_loss_filename = basedir + 'input/relative_intensities/Beer_3_T10_POS_losses_rel.csv'\n",
    "mzdiff_filename = None\n",
    "ms1_filename = basedir + 'input/relative_intensities/Beer_3_T10_POS_ms1_rel.csv'\n",
    "ms2_filename = basedir + 'input/relative_intensities/Beer_3_T10_POS_ms2_rel.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ms2lda = Ms2Lda.lcms_data_from_R(fragment_filename, neutral_loss_filename, mzdiff_filename, \n",
    "                                 ms1_filename, ms2_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ms2lda.run_lda(n_topics, n_samples, n_burn, n_thin, alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we save the results of this LDA on beer3 and produce the output matrices etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ms2lda.do_thresholding(th_doc_topic=0.05, th_topic_word=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ms2lda.write_results('beer3_pos_rel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ms2lda.print_topic_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ms2lda.plot_log_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we show the ranking of the top-10 topics by their h-indices. Change the *sort_by* parameter to rank by either h-index or in-degree and remove the *top_N* parameter to show the ranking of all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# topic_ranking, sorted_topic_counts = ms2lda.rank_topics(sort_by='in_degree')\n",
    "topic_ranking, sorted_topic_counts = ms2lda.rank_topics(sort_by='h_index', top_N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fragments of these topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "persisted_topics  = [57, 87, 16, 50, 101, 0, 5, 20, 21, 27]\n",
    "\n",
    "# Non-interactive visualisation\n",
    "# Remove the 'selected_topics' parameter to make the plots for all topics.\n",
    "# You can sort by either 'h_index' or 'in_degree'.\n",
    "ms2lda.plot_lda_fragments(consistency=0.50, sort_by=\"h_index\", selected_topics=persisted_topics)\n",
    "\n",
    "# uncomment below for interactive visualisation\n",
    "# ms2lda.plot_lda_fragments(consistency=0.50, sort_by=\"h_index\", interactive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to set all the top-10 topics from beer3pos above (or whatever defined in the *persisted_topics* variable) and run the LDA using them again on beer2.\n",
    "\n",
    "First we need to save the state of the LDA model that we just ran on  beer3pos. Below line will create two more files: the dumped model state (*beer3pos.model*) and the list of vocabularies of the 'words' used for the persisted topics (*beer3pos.vocab*). These files are written to the same location as the output matrices, i.e. in the *results/beer3_pos_rel* folder relative to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_filename = 'results/beer3_pos_rel/beer3pos.model'\n",
    "vocab_filename = 'results/beer3_pos_rel/beer3pos.vocab'\n",
    "ms2lda.persist_topics(model_filename, vocab_filename, persisted_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. LDA on Beer2 with persistent topics from Beer3\n",
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the previously saved model of beer3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lda_cgs import CollapseGibbsLda\n",
    "beer3_model = CollapseGibbsLda.load(model_filename)\n",
    "if hasattr(beer3_model, 'selected_topics'):\n",
    "    print \"Persistent topics = \" + str(beer3_model.selected_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to go to R and run the feature extraction script (*MS1MS2_MatrixGeneration.R*) on the Beer2pos data. <font color='red'>**This step has to be manually done for now .. although we should automate it as part of the pipeline later. If you change any of the initial LDA parameters in section 1, you should also do this before proceeding further (because it might result in different probabilities of the words)**</font>\n",
    "\n",
    "Specifically in that R script, make sure to set the following parameter (that specifies the vocabulary list of the persistent topics)\n",
    "\n",
    "    prev_words_file <- '/home/joewandy/git/metabolomics_tools/justin/notebooks/results/beer3_pos_rel/beer3pos.vocab'\n",
    "\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the LDA on beer2pos, there's now the additional parameter *previous_model* that needs to be passed in. Also, the total number of topics are now 135. The persistent topics (10) come first, and the remaining new topics (300) are appended after them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of NEW topics. The total number of topics is actually 10 (previous) + 300 (new) = 310. \n",
    "n_topics = 300 \n",
    "\n",
    "# How many samples to get during Gibbs sampling, recommended >500 for analysis\n",
    "n_samples = 500\n",
    "\n",
    "# No. of burn-in samples to discard before we start averaging over the samples. \n",
    "# If 0, then we'll use only the last sample for the results.\n",
    "n_burn = 0 \n",
    "\n",
    "# Thinning parameter when averaging over the samples. \n",
    "# If n_burn is 0 then this doesn't matter.\n",
    "n_thin = 1 \n",
    "\n",
    "# Follow the recommendation from Griffith & Styver\n",
    "total_no_of_topics = n_topics + len(beer3_model.selected_topics)\n",
    "alpha = 50.0/total_no_of_topics # hyper-parameter for document-topic distributions\n",
    "beta = 0.1 # hyper-parameter for topic-word distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fragment_filename = basedir + 'input/relative_intensities/Beer_2_T10_POS_fragments_rel.csv'\n",
    "neutral_loss_filename = basedir + 'input/relative_intensities/Beer_2_T10_POS_losses_rel.csv'\n",
    "mzdiff_filename = None\n",
    "ms1_filename = basedir + 'input/relative_intensities/Beer_2_T10_POS_ms1_rel.csv'\n",
    "ms2_filename = basedir + 'input/relative_intensities/Beer_2_T10_POS_ms2_rel.csv'\n",
    "\n",
    "ms2lda = Ms2Lda.lcms_data_from_R(fragment_filename, neutral_loss_filename, mzdiff_filename, \n",
    "                                 ms1_filename, ms2_filename)\n",
    "ms2lda.run_lda(n_topics, n_samples, n_burn, n_thin, alpha, beta, previous_model=beer3_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ms2lda.write_results('beer2_pos_rel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Beer2 Results\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The persisted topics from previous LDA run are placed first in list of topics of the new LDA run, so so old topic 57 becomes new topic 0, old topic 87 is new topic 1, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_persisted_index = beer3_model.selected_topics\n",
    "new_persisted_index = range(len(beer3_model.selected_topics))\n",
    "\n",
    "print \"in beer3pos = \" + str(old_persisted_index)\n",
    "print \"in beer2pos = \" + str(new_persisted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we show the ranking of the top-10 topics in beer2pos by the H-index, we see that the persisted topics from beer3 aren't very high up the list, i.e. we don't see topics 0 - 9 there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_ranking, sorted_topic_counts = ms2lda.rank_topics(sort_by='h_index', top_N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the fragments in topics 0 - 9 in beer2pos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ms2lda.do_thresholding(th_doc_topic=0.05, th_topic_word=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# non-interactive\n",
    "ms2lda.plot_lda_fragments(consistency=0.0, sort_by=\"h_index\", selected_topics=new_persisted_index)\n",
    "\n",
    "# interactive\n",
    "# ms2lda.plot_lda_fragments(consistency=0.0, sort_by=\"h_index\", interactive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot of the predictive distribution of the persisted topics (theta) in the old and new LDA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_old = beer3_model.posterior_alpha\n",
    "pred_old = pred_old / np.sum(pred_old)\n",
    "pred_old = pred_old[old_persisted_index]\n",
    "print pred_old\n",
    "\n",
    "beer2_model = ms2lda.model\n",
    "pred_new = beer2_model.posterior_alpha\n",
    "pred_new = pred_new / np.sum(pred_new)\n",
    "pred_new = pred_new[new_persisted_index]\n",
    "print pred_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = len(old_persisted_index)\n",
    "ind = np.arange(K)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, pred_old, width, color='g')\n",
    "rects2 = ax.bar(ind+width, pred_new, width, color='b')\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Predictive Probability')\n",
    "ax.set_xlabel('Persistent Topic')\n",
    "ax.set_title('Predictive distributions of persistent topics')\n",
    "ax.set_xticks(ind+width)\n",
    "ax.set_xticklabels(old_persisted_index)\n",
    "\n",
    "ax.legend( (rects1[0], rects2[0]), ('Beer3pos', 'Beer2pos') )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
