{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with synthetic data\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "basedir = '../'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "import cPickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from lda.lda import LDA\n",
    "from numpy import int32\n",
    "from numpy.random import RandomState\n",
    "\n",
    "from lda_cgs_numba import sample_numba\n",
    "from lda_cgs_numpy import sample_numpy\n",
    "from lda_generate_data import LdaDataGenerator\n",
    "import lda_utils as utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "from scipy.special import psi\n",
    "from lda_utils import estimate_alpha_from_counts\n",
    "from lda_cgs import CollapseGibbsLda\n",
    "import visualisation.pyLDAvis as pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multiplier = 1\n",
    "n_topics = 20 * multiplier\n",
    "n_docs = 100 * multiplier\n",
    "vocab_size = 200 * multiplier\n",
    "document_length = 50 * multiplier\n",
    "\n",
    "alpha = 0.1\n",
    "beta = 0.01    \n",
    "n_samples = 200\n",
    "n_burn = 0\n",
    "n_thin = 1\n",
    "\n",
    "random_state = RandomState(1234567890)\n",
    "gen = LdaDataGenerator(alpha, make_plot=True)\n",
    "df, vocab = gen.generate_from_file('../input/test1.csv', '../input/test1.words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CGS LDA initialising\n",
      "..........\n",
      "Using Numba for LDA sampling\n",
      "Preparing words\n",
      "Preparing Z matrix\n",
      "DONE\n",
      "Sample 1   Log joint likelihood = -40361.524 \n",
      "Sample 2   Log joint likelihood = -33684.924 \n",
      "Sample 3   Log joint likelihood = -30332.865 \n",
      "Sample 4   Log joint likelihood = -28611.731 \n",
      "Sample 5   Log joint likelihood = -27428.915 \n",
      "Sample 6   Log joint likelihood = -26771.378 \n",
      "Sample 7   Log joint likelihood = -26202.570 \n",
      "Sample 8   Log joint likelihood = -25637.589 \n",
      "Sample 9   Log joint likelihood = -25284.896 \n",
      "Sample 10   Log joint likelihood = -25072.341 \n",
      "Sample 11   Log joint likelihood = -24769.868 \n",
      "Sample 12   Log joint likelihood = -24480.904 \n",
      "Sample 13   Log joint likelihood = -24274.089 \n",
      "Sample 14   Log joint likelihood = -24078.263 \n",
      "Sample 15   Log joint likelihood = -23911.653 \n",
      "Sample 16   Log joint likelihood = -23786.088 \n",
      "Sample 17   Log joint likelihood = -23619.600 \n",
      "Sample 18   Log joint likelihood = -23579.539 \n",
      "Sample 19   Log joint likelihood = -23490.796 \n",
      "Sample 20   Log joint likelihood = -23431.194 \n",
      "Sample 21   Log joint likelihood = -23188.953 \n",
      "Sample 22   Log joint likelihood = -23014.971 \n",
      "Sample 23   Log joint likelihood = -22927.978 \n",
      "Sample 24   Log joint likelihood = -22898.366 \n",
      "Sample 25   Log joint likelihood = -22742.065 \n",
      "Sample 26   Log joint likelihood = -22727.969 \n",
      "Sample 27   Log joint likelihood = -22708.367 \n",
      "Sample 28   Log joint likelihood = -22573.799 \n",
      "Sample 29   Log joint likelihood = -22561.744 \n",
      "Sample 30   Log joint likelihood = -22534.232 \n",
      "Sample 31   Log joint likelihood = -22438.531 \n",
      "Sample 32   Log joint likelihood = -22411.348 \n",
      "Sample 33   Log joint likelihood = -22299.106 \n",
      "Sample 34   Log joint likelihood = -22203.817 \n",
      "Sample 35   Log joint likelihood = -22197.208 \n",
      "Sample 36   Log joint likelihood = -22225.626 \n",
      "Sample 37   Log joint likelihood = -22302.416 \n",
      "Sample 38   Log joint likelihood = -22225.666 \n",
      "Sample 39   Log joint likelihood = -22160.404 \n",
      "Sample 40   Log joint likelihood = -22100.600 \n",
      "Sample 41   Log joint likelihood = -22055.533 \n",
      "Sample 42   Log joint likelihood = -22061.790 \n",
      "Sample 43   Log joint likelihood = -22025.745 \n",
      "Sample 44   Log joint likelihood = -21954.797 \n",
      "Sample 45   Log joint likelihood = -21991.328 \n",
      "Sample 46   Log joint likelihood = -21879.424 \n",
      "Sample 47   Log joint likelihood = -21901.829 \n",
      "Sample 48   Log joint likelihood = -21818.309 \n",
      "Sample 49   Log joint likelihood = -21676.173 \n",
      "Sample 50   Log joint likelihood = -21643.429 \n",
      "Sample 51   Log joint likelihood = -21532.788 \n",
      "Sample 52   Log joint likelihood = -21566.942 \n",
      "Sample 53   Log joint likelihood = -21569.022 \n",
      "Sample 54   Log joint likelihood = -21560.693 \n",
      "Sample 55   Log joint likelihood = -21564.809 \n",
      "Sample 56   Log joint likelihood = -21599.184 \n",
      "Sample 57   Log joint likelihood = -21550.518 \n",
      "Sample 58   Log joint likelihood = -21524.645 \n",
      "Sample 59   Log joint likelihood = -21567.148 \n",
      "Sample 60   Log joint likelihood = -21554.200 \n",
      "Sample 61   Log joint likelihood = -21522.789 \n",
      "Sample 62   Log joint likelihood = -21467.774 \n",
      "Sample 63   Log joint likelihood = -21357.631 \n",
      "Sample 64   Log joint likelihood = -21268.144 \n",
      "Sample 65   Log joint likelihood = -21230.678 \n",
      "Sample 66   Log joint likelihood = -21260.266 \n",
      "Sample 67   Log joint likelihood = -21316.608 \n",
      "Sample 68   Log joint likelihood = -21250.666 \n",
      "Sample 69   Log joint likelihood = -21323.109 \n",
      "Sample 70   Log joint likelihood = -21361.435 \n",
      "Sample 71   Log joint likelihood = -21371.537 \n",
      "Sample 72   Log joint likelihood = -21355.802 \n",
      "Sample 73   Log joint likelihood = -21234.902 \n",
      "Sample 74   Log joint likelihood = -21302.580 \n",
      "Sample 75   Log joint likelihood = -21416.991 \n",
      "Sample 76   Log joint likelihood = -21320.326 \n",
      "Sample 77   Log joint likelihood = -21288.739 \n",
      "Sample 78   Log joint likelihood = -21212.602 \n",
      "Sample 79   Log joint likelihood = -21268.686 \n",
      "Sample 80   Log joint likelihood = -21203.973 \n",
      "Sample 81   Log joint likelihood = -21208.004 \n",
      "Sample 82   Log joint likelihood = -21295.739 \n",
      "Sample 83   Log joint likelihood = -21178.505 \n",
      "Sample 84   Log joint likelihood = -21184.675 \n",
      "Sample 85   Log joint likelihood = -21172.696 \n",
      "Sample 86   Log joint likelihood = -21184.266 \n",
      "Sample 87   Log joint likelihood = -21215.840 \n",
      "Sample 88   Log joint likelihood = -21136.958 \n",
      "Sample 89   Log joint likelihood = -21154.061 \n",
      "Sample 90   Log joint likelihood = -21101.096 \n",
      "Sample 91   Log joint likelihood = -21148.072 \n",
      "Sample 92   Log joint likelihood = -21106.155 \n",
      "Sample 93   Log joint likelihood = -21064.759 \n",
      "Sample 94   Log joint likelihood = -21073.729 \n",
      "Sample 95   Log joint likelihood = -21045.177 \n",
      "Sample 96   Log joint likelihood = -21074.060 \n",
      "Sample 97   Log joint likelihood = -21189.239 \n",
      "Sample 98   Log joint likelihood = -21156.107 \n",
      "Sample 99   Log joint likelihood = -21119.290 \n",
      "Sample 100   Log joint likelihood = -21086.138 \n",
      "Sample 101   Log joint likelihood = -21106.962 \n",
      "Sample 102   Log joint likelihood = -21084.670 \n",
      "Sample 103   Log joint likelihood = -21078.329 \n",
      "Sample 104   Log joint likelihood = -21036.369 \n",
      "Sample 105   Log joint likelihood = -21023.966 \n",
      "Sample 106   Log joint likelihood = -21070.186 \n",
      "Sample 107   Log joint likelihood = -21069.315 \n",
      "Sample 108   Log joint likelihood = -21144.738 \n",
      "Sample 109   Log joint likelihood = -21129.935 \n",
      "Sample 110   Log joint likelihood = -21063.906 \n",
      "Sample 111   Log joint likelihood = -21067.625 \n",
      "Sample 112   Log joint likelihood = -21087.337 \n",
      "Sample 113   Log joint likelihood = -21090.607 \n",
      "Sample 114   Log joint likelihood = -21079.089 \n",
      "Sample 115   Log joint likelihood = -21076.800 \n",
      "Sample 116   Log joint likelihood = -21078.986 \n",
      "Sample 117   Log joint likelihood = -21047.907 \n",
      "Sample 118   Log joint likelihood = -21032.017 \n",
      "Sample 119   Log joint likelihood = -21128.278 \n",
      "Sample 120   Log joint likelihood = -21093.418 \n",
      "Sample 121   Log joint likelihood = -21072.811 \n",
      "Sample 122   Log joint likelihood = -21040.871 \n",
      "Sample 123   Log joint likelihood = -21046.450 \n",
      "Sample 124   Log joint likelihood = -21059.987 \n",
      "Sample 125   Log joint likelihood = -21092.122 \n",
      "Sample 126   Log joint likelihood = -21029.715 \n",
      "Sample 127   Log joint likelihood = -21069.321 \n",
      "Sample 128   Log joint likelihood = -21113.862 \n",
      "Sample 129   Log joint likelihood = -21045.068 \n",
      "Sample 130   Log joint likelihood = -21089.668 \n",
      "Sample 131   Log joint likelihood = -21085.238 \n",
      "Sample 132   Log joint likelihood = -21090.765 \n",
      "Sample 133   Log joint likelihood = -21098.835 \n",
      "Sample 134   Log joint likelihood = -21074.640 \n",
      "Sample 135   Log joint likelihood = -21069.740 \n",
      "Sample 136   Log joint likelihood = -21106.846 \n",
      "Sample 137   Log joint likelihood = -21142.890 \n",
      "Sample 138   Log joint likelihood = -21113.233 \n",
      "Sample 139   Log joint likelihood = -21041.033 \n",
      "Sample 140   Log joint likelihood = -21088.319 \n",
      "Sample 141   Log joint likelihood = -21100.426 \n",
      "Sample 142   Log joint likelihood = -21143.020 \n",
      "Sample 143   Log joint likelihood = -21091.504 \n",
      "Sample 144   Log joint likelihood = -21136.190 \n",
      "Sample 145   Log joint likelihood = -21112.845 \n",
      "Sample 146   Log joint likelihood = -21073.990 \n",
      "Sample 147   Log joint likelihood = -21074.716 \n",
      "Sample 148   Log joint likelihood = -21113.903 \n",
      "Sample 149   Log joint likelihood = -21051.268 \n",
      "Sample 150   Log joint likelihood = -21097.421 \n",
      "Sample 151   Log joint likelihood = -21123.219 \n",
      "Sample 152   Log joint likelihood = -21107.664 \n",
      "Sample 153   Log joint likelihood = -21087.244 \n",
      "Sample 154   Log joint likelihood = -21077.064 \n",
      "Sample 155   Log joint likelihood = -21087.056 \n",
      "Sample 156   Log joint likelihood = -21074.353 \n",
      "Sample 157   Log joint likelihood = -21060.079 \n",
      "Sample 158   Log joint likelihood = -21130.598 \n",
      "Sample 159   Log joint likelihood = -21060.878 \n",
      "Sample 160   Log joint likelihood = -21084.768 \n",
      "Sample 161   Log joint likelihood = -21081.456 \n",
      "Sample 162   Log joint likelihood = -21041.479 \n",
      "Sample 163   Log joint likelihood = -21130.681 \n",
      "Sample 164   Log joint likelihood = -21081.029 \n",
      "Sample 165   Log joint likelihood = -21082.025 \n",
      "Sample 166   Log joint likelihood = -21145.520 \n",
      "Sample 167   Log joint likelihood = -21090.094 \n",
      "Sample 168   Log joint likelihood = -21117.057 \n",
      "Sample 169   Log joint likelihood = -21141.807 \n",
      "Sample 170   Log joint likelihood = -21093.544 \n",
      "Sample 171   Log joint likelihood = -21168.742 \n",
      "Sample 172   Log joint likelihood = -21106.501 \n",
      "Sample 173   Log joint likelihood = -21127.269 \n",
      "Sample 174   Log joint likelihood = -21088.090 \n",
      "Sample 175   Log joint likelihood = -21063.255 \n",
      "Sample 176   Log joint likelihood = -21132.041 \n",
      "Sample 177   Log joint likelihood = -21066.669 \n",
      "Sample 178   Log joint likelihood = -21092.296 \n",
      "Sample 179   Log joint likelihood = -21079.202 \n",
      "Sample 180   Log joint likelihood = -21037.024 \n",
      "Sample 181   Log joint likelihood = -21021.222 \n",
      "Sample 182   Log joint likelihood = -21106.943 \n",
      "Sample 183   Log joint likelihood = -21123.454 \n",
      "Sample 184   Log joint likelihood = -21102.792 \n",
      "Sample 185   Log joint likelihood = -21145.814 \n",
      "Sample 186   Log joint likelihood = -21078.662 \n",
      "Sample 187   Log joint likelihood = -21116.496 \n",
      "Sample 188   Log joint likelihood = -21098.506 \n",
      "Sample 189   Log joint likelihood = -21116.496 \n",
      "Sample 190   Log joint likelihood = -21128.184 \n",
      "Sample 191   Log joint likelihood = -21105.103 \n",
      "Sample 192   Log joint likelihood = -21076.549 \n",
      "Sample 193   Log joint likelihood = -21101.239 \n",
      "Sample 194   Log joint likelihood = -21135.776 \n",
      "Sample 195   Log joint likelihood = -21066.936 \n",
      "Sample 196   Log joint likelihood = -21054.962 \n",
      "Sample 197   Log joint likelihood = -21078.864 \n",
      "Sample 198   Log joint likelihood = -21111.835 \n",
      "Sample 199   Log joint likelihood = -21083.018 \n",
      "Sample 200   Log joint likelihood = -21083.967 \n",
      "--- TOTAL TIME 0 seconds ---\n",
      "[ 0.11984591  0.10392335  0.12693174  0.06928709  0.05702727  0.11222245\n",
      "  0.10697857  0.06561399  0.05909916  0.11754279  0.08209015  0.08747931\n",
      "  0.08029843  0.08008511  0.06914558  0.08483886  0.08980647  0.06644501\n",
      "  0.10011025  0.10503111]\n"
     ]
    }
   ],
   "source": [
    "gibbs1 = CollapseGibbsLda(df, vocab, n_topics, alpha, beta, random_state=random_state, previous_model=None)\n",
    "start_time = time.time()\n",
    "gibbs1.run(n_burn, n_samples, n_thin, use_native=True)\n",
    "print(\"--- TOTAL TIME %d seconds ---\" % (time.time() - start_time))\n",
    "print gibbs1.posterior_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: gibbs1_word_2 (0.0841433021807), gibbs1_word_1 (0.0747975077882), gibbs1_word_0 (0.0623364485981), gibbs1_word_9 (0.0592211838006), gibbs1_word_127 (0.0561059190031), gibbs1_word_6 (0.0561059190031), gibbs1_word_7 (0.0561059190031), gibbs1_word_3 (0.0529906542056),\n",
      "Topic 1: gibbs1_word_134 (0.121981707317), gibbs1_word_139 (0.112835365854), gibbs1_word_137 (0.109786585366), gibbs1_word_130 (0.100640243902), gibbs1_word_138 (0.100640243902), gibbs1_word_135 (0.0975914634146), gibbs1_word_131 (0.0945426829268), gibbs1_word_133 (0.0945426829268), gibbs1_word_136 (0.07625), gibbs1_word_132 (0.0732012195122),\n",
      "Topic 2: gibbs1_word_143 (0.122590529248), gibbs1_word_140 (0.108662952646), gibbs1_word_149 (0.105877437326), gibbs1_word_144 (0.103091922006), gibbs1_word_146 (0.100306406685), gibbs1_word_142 (0.0919498607242), gibbs1_word_141 (0.0919498607242), gibbs1_word_147 (0.0919498607242), gibbs1_word_148 (0.0863788300836), gibbs1_word_145 (0.0808077994429),\n",
      "Topic 3: gibbs1_word_164 (0.126624472574), gibbs1_word_168 (0.113966244726), gibbs1_word_160 (0.113966244726), gibbs1_word_165 (0.101308016878), gibbs1_word_169 (0.0970886075949), gibbs1_word_163 (0.0928691983122), gibbs1_word_161 (0.0928691983122), gibbs1_word_166 (0.0844303797468), gibbs1_word_162 (0.0844303797468), gibbs1_word_167 (0.0759915611814),\n",
      "Topic 4: gibbs1_word_57 (0.136454545455), gibbs1_word_56 (0.127363636364), gibbs1_word_50 (0.127363636364), gibbs1_word_54 (0.109181818182), gibbs1_word_59 (0.109181818182), gibbs1_word_55 (0.091), gibbs1_word_53 (0.0819090909091), gibbs1_word_52 (0.0728181818182), gibbs1_word_51 (0.0637272727273), gibbs1_word_58 (0.0637272727273),\n",
      "Topic 5: gibbs1_word_171 (0.145092592593), gibbs1_word_178 (0.111141975309), gibbs1_word_179 (0.0987962962963), gibbs1_word_177 (0.0987962962963), gibbs1_word_175 (0.0957098765432), gibbs1_word_176 (0.0926234567901), gibbs1_word_170 (0.089537037037), gibbs1_word_174 (0.089537037037), gibbs1_word_172 (0.086450617284), gibbs1_word_173 (0.0710185185185),\n",
      "Topic 6: gibbs1_word_180 (0.121526479751), gibbs1_word_185 (0.109065420561), gibbs1_word_181 (0.105950155763), gibbs1_word_187 (0.105950155763), gibbs1_word_182 (0.105950155763), gibbs1_word_189 (0.102834890966), gibbs1_word_188 (0.0872585669782), gibbs1_word_184 (0.0810280373832), gibbs1_word_183 (0.0747975077882), gibbs1_word_186 (0.0654517133956),\n",
      "Topic 7: gibbs1_word_67 (0.213816793893), gibbs1_word_61 (0.129847328244), gibbs1_word_69 (0.106946564885), gibbs1_word_66 (0.106946564885), gibbs1_word_65 (0.091679389313), gibbs1_word_62 (0.091679389313), gibbs1_word_63 (0.0840458015267), gibbs1_word_64 (0.0764122137405), gibbs1_word_60 (0.0535114503817),\n",
      "Topic 8: gibbs1_word_118 (0.436025641026), gibbs1_word_112 (0.423205128205),\n",
      "Topic 9: gibbs1_word_32 (0.136396103896), gibbs1_word_38 (0.116915584416), gibbs1_word_35 (0.113668831169), gibbs1_word_37 (0.107175324675), gibbs1_word_36 (0.103928571429), gibbs1_word_30 (0.100681818182), gibbs1_word_34 (0.0941883116883), gibbs1_word_31 (0.0876948051948), gibbs1_word_39 (0.0649675324675), gibbs1_word_33 (0.058474025974),\n",
      "Topic 10: gibbs1_word_78 (0.13600877193), gibbs1_word_76 (0.122850877193), gibbs1_word_74 (0.114078947368), gibbs1_word_72 (0.109692982456), gibbs1_word_79 (0.105307017544), gibbs1_word_77 (0.092149122807), gibbs1_word_75 (0.0877631578947), gibbs1_word_71 (0.0746052631579), gibbs1_word_73 (0.0702192982456), gibbs1_word_70 (0.0614473684211),\n",
      "Topic 11: gibbs1_word_16 (0.137287581699), gibbs1_word_19 (0.114411764706), gibbs1_word_13 (0.107875816993), gibbs1_word_10 (0.107875816993), gibbs1_word_12 (0.104607843137), gibbs1_word_17 (0.0915359477124), gibbs1_word_15 (0.0882679738562), gibbs1_word_18 (0.0882679738562), gibbs1_word_11 (0.0817320261438), gibbs1_word_14 (0.0719281045752),\n",
      "Topic 12: gibbs1_word_108 (0.119333333333), gibbs1_word_106 (0.105298245614), gibbs1_word_109 (0.105298245614), gibbs1_word_107 (0.101789473684), gibbs1_word_105 (0.101789473684), gibbs1_word_101 (0.0947719298246), gibbs1_word_104 (0.0877543859649), gibbs1_word_100 (0.0842456140351), gibbs1_word_102 (0.0807368421053), gibbs1_word_103 (0.0737192982456),\n",
      "Topic 13: gibbs1_word_81 (0.127451737452), gibbs1_word_88 (0.123590733591), gibbs1_word_82 (0.123590733591), gibbs1_word_85 (0.100424710425), gibbs1_word_84 (0.100424710425), gibbs1_word_83 (0.0927027027027), gibbs1_word_86 (0.0888416988417), gibbs1_word_87 (0.0811196911197), gibbs1_word_80 (0.0733976833977), gibbs1_word_89 (0.0656756756757),\n",
      "Topic 14: gibbs1_word_23 (0.137974137931), gibbs1_word_25 (0.107801724138), gibbs1_word_29 (0.10349137931), gibbs1_word_21 (0.10349137931), gibbs1_word_24 (0.0948706896552), gibbs1_word_22 (0.0905603448276), gibbs1_word_20 (0.0905603448276), gibbs1_word_27 (0.08625), gibbs1_word_26 (0.0776293103448), gibbs1_word_28 (0.0776293103448),\n",
      "Topic 15: gibbs1_word_46 (0.128933333333), gibbs1_word_42 (0.128933333333), gibbs1_word_45 (0.124488888889), gibbs1_word_48 (0.111155555556), gibbs1_word_49 (0.106711111111), gibbs1_word_44 (0.0978222222222), gibbs1_word_43 (0.0844888888889), gibbs1_word_40 (0.0844888888889), gibbs1_word_47 (0.0578222222222),\n",
      "Topic 16: gibbs1_word_110 (0.164789272031), gibbs1_word_115 (0.141800766284), gibbs1_word_113 (0.13030651341), gibbs1_word_117 (0.126475095785), gibbs1_word_111 (0.114980842912), gibbs1_word_114 (0.111149425287), gibbs1_word_119 (0.103486590038), gibbs1_word_116 (0.0919923371648),\n",
      "Topic 17: gibbs1_word_196 (0.137417582418), gibbs1_word_194 (0.126428571429), gibbs1_word_192 (0.109945054945), gibbs1_word_197 (0.104450549451), gibbs1_word_193 (0.0934615384615), gibbs1_word_199 (0.087967032967), gibbs1_word_191 (0.0824725274725), gibbs1_word_190 (0.0824725274725), gibbs1_word_195 (0.076978021978), gibbs1_word_198 (0.065989010989),\n",
      "Topic 18: gibbs1_word_157 (0.181111111111), gibbs1_word_159 (0.111152263374), gibbs1_word_153 (0.107037037037), gibbs1_word_151 (0.107037037037), gibbs1_word_156 (0.1029218107), gibbs1_word_155 (0.0988065843621), gibbs1_word_152 (0.0782304526749), gibbs1_word_150 (0.07), gibbs1_word_154 (0.0658847736626), gibbs1_word_158 (0.0535390946502),\n",
      "Topic 19: gibbs1_word_91 (0.149039735099), gibbs1_word_95 (0.125860927152), gibbs1_word_96 (0.105993377483), gibbs1_word_92 (0.102682119205), gibbs1_word_97 (0.0993708609272), gibbs1_word_98 (0.0894370860927), gibbs1_word_93 (0.0861258278146), gibbs1_word_99 (0.0861258278146), gibbs1_word_90 (0.0761920529801), gibbs1_word_94 (0.072880794702),\n"
     ]
    }
   ],
   "source": [
    "gibbs1.print_topic_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using visualisation script defined in /LDAvis.js\n",
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8889/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Jul/2015 17:58:15] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Jul/2015 17:58:15] \"GET /LDAvis.css HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stopping Server...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Jul/2015 17:58:15] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Jul/2015 17:58:15] \"GET /LDAvis.js HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "model = gibbs1\n",
    "data = {}\n",
    "data['topic_term_dists'] = model.topic_word_\n",
    "data['doc_topic_dists'] = model.doc_topic_\n",
    "data['doc_lengths'] = model.cd\n",
    "data['vocab'] = model.vocab\n",
    "data['term_frequency'] = np.sum(model.ckn, axis=0)    \n",
    "vis_data = pyLDAvis.prepare(**data)  \n",
    "pyLDAvis.show(vis_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
