{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Notebook\n",
    "============"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Run LDA the first time</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "basedir = '../'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "from lda_for_fragments import Ms2Lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape (1588, 3171)\n"
     ]
    }
   ],
   "source": [
    "fragment_filename = basedir + 'input/final/Beer_3_full1_5_2E5_pos_fragments.csv'\n",
    "neutral_loss_filename = basedir + 'input/final/Beer_3_full1_5_2E5_pos_losses.csv'\n",
    "mzdiff_filename = None\n",
    "ms1_filename = basedir + 'input/final/Beer_3_full1_5_2E5_pos_ms1.csv'\n",
    "ms2_filename = basedir + 'input/final/Beer_3_full1_5_2E5_pos_ms2.csv'\n",
    "ms2lda = Ms2Lda.lcms_data_from_R(fragment_filename, neutral_loss_filename, mzdiff_filename, \n",
    "                             ms1_filename, ms2_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "CGS LDA initialising\n",
      "...............................................................................................................................................................\n",
      "Using Numba for LDA sampling\n",
      "Preparing words\n",
      "Preparing Z matrix\n",
      "DONE\n",
      "Burn-in 1 \n",
      "Burn-in 2 \n",
      "Burn-in 3 \n",
      "Burn-in 4 \n",
      "Burn-in 5 \n",
      "Burn-in 6 \n",
      "Burn-in 7 \n",
      "Burn-in 8 \n",
      "Burn-in 9 \n",
      "Burn-in 10 \n",
      "Burn-in 11 \n",
      "Burn-in 12 \n",
      "Burn-in 13 \n",
      "Burn-in 14 \n",
      "Burn-in 15 \n",
      "Burn-in 16 \n",
      "Burn-in 17 \n",
      "Burn-in 18 \n",
      "Burn-in 19 \n",
      "Burn-in 20 \n",
      "Burn-in 21 \n",
      "Burn-in 22 \n",
      "Burn-in 23 \n",
      "Burn-in 24 \n",
      "Burn-in 25 \n",
      "Burn-in 26 \n",
      "Burn-in 27 \n",
      "Burn-in 28 \n",
      "Burn-in 29 \n",
      "Burn-in 30 \n",
      "Burn-in 31 \n",
      "Burn-in 32 \n",
      "Burn-in 33 \n",
      "Burn-in 34 \n",
      "Burn-in 35 \n",
      "Burn-in 36 \n",
      "Burn-in 37 \n",
      "Burn-in 38 \n",
      "Burn-in 39 \n",
      "Burn-in 40 \n",
      "Burn-in 41 \n",
      "Burn-in 42 \n",
      "Burn-in 43 \n",
      "Burn-in 44 \n",
      "Burn-in 45 \n",
      "Burn-in 46 \n",
      "Burn-in 47 \n",
      "Burn-in 48 \n",
      "Burn-in 49 \n",
      "Burn-in 50 \n",
      "Burn-in 51 \n",
      "Burn-in 52 \n",
      "Burn-in 53 \n",
      "Burn-in 54 \n",
      "Burn-in 55 \n",
      "Burn-in 56 \n",
      "Burn-in 57 \n",
      "Burn-in 58 \n",
      "Burn-in 59 \n",
      "Burn-in 60 \n",
      "Burn-in 61 \n",
      "Burn-in 62 \n",
      "Burn-in 63 \n",
      "Burn-in 64 \n",
      "Burn-in 65 \n",
      "Burn-in 66 \n",
      "Burn-in 67 \n",
      "Burn-in 68 \n",
      "Burn-in 69 \n",
      "Burn-in 70 \n",
      "Burn-in 71 \n",
      "Burn-in 72 \n",
      "Burn-in 73 \n",
      "Burn-in 74 \n",
      "Burn-in 75 \n",
      "Burn-in 76 \n",
      "Burn-in 77 \n",
      "Burn-in 78 \n",
      "Burn-in 79 \n",
      "Burn-in 80 \n",
      "Burn-in 81 \n",
      "Burn-in 82 \n",
      "Burn-in 83 \n",
      "Burn-in 84 \n",
      "Burn-in 85 \n",
      "Burn-in 86 \n",
      "Burn-in 87 \n",
      "Burn-in 88 \n",
      "Burn-in 89 \n",
      "Burn-in 90 \n",
      "Burn-in 91 \n",
      "Burn-in 92 \n",
      "Burn-in 93 \n",
      "Burn-in 94 \n",
      "Burn-in 95 \n",
      "Burn-in 96 \n",
      "Burn-in 97 \n",
      "Burn-in 98 \n",
      "Burn-in 99 \n",
      "Burn-in 100 \n",
      "Burn-in 101 \n",
      "Burn-in 102 \n",
      "Burn-in 103 \n",
      "Burn-in 104 \n",
      "Burn-in 105 \n",
      "Burn-in 106 \n",
      "Burn-in 107 \n",
      "Burn-in 108 \n",
      "Burn-in 109 \n",
      "Burn-in 110 \n",
      "Burn-in 111 \n",
      "Burn-in 112 \n",
      "Burn-in 113 \n",
      "Burn-in 114 \n",
      "Burn-in 115 \n",
      "Burn-in 116 \n",
      "Burn-in 117 \n",
      "Burn-in 118 \n",
      "Burn-in 119 \n",
      "Burn-in 120 \n",
      "Burn-in 121 \n",
      "Burn-in 122 \n",
      "Burn-in 123 \n",
      "Burn-in 124 \n",
      "Burn-in 125 \n",
      "Burn-in 126 \n",
      "Burn-in 127 \n",
      "Burn-in 128 \n",
      "Burn-in 129 \n",
      "Burn-in 130 \n",
      "Burn-in 131 \n",
      "Burn-in 132 \n",
      "Burn-in 133 \n",
      "Burn-in 134 \n",
      "Burn-in 135 \n",
      "Burn-in 136 \n",
      "Burn-in 137 \n",
      "Burn-in 138 \n",
      "Burn-in 139 \n",
      "Burn-in 140 \n",
      "Burn-in 141 \n",
      "Burn-in 142 \n",
      "Burn-in 143 \n",
      "Burn-in 144 \n",
      "Burn-in 145 \n",
      "Burn-in 146 \n",
      "Burn-in 147 \n",
      "Burn-in 148 \n",
      "Burn-in 149 \n",
      "Burn-in 150 \n",
      "Burn-in 151 \n",
      "Burn-in 152 \n",
      "Burn-in 153 \n",
      "Burn-in 154 \n",
      "Burn-in 155 \n",
      "Burn-in 156 \n",
      "Burn-in 157 \n",
      "Burn-in 158 \n",
      "Burn-in 159 \n",
      "Burn-in 160 \n",
      "Burn-in 161 \n",
      "Burn-in 162 \n",
      "Burn-in 163 \n",
      "Burn-in 164 \n",
      "Burn-in 165 \n",
      "Burn-in 166 \n",
      "Burn-in 167 \n",
      "Burn-in 168 \n",
      "Burn-in 169 \n",
      "Burn-in 170 \n",
      "Burn-in 171 \n",
      "Burn-in 172 \n",
      "Burn-in 173 \n",
      "Burn-in 174 \n",
      "Burn-in 175 \n",
      "Burn-in 176 \n",
      "Burn-in 177 \n",
      "Burn-in 178 \n",
      "Burn-in 179 \n",
      "Burn-in 180 \n",
      "Burn-in 181 \n",
      "Burn-in 182 \n",
      "Burn-in 183 \n",
      "Burn-in 184 \n",
      "Burn-in 185 \n",
      "Burn-in 186 \n",
      "Burn-in 187 \n",
      "Burn-in 188 \n",
      "Burn-in 189 \n",
      "Burn-in 190 \n",
      "Burn-in 191 \n",
      "Burn-in 192 \n",
      "Burn-in 193 \n",
      "Burn-in 194 \n",
      "Burn-in 195 \n",
      "Burn-in 196 \n",
      "Burn-in 197 \n",
      "Burn-in 198 \n",
      "Burn-in 199 \n",
      "Burn-in 200 \n",
      "Burn-in 201 \n",
      "Burn-in 202 \n",
      "Burn-in 203 \n",
      "Burn-in 204 \n",
      "Burn-in 205 \n",
      "Burn-in 206 \n",
      "Burn-in 207 \n",
      "Burn-in 208 \n",
      "Burn-in 209 \n",
      "Burn-in 210 \n",
      "Burn-in 211 \n",
      "Burn-in 212 \n",
      "Burn-in 213 \n",
      "Burn-in 214 \n",
      "Burn-in 215 \n",
      "Burn-in 216 \n",
      "Burn-in 217 \n",
      "Burn-in 218 \n",
      "Burn-in 219 \n",
      "Burn-in 220 \n",
      "Burn-in 221 \n",
      "Burn-in 222 \n",
      "Burn-in 223 \n",
      "Burn-in 224 \n",
      "Burn-in 225 \n",
      "Burn-in 226 \n",
      "Burn-in 227 \n",
      "Burn-in 228 \n",
      "Burn-in 229 \n",
      "Burn-in 230 \n",
      "Burn-in 231 \n",
      "Burn-in 232 \n",
      "Burn-in 233 \n",
      "Burn-in 234 \n",
      "Burn-in 235 \n",
      "Burn-in 236 \n",
      "Burn-in 237 \n",
      "Burn-in 238 \n",
      "Burn-in 239 \n",
      "Burn-in 240 \n",
      "Burn-in 241 \n",
      "Burn-in 242 \n",
      "Burn-in 243 \n",
      "Burn-in 244 \n",
      "Burn-in 245 \n",
      "Burn-in 246 \n",
      "Burn-in 247 \n",
      "Burn-in 248 \n",
      "Burn-in 249 \n",
      "Sample 250 \n",
      "Sample 251 \n",
      "Sample 252 \n",
      "Sample 253 \n",
      "Sample 254 \n",
      "Sample 255   Log joint likelihood = -2100292.816 \n",
      "Sample 256 \n",
      "Sample 257 \n",
      "Sample 258 \n",
      "Sample 259 \n",
      "Sample 260   Log joint likelihood = -2100106.592 \n",
      "Sample 261 \n",
      "Sample 262 \n",
      "Sample 263 \n",
      "Sample 264 \n",
      "Sample 265   Log joint likelihood = -2100059.955 \n",
      "Sample 266 \n",
      "Sample 267 \n",
      "Sample 268 \n",
      "Sample 269 \n",
      "Sample 270   Log joint likelihood = -2100116.306 \n",
      "Sample 271 \n",
      "Sample 272 \n",
      "Sample 273 \n",
      "Sample 274 \n",
      "Sample 275   Log joint likelihood = -2100176.687 \n",
      "Sample 276 \n",
      "Sample 277 \n",
      "Sample 278 \n",
      "Sample 279 \n",
      "Sample 280   Log joint likelihood = -2099975.402 \n",
      "Sample 281 \n",
      "Sample 282 \n",
      "Sample 283 \n",
      "Sample 284 \n",
      "Sample 285   Log joint likelihood = -2099103.824 \n",
      "Sample 286 \n",
      "Sample 287 \n",
      "Sample 288 \n",
      "Sample 289 \n",
      "Sample 290   Log joint likelihood = -2099242.688 \n",
      "Sample 291 \n",
      "Sample 292 \n",
      "Sample 293 \n",
      "Sample 294 \n",
      "Sample 295   Log joint likelihood = -2098554.790 \n",
      "Sample 296 \n",
      "Sample 297 \n",
      "Sample 298 \n",
      "Sample 299 \n",
      "Sample 300   Log joint likelihood = -2099129.274 \n",
      "Sample 301 \n",
      "Sample 302 \n",
      "Sample 303 \n",
      "Sample 304 \n",
      "Sample 305   Log joint likelihood = -2098813.586 \n",
      "Sample 306 \n",
      "Sample 307 \n",
      "Sample 308 \n",
      "Sample 309 \n",
      "Sample 310   Log joint likelihood = -2098799.258 \n",
      "Sample 311 \n",
      "Sample 312 \n",
      "Sample 313 \n",
      "Sample 314 \n",
      "Sample 315   Log joint likelihood = -2098602.363 \n",
      "Sample 316 \n",
      "Sample 317 \n",
      "Sample 318 \n",
      "Sample 319 \n",
      "Sample 320   Log joint likelihood = -2098734.351 \n",
      "Sample 321 \n",
      "Sample 322 \n",
      "Sample 323 \n",
      "Sample 324 \n",
      "Sample 325   Log joint likelihood = -2098376.327 \n",
      "Sample 326 \n",
      "Sample 327 \n",
      "Sample 328 \n",
      "Sample 329 \n",
      "Sample 330   Log joint likelihood = -2097845.041 \n",
      "Sample 331 \n",
      "Sample 332 \n",
      "Sample 333 \n",
      "Sample 334 \n",
      "Sample 335   Log joint likelihood = -2098289.989 \n",
      "Sample 336 \n",
      "Sample 337 \n",
      "Sample 338 \n",
      "Sample 339 \n",
      "Sample 340   Log joint likelihood = -2097530.589 \n",
      "Sample 341 \n",
      "Sample 342 \n",
      "Sample 343 \n",
      "Sample 344 \n",
      "Sample 345   Log joint likelihood = -2098114.458 \n",
      "Sample 346 \n",
      "Sample 347 \n",
      "Sample 348 \n",
      "Sample 349 \n",
      "Sample 350   Log joint likelihood = -2097872.114 \n",
      "Sample 351 \n",
      "Sample 352 \n",
      "Sample 353 \n",
      "Sample 354 \n",
      "Sample 355   Log joint likelihood = -2098156.126 \n",
      "Sample 356 \n",
      "Sample 357 \n",
      "Sample 358 \n",
      "Sample 359 \n",
      "Sample 360   Log joint likelihood = -2097685.196 \n",
      "Sample 361 \n",
      "Sample 362 \n",
      "Sample 363 \n",
      "Sample 364 \n",
      "Sample 365   Log joint likelihood = -2097555.948 \n",
      "Sample 366 \n",
      "Sample 367 \n",
      "Sample 368 \n",
      "Sample 369 \n",
      "Sample 370   Log joint likelihood = -2097560.348 \n",
      "Sample 371 \n",
      "Sample 372 \n",
      "Sample 373 \n",
      "Sample 374 \n",
      "Sample 375   Log joint likelihood = -2098244.203 \n",
      "Sample 376 \n",
      "Sample 377 \n",
      "Sample 378 \n",
      "Sample 379 \n",
      "Sample 380   Log joint likelihood = -2098281.753 \n",
      "Sample 381 \n",
      "Sample 382 \n",
      "Sample 383 \n",
      "Sample 384 \n",
      "Sample 385   Log joint likelihood = -2098650.774 \n",
      "Sample 386 \n",
      "Sample 387 \n",
      "Sample 388 \n",
      "Sample 389 \n",
      "Sample 390   Log joint likelihood = -2098036.292 \n",
      "Sample 391 \n",
      "Sample 392 \n",
      "Sample 393 \n",
      "Sample 394 \n",
      "Sample 395   Log joint likelihood = -2097314.131 \n",
      "Sample 396 \n",
      "Sample 397 \n",
      "Sample 398 \n",
      "Sample 399 \n",
      "Sample 400   Log joint likelihood = -2097603.714 \n",
      "Sample 401 \n",
      "Sample 402 \n",
      "Sample 403 \n",
      "Sample 404 \n",
      "Sample 405   Log joint likelihood = -2097200.520 \n",
      "Sample 406 \n",
      "Sample 407 \n",
      "Sample 408 \n",
      "Sample 409 \n",
      "Sample 410   Log joint likelihood = -2097514.756 \n",
      "Sample 411 \n",
      "Sample 412 \n",
      "Sample 413 \n",
      "Sample 414 \n",
      "Sample 415   Log joint likelihood = -2097265.847 \n",
      "Sample 416 \n",
      "Sample 417 \n",
      "Sample 418 \n",
      "Sample 419 \n",
      "Sample 420   Log joint likelihood = -2096772.322 \n",
      "Sample 421 \n",
      "Sample 422 \n",
      "Sample 423 \n",
      "Sample 424 \n",
      "Sample 425   Log joint likelihood = -2096846.471 \n",
      "Sample 426 \n",
      "Sample 427 \n",
      "Sample 428 \n",
      "Sample 429 \n",
      "Sample 430   Log joint likelihood = -2096843.650 \n",
      "Sample 431 \n",
      "Sample 432 \n",
      "Sample 433 \n",
      "Sample 434 \n",
      "Sample 435   Log joint likelihood = -2097111.076 \n",
      "Sample 436 \n",
      "Sample 437 \n",
      "Sample 438 \n",
      "Sample 439 \n",
      "Sample 440   Log joint likelihood = -2098105.746 \n",
      "Sample 441 \n",
      "Sample 442 \n",
      "Sample 443 \n",
      "Sample 444 \n",
      "Sample 445   Log joint likelihood = -2097438.783 \n",
      "Sample 446 \n",
      "Sample 447 \n",
      "Sample 448 \n",
      "Sample 449 \n",
      "Sample 450   Log joint likelihood = -2097253.601 \n",
      "Sample 451 \n",
      "Sample 452 \n",
      "Sample 453 \n",
      "Sample 454 \n",
      "Sample 455   Log joint likelihood = -2096784.858 \n",
      "Sample 456 \n",
      "Sample 457 \n",
      "Sample 458 \n",
      "Sample 459 \n",
      "Sample 460   Log joint likelihood = -2096990.258 \n",
      "Sample 461 \n",
      "Sample 462 \n",
      "Sample 463 \n",
      "Sample 464 \n",
      "Sample 465   Log joint likelihood = -2096935.712 \n",
      "Sample 466 \n",
      "Sample 467 \n",
      "Sample 468 \n",
      "Sample 469 \n",
      "Sample 470   Log joint likelihood = -2096241.138 \n",
      "Sample 471 \n",
      "Sample 472 \n",
      "Sample 473 \n",
      "Sample 474 \n",
      "Sample 475   Log joint likelihood = -2096891.731 \n",
      "Sample 476 \n",
      "Sample 477 \n",
      "Sample 478 \n",
      "Sample 479 \n",
      "Sample 480   Log joint likelihood = -2096526.992 \n",
      "Sample 481 \n",
      "Sample 482 \n",
      "Sample 483 \n",
      "Sample 484 \n",
      "Sample 485   Log joint likelihood = -2097390.929 \n",
      "Sample 486 \n",
      "Sample 487 \n",
      "Sample 488 \n",
      "Sample 489 \n",
      "Sample 490   Log joint likelihood = -2097390.034 \n",
      "Sample 491 \n",
      "Sample 492 \n",
      "Sample 493 \n",
      "Sample 494 \n",
      "Sample 495   Log joint likelihood = -2096477.758 \n",
      "Sample 496 \n",
      "Sample 497 \n",
      "Sample 498 \n",
      "Sample 499 \n",
      "Sample 500   Log joint likelihood = -2096575.921 \n",
      "Using all samples\n",
      "DONE. Time=1345.12675595\n"
     ]
    }
   ],
   "source": [
    "### all the parameters you need to specify to run LDA ###\n",
    "\n",
    "n_topics = 300 # 300 - 400 topics from cross-validation\n",
    "n_samples = 500 # 100 is probably okay for testing. For manuscript, use > 500-1000.\n",
    "n_burn = 250 # if 0 then we only use the last sample\n",
    "n_thin = 5 # every n-th sample to use for averaging after burn-in\n",
    "alpha = 50.0/n_topics # hyper-parameter for document-topic distributions\n",
    "beta = 0.1 # hyper-parameter for topic-word distributions\n",
    "\n",
    "ms2lda.run_lda(n_topics, n_samples, n_burn, n_thin, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing topics to results/beer3_test_method3/beer3_test_method3_topics.csv\n",
      "Writing fragments x topics to results/beer3_test_method3/beer3_test_method3_all.csv\n",
      "Writing topic docs to results/beer3_test_method3/beer3_test_method3_docs.csv\n",
      "Project saved to results/beer3pos.project time taken = 21.3921849728\n"
     ]
    }
   ],
   "source": [
    "ms2lda.write_results('beer3_test_method3')\n",
    "ms2lda.save_project('results/beer3pos.project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Resuming from Previous Run</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did the save_project() above, you can resume from this step directly the next time you load the notebook .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "basedir = '../'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "from lda_for_fragments import Ms2Lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project loaded from results/beer3pos.project time taken = 17.6787488461\n",
      " - input_filenames = \n",
      "\t../input/final/Beer_3_full1_5_2E5_pos_fragments.csv\n",
      "\t../input/final/Beer_3_full1_5_2E5_pos_losses.csv\n",
      "\t../input/final/Beer_3_full1_5_2E5_pos_ms1.csv\n",
      "\t../input/final/Beer_3_full1_5_2E5_pos_ms2.csv\n",
      " - df.shape = (1588, 3171)\n",
      " - K = 300\n",
      " - alpha = 0.166666666667\n",
      " - beta = 0.1\n",
      " - last_saved_timestamp = Thu Aug  6 16:13:04 2015\n"
     ]
    }
   ],
   "source": [
    "ms2lda = Ms2Lda.resume_from('results/beer3pos.project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Visualisation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the 'interactive' parameter below is True, we will show an interactive visualisation of the results in a separate tab. You need to interrupt the kernel to stop it once you're done with it (from the menu above, Kernel > Interrupt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: fragment_176.87617 (0.305917347625), fragment_119.04873 (0.236429857941), loss_54.01014 (0.0593820676499),\n",
      "Topic 1: fragment_275.11062 (0.562559598374), fragment_159.02737 (0.190113652915),\n",
      "Topic 2: fragment_119.04988 (0.159031248426), fragment_272.14799 (0.120978493019), fragment_240.12245 (0.106465355378), loss_96.04228 (0.0558866742727),\n",
      "Topic 3: fragment_121.06488 (0.632489599641), fragment_103.05448 (0.112052512239), fragment_93.06981 (0.0846015130716),\n",
      "Topic 4: fragment_130.05044 (0.871631350453),\n",
      "Topic 5: fragment_53.00259 (0.49781597501), fragment_85.06476 (0.366667217937),\n",
      "Topic 6: loss_143.05788 (0.664422815706), fragment_215.13975 (0.117796346517),\n",
      "Topic 7: fragment_118.08616 (0.638593921988), fragment_132.11306 (0.0966651989898),\n",
      "Topic 8: fragment_153.06589 (0.191235471369), loss_119.06984 (0.126144913845), fragment_143.01705 (0.105913741549), loss_115.02683 (0.0635469443177), fragment_366.08148 (0.0608730622739),\n",
      "Topic 9: loss_53.04741 (0.299999048735), fragment_206.07152 (0.112516286614), fragment_287.12847 (0.0697185489705), fragment_274.12651 (0.0605513629679), fragment_175.14848 (0.0562134769726),\n",
      "Topic 10: loss_92.04705 (0.394143940308),\n",
      "Topic 11: loss_55.98946 (0.341098566288), loss_27.98353 (0.103600866241), fragment_65.03879 (0.0978546338063), fragment_111.04425 (0.0683429975251),\n",
      "Topic 12: loss_17.02617 (0.475157424077), fragment_111.11655 (0.247843722605), loss_27.01099 (0.0512716188891),\n",
      "Topic 13: fragment_146.08111 (0.228046464715), loss_158.058 (0.164858742072), fragment_194.118 (0.0751129575423), fragment_82.06497 (0.0746461907658), fragment_138.05453 (0.0729071743727), fragment_110.05977 (0.0577058270079),\n",
      "Topic 14: loss_18.01013 (0.898737788716),\n",
      "Topic 15: fragment_228.15926 (0.152300603634), fragment_176.96089 (0.151600092124), fragment_202.978 (0.0890715461984), loss_147.06464 (0.076886946527), loss_159.12564 (0.0728212698565),\n",
      "Topic 16: fragment_128.0705 (0.412108301817), loss_88.08852 (0.0910603619561), fragment_174.14818 (0.0666688770539), fragment_71.06024 (0.0650990408358),\n",
      "Topic 17: loss_43.00565 (0.241694898408), fragment_94.06563 (0.205426532672), fragment_178.1231 (0.0775464036589), loss_61.01619 (0.0525105014325),\n",
      "Topic 18: fragment_56.04977 (0.920275134465),\n",
      "Topic 19: fragment_89.03864 (0.198584347109), fragment_335.0942 (0.127072784631), fragment_163.03895 (0.0977274831765), fragment_160.06129 (0.0938579790168), fragment_132.0814 (0.0920395044335), fragment_307.0983 (0.0593726818877),\n",
      "Topic 20: fragment_56.9651 (0.319334122817), loss_46.04151 (0.19367867294), fragment_102.05546 (0.0673735819937), fragment_82.02903 (0.050754432604),\n",
      "Topic 21: fragment_89.06008 (0.314381826541), fragment_133.08628 (0.19378216233),\n",
      "Topic 22: fragment_143.03188 (0.288429916094), loss_187.08399 (0.174769240603), fragment_168.10235 (0.0845235290344), fragment_113.02152 (0.083032225158),\n",
      "Topic 23: loss_115.06314 (0.361357224434), loss_161.0684 (0.209941236802), fragment_114.06689 (0.159265938205),\n",
      "Topic 24: loss_59.03694 (0.737379671822),\n",
      "Topic 25: fragment_95.04936 (0.482621196895), fragment_182.11708 (0.11205384147), fragment_204.1162 (0.067799659079),\n",
      "Topic 26: loss_74.07299 (0.360837115982), loss_70.00532 (0.256198110261), fragment_99.00811 (0.0853501235029),\n",
      "Topic 27: loss_102.06795 (0.462916615145), fragment_142.1225 (0.160104971171), loss_177.09984 (0.0529410223759), fragment_206.08239 (0.0529306454578),\n",
      "Topic 28: fragment_201.12852 (0.147209129329), loss_174.06378 (0.126486280324), fragment_173.13209 (0.0587237567013),\n",
      "Topic 29: loss_116.06679 (0.206848453591), fragment_143.0738 (0.155600661289), fragment_117.07403 (0.137855296643), loss_162.07263 (0.0739037406448),\n",
      "Topic 30: loss_189.06316 (0.292512162389),\n",
      "Topic 31: loss_150.05243 (0.354903137747), loss_101.06797 (0.0641277203245), fragment_212.05412 (0.060630553639),\n",
      "Topic 32: fragment_83.06041 (0.446161879609), fragment_111.05543 (0.195512899005), fragment_82.05232 (0.141101341558),\n",
      "Topic 33: fragment_126.0919 (0.568449929797), loss_126.03165 (0.0946254575588),\n",
      "Topic 34: loss_17.02532 (0.236365378417), loss_180.09019 (0.091647076653), fragment_219.06551 (0.0790395562404), fragment_210.05357 (0.0790395562404), fragment_319.15176 (0.0789914573678), fragment_275.12794 (0.0716308652791), fragment_227.11998 (0.0545457600622),\n",
      "Topic 35: loss_87.07973 (0.205426507584), fragment_87.04393 (0.171157430797), loss_59.04829 (0.133609558395), fragment_115.07544 (0.131892032769), fragment_174.12434 (0.089491338871),\n",
      "Topic 36: loss_42.01041 (0.486819275675), fragment_124.08694 (0.0505674999232), fragment_329.17529 (0.0502558384297),\n",
      "Topic 37: fragment_64.92754 (0.282330210628), fragment_102.05469 (0.179240348474), fragment_87.06381 (0.0792176389358), fragment_123.96388 (0.0785433142688), fragment_123.96519 (0.0610702184053),\n",
      "Topic 38: loss_78.03156 (0.655488208087), loss_94.02691 (0.087095411013),\n",
      "Topic 39: fragment_84.08072 (0.864424223241),\n",
      "Topic 40: fragment_130.08588 (0.295253086176), fragment_150.09234 (0.105538191251), fragment_233.14961 (0.054366394025),\n",
      "Topic 41: fragment_55.05466 (0.93231204726),\n",
      "Topic 42: loss_36.0226 (0.141527791298), fragment_389.16928 (0.134343429263), fragment_263.13824 (0.0577635995631),\n",
      "Topic 43: fragment_72.04468 (0.508047965875), fragment_249.11671 (0.0939490856938), fragment_92.05779 (0.053945042117),\n",
      "Topic 44: fragment_112.05107 (0.369821475451), loss_133.04521 (0.197397051298), fragment_113.05456 (0.0753712740362),\n",
      "Topic 45: loss_116.05856 (0.159944353137), loss_162.06423 (0.129708222405), fragment_200.14096 (0.105571245918), loss_161.0592 (0.076980388059),\n",
      "Topic 46: loss_129.04229 (0.435270547918), fragment_119.08937 (0.247734654558),\n",
      "Topic 47: loss_17.02681 (0.442132602534), fragment_96.02703 (0.151459105257), fragment_113.04771 (0.0818315523287),\n",
      "Topic 48: fragment_60.05587 (0.333027919217), fragment_131.12919 (0.13159739149), fragment_165.07002 (0.0560726341943),\n",
      "Topic 49: loss_88.05221 (0.345109472518), loss_76.05228 (0.125778778412), fragment_130.06468 (0.114663358296), fragment_132.04417 (0.0614739015625), fragment_209.04387 (0.0568032884522),\n",
      "Topic 50: fragment_69.07013 (0.918887144322),\n",
      "Topic 51: fragment_113.06015 (0.101099546363), loss_98.05764 (0.0749857008669), loss_54.03152 (0.0664172246747),\n",
      "Topic 52: fragment_88.0218 (0.248388816242), fragment_134.02762 (0.0956559056219), fragment_121.0763 (0.0889193056544), loss_182.07866 (0.0888118213973),\n",
      "Topic 53: fragment_120.08041 (0.0800505441142), fragment_292.11654 (0.0788471807576), fragment_310.12703 (0.0766856641035), loss_18.01208 (0.0687168783261), loss_36.0226 (0.0593797344304), fragment_91.05413 (0.0532514726911), fragment_103.05448 (0.052328948412),\n",
      "Topic 54: fragment_262.12857 (0.174739905124), fragment_244.11769 (0.124101181852), fragment_72.0807 (0.0830472229071), fragment_216.12458 (0.0645426402436), fragment_84.08072 (0.0611068146445),\n",
      "Topic 55: fragment_152.05769 (0.267340960797), fragment_153.04073 (0.0911417953171), fragment_265.15335 (0.0698551521469), fragment_153.06148 (0.0664193313544),\n",
      "Topic 56: loss_90.0315 (0.382455375037), loss_48.02102 (0.177933915567), fragment_123.09132 (0.0880577170055), fragment_303.15518 (0.051565499308),\n",
      "Topic 57: loss_105.04249 (0.438946373033), fragment_102.09179 (0.125648619628), fragment_144.10184 (0.108201113832), fragment_85.06476 (0.0852625982412), loss_123.05306 (0.0712205126388),\n",
      "Topic 58: fragment_264.10758 (0.078846581588), fragment_74.06031 (0.0765315099102), fragment_230.15973 (0.0733297458092), fragment_186.07564 (0.0645183262932), fragment_120.06535 (0.0583222819132), fragment_102.05469 (0.0548890915695),\n",
      "Topic 59: loss_65.04764 (0.230529365998), fragment_238.07137 (0.173417754991), fragment_109.02883 (0.0577313432511), fragment_162.07573 (0.0561338721152),\n",
      "Topic 60: fragment_59.0494 (0.59109805901), loss_134.09368 (0.116697039393),\n",
      "Topic 61: fragment_62.92943 (0.327231130163), loss_35.0368 (0.205101888226), fragment_172.07638 (0.0865406164312), fragment_157.01328 (0.0710123241983), fragment_121.96576 (0.0538645506772),\n",
      "Topic 62: fragment_85.08407 (0.315832263013), fragment_112.03979 (0.0571329749731), fragment_134.04526 (0.0514027853704),\n",
      "Topic 63: loss_106.02641 (0.550434504645), fragment_68.9972 (0.0748875363253), fragment_187.88245 (0.0685190474646),\n",
      "Topic 64: fragment_281.15036 (0.16858301268), fragment_147.09166 (0.131526450056), fragment_103.02165 (0.0839537059994), loss_18.01269 (0.0670268378376), loss_134.05763 (0.0665960604617), fragment_303.13504 (0.0599075461988), fragment_120.08149 (0.052037638675),\n",
      "Topic 65: fragment_67.05453 (0.666372045065), fragment_93.06981 (0.138385102023), fragment_95.0854 (0.0662554635988),\n",
      "Topic 66: fragment_225.12354 (0.111999090842), fragment_128.07019 (0.106766196363), loss_65.04764 (0.0750024632149), fragment_82.06497 (0.0617221059715), fragment_110.05977 (0.0519612651369),\n",
      "Topic 67: loss_62.00037 (0.22246857648), loss_89.01109 (0.068700619915), fragment_148.1123 (0.0678922715875), fragment_149.01164 (0.0675804153091), fragment_175.12228 (0.0526869182813),\n",
      "Topic 68: fragment_177.05475 (0.227727259123), fragment_89.03864 (0.19072541079), fragment_145.02842 (0.178207202406), fragment_117.03315 (0.112916132632), loss_193.05888 (0.0536610199808),\n",
      "Topic 69: loss_188.07989 (0.100199209553), fragment_146.05935 (0.0891443213109), fragment_301.14341 (0.079924461772), fragment_326.12622 (0.0789428982391), fragment_337.19122 (0.0529970024484), fragment_319.1564 (0.0521436870666), fragment_119.08567 (0.0502943598935),\n",
      "Topic 70: fragment_73.06468 (0.490192393708), fragment_101.05977 (0.144454573111), loss_46.01664 (0.0612961899464),\n",
      "Topic 71: fragment_231.08319 (0.157018807996), fragment_158.09745 (0.128273017097), fragment_143.0738 (0.0875211862421), fragment_99.06318 (0.0840438554084), fragment_213.0955 (0.0586922916537),\n",
      "Topic 72: fragment_157.05048 (0.179044567975), fragment_53.00259 (0.136808072624), loss_140.11981 (0.0768904779476), fragment_195.06558 (0.0765572676531),\n",
      "Topic 73: fragment_55.01824 (0.685872740594), fragment_143.01554 (0.0800382365575), fragment_89.00596 (0.055299989718),\n",
      "Topic 74: fragment_125.07097 (0.591103160799),\n",
      "Topic 75: fragment_137.07044 (0.163132517629), loss_43.01826 (0.153257179986), loss_41.99961 (0.153010617032), fragment_94.04142 (0.147606219512), fragment_95.05998 (0.134297191218), fragment_314.0858 (0.0527501583263),\n",
      "Topic 76: loss_17.02652 (0.152074832097), fragment_276.14356 (0.139575239053), fragment_258.13323 (0.136809031333), fragment_230.13968 (0.112002081713), fragment_276.11298 (0.064238238401),\n",
      "Topic 77: loss_46.00529 (0.927644111462),\n",
      "Topic 78: fragment_132.10156 (0.165496901355), fragment_143.11763 (0.117105102504), fragment_171.11257 (0.0951830206193), fragment_107.06037 (0.0901440699247), fragment_189.12422 (0.0901172694824), loss_188.11573 (0.0517229149571),\n",
      "Topic 79: fragment_118.06547 (0.243471992521), fragment_188.07054 (0.0843298360727), fragment_130.06468 (0.0737571447473),\n",
      "Topic 80: loss_158.13029 (0.111973056405), fragment_221.08133 (0.110045810181), loss_140.11981 (0.058949698534), fragment_239.09207 (0.0569029932605),\n",
      "Topic 81: fragment_114.05539 (0.644436192791),\n",
      "Topic 82: loss_162.05272 (0.768751512117), fragment_266.15956 (0.0695710336676),\n",
      "Topic 83: fragment_137.04628 (0.3173483637), fragment_140.06868 (0.25371040744), fragment_74.09666 (0.131449672281),\n",
      "Topic 84: loss_137.08438 (0.157448766237), loss_43.04187 (0.10004565443), fragment_109.02883 (0.0808439848405), fragment_139.03824 (0.0784663374399), fragment_195.00355 (0.0784345596417), fragment_126.05566 (0.0636661437672),\n",
      "Topic 85: fragment_261.11313 (0.0551516356029),\n",
      "Topic 86: fragment_111.07435 (0.126521074029), fragment_258.10968 (0.0834856994928), fragment_416.26144 (0.0631717682779),\n",
      "Topic 87: loss_132.04201 (0.653957475033), fragment_166.07209 (0.0634176530932),\n",
      "Topic 88: loss_113.08398 (0.348191593081), fragment_248.10224 (0.129750604393), fragment_160.13255 (0.0775188393183), fragment_202.09763 (0.0695774712096),\n",
      "Topic 89: loss_118.06296 (0.233803397578), loss_133.085 (0.0913013870924), fragment_195.08875 (0.0776319728591), fragment_82.05232 (0.0507003752187),\n",
      "Topic 90: loss_18.01208 (0.260860657217), fragment_283.10602 (0.12943848405), fragment_140.04999 (0.0642093176459), loss_161.0684 (0.0640201998656), fragment_185.07115 (0.0639161281775), loss_116.0471 (0.0634071926055), fragment_211.08559 (0.0543842258426),\n",
      "Topic 91: fragment_200.12846 (0.260795771224), fragment_247.14087 (0.130451034766), loss_134.05763 (0.0697692639089),\n",
      "Topic 92: fragment_144.10184 (0.322605159577), fragment_98.09684 (0.133680361824), fragment_495.26508 (0.12643698881),\n",
      "Topic 93: loss_18.00971 (0.519459732467), fragment_209.118 (0.053559629014),\n",
      "Topic 94: loss_144.08962 (0.210501222485), loss_156.05305 (0.188459454786), loss_158.06888 (0.161388571886), loss_17.02575 (0.0608790613654), loss_142.07388 (0.0571172391766),\n",
      "Topic 95: loss_87.06831 (0.310121479209), fragment_125.108 (0.111627002618), loss_152.05832 (0.0728085317966),\n",
      "Topic 96: fragment_71.06873 (0.82573417921),\n",
      "Topic 97: loss_91.02671 (0.383392837396), loss_141.04228 (0.222310232856), fragment_134.09687 (0.0583738540774), fragment_261.09335 (0.0564640847391),\n",
      "Topic 98: fragment_81.07001 (0.433889892176), fragment_93.06981 (0.141258580488), fragment_107.08559 (0.109177743954), fragment_95.0854 (0.083017238479),\n",
      "Topic 99: fragment_303.05017 (0.221610936074), fragment_138.06834 (0.110838485785), fragment_286.15268 (0.110816461412),\n",
      "Topic 100: loss_144.04211 (0.414340830616), loss_190.04773 (0.215852182919), loss_177.06356 (0.0720514047808),\n",
      "Topic 101: fragment_90.05538 (0.394896884507), fragment_234.09784 (0.171326841284),\n",
      "Topic 102: loss_30.01064 (0.35570250356), loss_58.00531 (0.234122019235),\n",
      "Topic 103: fragment_112.08703 (0.292241713815), fragment_337.16901 (0.069509677404),\n",
      "Topic 104: fragment_157.1084 (0.0687786409646), fragment_425.21615 (0.0554001070785), fragment_60.05587 (0.0518210322982),\n",
      "Topic 105: fragment_87.04393 (0.599426657557), loss_133.07371 (0.110352405519), fragment_146.09281 (0.0702618447395),\n",
      "Topic 106: fragment_276.15635 (0.148687828504), fragment_84.06386 (0.132563170401), fragment_286.06955 (0.0799655058886), fragment_263.06899 (0.0590635165698),\n",
      "Topic 107: loss_36.02112 (0.645141756005), fragment_164.09255 (0.0656749208533),\n",
      "Topic 108:\n",
      "Topic 109: fragment_72.0807 (0.932815072163),\n",
      "Topic 110: loss_59.98469 (0.362068589322), fragment_146.11805 (0.132252655957), fragment_177.10201 (0.0815408903586), loss_91.06321 (0.0657541418279),\n",
      "Topic 111: fragment_88.03907 (0.45150400273), fragment_87.0554 (0.0970093247402), fragment_133.06135 (0.0813658842014), fragment_70.03679 (0.0759805842784),\n",
      "Topic 112: loss_60.05733 (0.179319500856), loss_42.04671 (0.0917405449992), fragment_126.05566 (0.0874003135455), fragment_203.07458 (0.0859907072321),\n",
      "Topic 113: fragment_126.0665 (0.300748830669), loss_45.05755 (0.145880993361), fragment_385.20139 (0.0927142727572), fragment_205.05127 (0.0528775356937),\n",
      "Topic 114: fragment_53.03893 (0.670556757572), loss_55.0056 (0.102232654951),\n",
      "Topic 115: loss_59.04829 (0.389926800045), fragment_147.09706 (0.0906558950296), fragment_88.04762 (0.0809555728634), fragment_87.06309 (0.0753442875916),\n",
      "Topic 116: fragment_110.05977 (0.427706456806), fragment_126.05566 (0.146587948303), fragment_108.04424 (0.133248968451),\n",
      "Topic 117: fragment_85.02832 (0.918621199094),\n",
      "Topic 118: fragment_239.09207 (0.178760703637), loss_56.06191 (0.176214161467), fragment_246.13408 (0.0898381950902), fragment_98.09684 (0.0585563864003),\n",
      "Topic 119: fragment_109.07641 (0.137940786998), fragment_259.13135 (0.0773967303479),\n",
      "Topic 120: fragment_152.05608 (0.422893264405), fragment_153.04073 (0.151678650509),\n",
      "Topic 121: fragment_110.0718 (0.435565394061), fragment_93.04508 (0.0851507760281), fragment_235.12022 (0.082750515284), fragment_83.06041 (0.0772747973543),\n",
      "Topic 122: fragment_80.0495 (0.716725028471), loss_27.99522 (0.157464322354),\n",
      "Topic 123: loss_73.01609 (0.561266901243), fragment_141.01851 (0.0693236449883), fragment_277.14277 (0.0668130161122),\n",
      "Topic 124: fragment_211.07077 (0.259221990925), fragment_277.09025 (0.209074971243), fragment_259.09201 (0.0784971176243), fragment_255.09603 (0.0688930010547),\n",
      "Topic 125: fragment_84.04485 (0.572513548446), fragment_185.09255 (0.06799651154),\n",
      "Topic 126: fragment_248.11293 (0.261085836481), loss_97.05255 (0.108057897594), fragment_287.09125 (0.0521275266377),\n",
      "Topic 127: fragment_123.05504 (0.474300076787), fragment_141.06615 (0.0973302401328),\n",
      "Topic 128: loss_99.0682 (0.0628223651977), fragment_179.06939 (0.0611932370017), fragment_112.04996 (0.0610092103952), loss_123.05306 (0.0569462850717),\n",
      "Topic 129: fragment_164.03449 (0.277657769366), fragment_80.0495 (0.198818049259), fragment_182.08222 (0.0934739881418), loss_86.03662 (0.0766581132661), loss_18.04677 (0.0563499751608),\n",
      "Topic 130: loss_36.02195 (0.284369987729), fragment_200.06966 (0.090019256025), fragment_202.14431 (0.0896781695497),\n",
      "Topic 131: fragment_121.02828 (0.172127765534), loss_158.13029 (0.13444247954), fragment_81.07001 (0.0749754707354), loss_198.08902 (0.0711579786295), fragment_357.24902 (0.0664846019243), fragment_205.08717 (0.058710642932), fragment_223.06189 (0.0558547407539),\n",
      "Topic 132: fragment_149.05999 (0.0762662835303), fragment_121.06488 (0.0758361243858), fragment_107.08559 (0.0715159989295), fragment_191.14367 (0.0691265894764), fragment_209.15543 (0.0614990093206), fragment_78.04656 (0.0533179792796),\n",
      "Topic 133: loss_113.04753 (0.339912034066), loss_159.05301 (0.100382516522), fragment_325.09018 (0.0943514950547), loss_143.02178 (0.0833366124747),\n",
      "Topic 134: fragment_62.06024 (0.186608475498), fragment_215.10369 (0.0982686315618), fragment_167.08153 (0.0753356282528), loss_84.04244 (0.0699464129368), fragment_70.02896 (0.0540175312418),\n",
      "Topic 135: loss_188.11573 (0.0996143948507), loss_110.0215 (0.062635118719), fragment_246.18182 (0.0624233843979), fragment_204.15456 (0.0623232259086), loss_134.02127 (0.0583202739622), loss_98.02137 (0.0528572009826),\n",
      "Topic 136: loss_145.07367 (0.322977260922), loss_126.104 (0.276070062419), fragment_60.04476 (0.0735760517926),\n",
      "Topic 137: loss_60.02098 (0.866665582224),\n",
      "Topic 138: loss_173.06836 (0.106273983763), fragment_100.11239 (0.105231701794), loss_66.03185 (0.0567425885811), fragment_188.07054 (0.0550016696332),\n",
      "Topic 139: fragment_232.08297 (0.132210731148), fragment_250.09291 (0.107118509732), loss_124.07375 (0.106055184411),\n",
      "Topic 140: fragment_147.07584 (0.271858412473), loss_154.13575 (0.215924128903), fragment_102.05469 (0.06572211069),\n",
      "Topic 141: fragment_136.06236 (0.775322310704),\n",
      "Topic 142: fragment_182.08222 (0.253939091064), fragment_136.076 (0.080457342695), fragment_160.07483 (0.0692157277003), fragment_123.04464 (0.0548859105436), loss_115.10009 (0.0501188288723),\n",
      "Topic 143: fragment_81.03333 (0.917193220644),\n",
      "Topic 144: fragment_98.06001 (0.92146129367),\n",
      "Topic 145: fragment_214.1078 (0.0970601069183), fragment_88.07588 (0.0920209813501), fragment_236.99129 (0.0766684354066), loss_176.03168 (0.0658938415798),\n",
      "Topic 146: fragment_184.07217 (0.217436031096), fragment_124.99932 (0.136686238994), loss_45.02202 (0.131304106075), fragment_178.13438 (0.0831167343982), fragment_71.07323 (0.0532372443109), fragment_133.03667 (0.0531535753249), loss_85.08925 (0.0501671093157),\n",
      "Topic 147: loss_36.0226 (0.0962276520234), fragment_308.11163 (0.0956923271852), fragment_123.04464 (0.085381010975), fragment_95.04936 (0.0806492544887), fragment_133.04987 (0.0589795440508), fragment_119.04873 (0.054933614545), fragment_148.07572 (0.0545323849761),\n",
      "Topic 148: fragment_103.05448 (0.381713390587), fragment_122.068 (0.0906313582964), fragment_63.00673 (0.0563280464317), fragment_439.12939 (0.0562504414912),\n",
      "Topic 149: fragment_138.05087 (0.171902798451), fragment_251.12762 (0.0857643015902), fragment_295.09447 (0.0747322645728),\n",
      "Topic 150: fragment_105.07018 (0.72805509296),\n",
      "Topic 151: fragment_53.03881 (0.378830643348), fragment_163.07456 (0.0630490508338), fragment_95.02391 (0.0629925968134), fragment_150.09067 (0.0598501932161), fragment_195.10184 (0.0543049593329), loss_122.05728 (0.0530657944799),\n",
      "Topic 152: loss_91.06321 (0.208997784871), fragment_101.09635 (0.143136115857), fragment_133.08628 (0.114154459022),\n",
      "Topic 153: fragment_144.06589 (0.201146115667), loss_118.02662 (0.159092367957), fragment_186.05884 (0.057289399239), fragment_233.11895 (0.0572550035066), loss_144.07803 (0.0563456767957), fragment_204.10238 (0.0548283731121), loss_60.05733 (0.0543349394566),\n",
      "Topic 154: fragment_225.07569 (0.0998034343113),\n",
      "Topic 155: loss_134.05763 (0.159448892281), fragment_377.1485 (0.110454382122), fragment_243.08643 (0.0905825292662), loss_126.05281 (0.075219845622), fragment_172.08725 (0.0508277927241),\n",
      "Topic 156: fragment_239.09207 (0.0713878745872), fragment_345.20828 (0.0692052865195), fragment_327.19605 (0.0667754611273), fragment_91.05413 (0.0532452001247),\n",
      "Topic 157: fragment_261.11313 (0.181620296063), fragment_195.02932 (0.0950129909969), loss_18.00923 (0.0724883053097), fragment_211.0969 (0.0611214031938), loss_68.02603 (0.0593828118065), fragment_279.12277 (0.0547600616117), fragment_205.04866 (0.0526013206809),\n",
      "Topic 158: fragment_79.05416 (0.711433793783), fragment_93.06981 (0.0683014397018),\n",
      "Topic 159: fragment_57.03371 (0.848635456072),\n",
      "Topic 160: loss_64.01586 (0.722699293154),\n",
      "Topic 161: loss_180.06319 (0.57979178692), loss_198.07396 (0.185637544881), fragment_120.09034 (0.0589095821782),\n",
      "Topic 162: fragment_156.04141 (0.273108331217), fragment_103.0239 (0.233429239157), fragment_151.04573 (0.142412606662), loss_36.01969 (0.0600472796787), fragment_272.11403 (0.0583867381767),\n",
      "Topic 163: fragment_114.10277 (0.352760759367), fragment_131.12919 (0.0719288302449), loss_170.07967 (0.0590714663819), fragment_242.16142 (0.0522833775046), loss_128.05815 (0.0518397336428),\n",
      "Topic 164: loss_103.03787 (0.201732662735), fragment_176.10671 (0.127241847864), fragment_352.16144 (0.105457392655),\n",
      "Topic 165: fragment_71.04937 (0.46154263783), fragment_167.03443 (0.124404122677), loss_124.12506 (0.0944231697527),\n",
      "Topic 166: fragment_82.06497 (0.272447634999), fragment_128.10735 (0.20093353325), fragment_128.0705 (0.126025975668), fragment_111.02746 (0.0656690459398),\n",
      "Topic 167: loss_108.04219 (0.103749802619), loss_82.02688 (0.0948025721011), fragment_146.08111 (0.0834465679512), fragment_101.05977 (0.0573060028934), fragment_128.0705 (0.0524504817303), fragment_82.06497 (0.0504380093789),\n",
      "Topic 168: fragment_104.10736 (0.563238045155), fragment_60.08102 (0.0720534271876), loss_154.00263 (0.0701414395857), fragment_258.10968 (0.052435062488),\n",
      "Topic 169: fragment_181.09787 (0.323410557121), fragment_209.09229 (0.249951754853),\n",
      "Topic 170: loss_84.04244 (0.298706685006), fragment_153.06035 (0.179282793692), loss_96.04228 (0.0728882336184), fragment_154.04459 (0.0698231936611),\n",
      "Topic 171: fragment_150.07833 (0.1820003407), fragment_221.08133 (0.17517379747), fragment_361.20073 (0.0664464379818),\n",
      "Topic 172: loss_92.01077 (0.139027761178), fragment_225.07569 (0.107394343534), fragment_179.06939 (0.0931914839219), fragment_151.03854 (0.0808024593275), loss_120.04217 (0.0764979023793), fragment_317.064 (0.05332471983), loss_176.03168 (0.0508352421239), fragment_95.04936 (0.050567687606),\n",
      "Topic 173: fragment_90.09154 (0.0981213802072), fragment_189.88155 (0.0973398435667), fragment_108.04424 (0.0752463403239),\n",
      "Topic 174: fragment_173.11737 (0.0774522174556), fragment_331.19058 (0.0523566001482),\n",
      "Topic 175: fragment_129.05467 (0.214153438371), fragment_147.06554 (0.132451222326), fragment_86.03199 (0.0751160334614), loss_99.05299 (0.0705008683903), fragment_111.04425 (0.0665806475987), loss_115.04894 (0.0584705815493), loss_81.04231 (0.0544886611064),\n",
      "Topic 176: fragment_221.01907 (0.240445407171), loss_162.09997 (0.167905529817), fragment_280.09383 (0.166955768724), loss_193.99396 (0.0982403567633), fragment_116.9864 (0.0668250841055),\n",
      "Topic 177: fragment_85.04763 (0.241487506677), loss_144.06164 (0.118224601628), fragment_117.07403 (0.110671804101), fragment_211.09389 (0.0641822016779), fragment_182.10119 (0.0598830864715),\n",
      "Topic 178: fragment_193.07569 (0.0926141977855), fragment_182.08476 (0.0860333526786), fragment_287.13715 (0.0825070429963), fragment_58.99527 (0.0822706412846), fragment_184.04372 (0.0597165385987), loss_171.0528 (0.0573673407216), fragment_209.1073 (0.0538053594875),\n",
      "Topic 179: loss_64.01586 (0.135729405169), fragment_115.09425 (0.105536452337), fragment_110.09589 (0.105142970052), loss_59.01885 (0.0952947444763), loss_119.05807 (0.0622907577799),\n",
      "Topic 180: fragment_312.14144 (0.130515593143), fragment_171.11257 (0.130152263902), fragment_330.15717 (0.102801190446), fragment_111.04425 (0.062679593511),\n",
      "Topic 181: loss_172.07348 (0.098054772856), loss_131.05794 (0.0590471551633),\n",
      "Topic 182: fragment_107.04882 (0.364271467658), fragment_137.06566 (0.151482827527), loss_171.0528 (0.0522418538736),\n",
      "Topic 183: loss_59.0734 (0.37050868713), loss_105.07878 (0.257745938531), fragment_60.08102 (0.0889390565025),\n",
      "Topic 184: fragment_122.06028 (0.334715503719), fragment_367.12 (0.116973133504), fragment_140.07025 (0.0696313335935),\n",
      "Topic 185: fragment_175.12004 (0.17044447698), loss_130.06263 (0.106708624193), loss_119.09447 (0.106704407046),\n",
      "Topic 186: loss_63.03174 (0.72551958245), loss_75.03187 (0.06069438143), fragment_116.07015 (0.0601399257627),\n",
      "Topic 187: fragment_87.04383 (0.181372496895), fragment_160.07656 (0.14639030802), loss_73.05272 (0.0746453973328), fragment_132.0814 (0.0590797643553),\n",
      "Topic 188: fragment_116.07115 (0.687789355426), fragment_227.1037 (0.0933294222073),\n",
      "Topic 189: fragment_68.04979 (0.653707096263), fragment_142.05023 (0.104200874744), fragment_142.08716 (0.0577413510574),\n",
      "Topic 190: fragment_109.02883 (0.208941114923), fragment_240.13519 (0.136846340637), fragment_253.12898 (0.0692646357833),\n",
      "Topic 191: fragment_210.11133 (0.315126202852), loss_114.06836 (0.26208570366), loss_116.08392 (0.0569519408446),\n",
      "Topic 192: fragment_174.03912 (0.257690667039), loss_151.04855 (0.11357117407), fragment_142.08578 (0.0914747703838), loss_104.04723 (0.05214416377),\n",
      "Topic 193: fragment_81.04484 (0.328755390127), loss_46.0247 (0.215463508259), fragment_127.06992 (0.0950077406458), loss_45.99413 (0.0788322485973),\n",
      "Topic 194: fragment_196.09799 (0.179889868506), fragment_150.05571 (0.136645436111), fragment_178.08705 (0.0843841895994), loss_46.04151 (0.0707505246542), fragment_135.03122 (0.055842041501),\n",
      "Topic 195: loss_192.06359 (0.272333284173), loss_32.02595 (0.147061069204), fragment_289.1514 (0.106768566902), fragment_303.13504 (0.0847907723845),\n",
      "Topic 196: fragment_120.08041 (0.586137717182), fragment_103.05448 (0.123125832745), loss_45.99223 (0.0627801104422),\n",
      "Topic 197: loss_179.07906 (0.644506559848), fragment_146.07108 (0.051739298726), loss_108.04219 (0.051732049552),\n",
      "Topic 198: loss_120.04217 (0.489185372273), loss_103.06314 (0.0943883229669), fragment_230.08052 (0.0665026531879),\n",
      "Topic 199: fragment_73.0285 (0.468039480845), fragment_61.02855 (0.104411791187), loss_162.02823 (0.075497908951), fragment_91.03926 (0.0637703970652),\n",
      "Topic 200: fragment_209.08092 (0.195183675014), loss_89.01109 (0.124039984945), loss_56.02604 (0.11661019068), fragment_307.16486 (0.0547607922025),\n",
      "Topic 201: fragment_74.02354 (0.603038608285), loss_60.05733 (0.112359216111), fragment_75.02734 (0.0504970959021),\n",
      "Topic 202: fragment_84.0442 (0.950696586436),\n",
      "Topic 203: loss_46.0042 (0.594596526435), fragment_146.04575 (0.0769111038396), fragment_223.07457 (0.0521298858047),\n",
      "Topic 204: loss_116.0471 (0.463015314603), fragment_105.10996 (0.226572820257),\n",
      "Topic 205: loss_197.08992 (0.487528905475), fragment_75.04398 (0.0805904000643), fragment_123.06533 (0.0544636491124),\n",
      "Topic 206: fragment_153.10205 (0.279956610165),\n",
      "Topic 207: loss_18.00923 (0.265669898127), fragment_198.1138 (0.153834185269), fragment_216.07642 (0.0655221628194), fragment_216.12458 (0.0598556072493), fragment_113.03508 (0.051294582818),\n",
      "Topic 208: fragment_158.11804 (0.186755168972), fragment_216.08565 (0.148434291536), loss_149.0688 (0.125579875967),\n",
      "Topic 209: fragment_177.11282 (0.145386059997), fragment_111.04425 (0.128737547548), fragment_137.05991 (0.0736612793102), fragment_562.25366 (0.0636577467488), fragment_105.03343 (0.0603934230541), fragment_165.07803 (0.0570573834642),\n",
      "Topic 210: fragment_95.06072 (0.648959160941),\n",
      "Topic 211: fragment_189.09504 (0.12186332408), loss_130.07414 (0.121685126887), fragment_74.06031 (0.0641453873757), fragment_104.07013 (0.0592257209928), loss_76.06364 (0.0518411120767), fragment_168.11248 (0.0512326372184), fragment_227.14938 (0.0508294165066),\n",
      "Topic 212: fragment_98.98384 (0.81255523967),\n",
      "Topic 213: loss_135.05353 (0.177979001166), loss_89.99513 (0.0778086212853), fragment_197.12833 (0.074768819218), fragment_174.10217 (0.0745759059587),\n",
      "Topic 214: fragment_106.06504 (0.20018898828), fragment_180.06459 (0.152686780685), fragment_108.04424 (0.0769804941727), loss_114.04274 (0.0542647543345), fragment_192.06434 (0.0519142929133),\n",
      "Topic 215: loss_77.0475 (0.362823840367), fragment_159.0652 (0.185693312347), fragment_189.88155 (0.116054648168), fragment_119.07333 (0.0607107692506), loss_117.05465 (0.0590912301802),\n",
      "Topic 216: loss_175.04799 (0.28643276453), fragment_256.11674 (0.0831239825204), fragment_238.07137 (0.0631563857738), loss_18.00971 (0.062215486871),\n",
      "Topic 217: fragment_98.98471 (0.547326193739), loss_149.07571 (0.0780412247617),\n",
      "Topic 218: fragment_89.05945 (0.231062826135), fragment_207.11359 (0.0593105389665), loss_125.06865 (0.057785547846), fragment_181.08645 (0.0574573437817), loss_56.06191 (0.0570670739526),\n",
      "Topic 219: fragment_69.03367 (0.949045701884),\n",
      "Topic 220: fragment_166.0863 (0.349973638035), fragment_158.09176 (0.124790150513), loss_88.02704 (0.0671870589094), loss_45.02004 (0.0640697713859), fragment_192.06667 (0.0639935887182),\n",
      "Topic 221: loss_62.03666 (0.411720645047), fragment_143.0819 (0.157335784763),\n",
      "Topic 222: loss_18.0114 (0.274611528832), fragment_148.06039 (0.116443836741), fragment_281.13703 (0.087848798096), fragment_292.10196 (0.0877603846098), fragment_225.07569 (0.069902348542), fragment_383.07324 (0.0630786093078),\n",
      "Topic 223: fragment_164.10655 (0.135667236508), fragment_293.3515 (0.131874220963), fragment_242.10395 (0.0949529875234), fragment_216.0883 (0.0508145940902),\n",
      "Topic 224: loss_159.08942 (0.474551773484), fragment_229.15629 (0.215875577306), fragment_185.16423 (0.0642773583115),\n",
      "Topic 225: loss_184.09487 (0.147678543895), loss_59.04829 (0.128080627877), fragment_195.1119 (0.126803415659), fragment_254.15944 (0.114047030988), loss_156.10035 (0.0812630422599), loss_129.09027 (0.0546032421369), fragment_125.07097 (0.0516423189244),\n",
      "Topic 226: fragment_83.04906 (0.67096955211),\n",
      "Topic 227: fragment_134.06004 (0.21936316265), fragment_152.07027 (0.144926230018), fragment_151.08086 (0.102592451537), fragment_106.11161 (0.102561699368), fragment_151.06146 (0.0763110933847), fragment_273.10892 (0.0513013620355),\n",
      "Topic 228: fragment_127.05086 (0.219489022505), fragment_110.02365 (0.166409456634), loss_17.01518 (0.163237979137), fragment_82.02903 (0.0843514960294),\n",
      "Topic 229: fragment_213.12449 (0.250779045208), fragment_371.18332 (0.201830852554), fragment_135.09209 (0.125120136734),\n",
      "Topic 230: loss_18.01071 (0.630132571091), fragment_144.0806 (0.154306943792), fragment_527.15831 (0.0935114499436),\n",
      "Topic 231: loss_66.03185 (0.185466149716), fragment_115.08712 (0.0817449732097), fragment_276.12203 (0.0708488113165), loss_160.08465 (0.0625109591603), loss_132.08976 (0.0595295967507), fragment_231.14648 (0.050481617368),\n",
      "Topic 232: fragment_260.11151 (0.223229936503), fragment_160.08098 (0.0741481862564), loss_18.01172 (0.0691832804702), fragment_83.01234 (0.0689641719871), fragment_100.07535 (0.063128377764),\n",
      "Topic 233: loss_89.04757 (0.523011348837), fragment_168.06504 (0.134369852525), fragment_140.07025 (0.0837520957146),\n",
      "Topic 234: loss_88.01592 (0.589366557239), fragment_156.01203 (0.10050611457),\n",
      "Topic 235: fragment_190.10768 (0.168227384628), loss_35.0368 (0.111566123199), loss_95.05811 (0.107815146399), loss_83.05816 (0.0960475419843), loss_71.05755 (0.0595508477177),\n",
      "Topic 236: loss_157.03734 (0.411282157049), loss_111.03186 (0.27065630653),\n",
      "Topic 237: loss_148.07322 (0.235288088123), fragment_131.08504 (0.12258432957), fragment_314.18594 (0.0706369413296), loss_96.04228 (0.0502357510318),\n",
      "Topic 238: loss_84.02104 (0.123416898839), fragment_122.06028 (0.085409679535), fragment_309.08884 (0.0842279644592), fragment_112.03873 (0.0773677998677),\n",
      "Topic 239: loss_86.00017 (0.301727152609), loss_101.99538 (0.23874986436),\n",
      "Topic 240: fragment_145.04996 (0.661234643715), fragment_163.06064 (0.125518495467),\n",
      "Topic 241: loss_194.04254 (0.259584412111), loss_166.04765 (0.0593682315896), fragment_126.05566 (0.0581413576603), fragment_292.10596 (0.0578974053776),\n",
      "Topic 242: loss_183.0893 (0.26085236049), loss_58.04156 (0.176539224042), fragment_267.15897 (0.0589560745014),\n",
      "Topic 243: loss_43.98972 (0.262862490014), fragment_160.10738 (0.0927156210289), fragment_164.04266 (0.0810614049753), fragment_116.04752 (0.0766580527612), fragment_211.10854 (0.0655034663911),\n",
      "Topic 244: fragment_70.06514 (0.98369427174),\n",
      "Topic 245: loss_172.10993 (0.201506954701), fragment_74.05986 (0.131739986356), fragment_229.14301 (0.131427626431), fragment_241.15314 (0.131401294254),\n",
      "Topic 246: fragment_91.05413 (0.830798926595),\n",
      "Topic 247: loss_56.06278 (0.334382432105), fragment_225.07569 (0.0962112821103), fragment_195.06558 (0.0959443405939),\n",
      "Topic 248: loss_173.06836 (0.20935978825), fragment_197.12833 (0.118072748926), loss_187.08399 (0.0635401673294),\n",
      "Topic 249: fragment_149.02373 (0.373094880922), fragment_216.12458 (0.124356460341), loss_18.00883 (0.0904657157459),\n",
      "Topic 250: fragment_212.09533 (0.271467781362), fragment_319.16226 (0.0963870781992), fragment_204.09945 (0.0782720058281), fragment_319.1564 (0.0751939274405),\n",
      "Topic 251: fragment_146.05935 (0.174519584291), fragment_276.0913 (0.168820272561), loss_36.01969 (0.0565684273433),\n",
      "Topic 252: fragment_61.01085 (0.453838268014), fragment_191.08556 (0.0530225944948),\n",
      "Topic 253: fragment_278.05566 (0.107024322867), fragment_131.12919 (0.0912564263841), fragment_114.10277 (0.0751598249792), fragment_55.05466 (0.0690016050579), fragment_60.05587 (0.0508338853223),\n",
      "Topic 254: fragment_295.17128 (0.0886380471754), fragment_149.13256 (0.0809584926312), loss_36.01863 (0.0581753648105), fragment_195.13771 (0.0576233817569), fragment_134.07172 (0.0576001404745),\n",
      "Topic 255: fragment_73.08416 (0.263832583325), loss_42.02181 (0.23665351782), fragment_290.133 (0.0801193188025), fragment_60.05587 (0.0615424983053), fragment_115.09525 (0.0520903634392),\n",
      "Topic 256: fragment_114.09123 (0.456937254478), loss_101.04756 (0.0929777403648),\n",
      "Topic 257: fragment_65.03879 (0.40219716867), loss_46.00618 (0.220372119324), fragment_166.04998 (0.0639226896273), loss_119.02139 (0.0596642862141),\n",
      "Topic 258: fragment_206.1023 (0.276447312573), loss_18.01044 (0.0989121715489), fragment_188.0925 (0.0809783575446), fragment_74.06031 (0.0663161508054),\n",
      "Topic 259: fragment_302.12175 (0.175353224124), loss_103.02699 (0.130392917204), fragment_180.08075 (0.0618552669535), fragment_182.09618 (0.0585019845211), fragment_273.09604 (0.0584901186218), loss_76.01584 (0.0542425057022),\n",
      "Topic 260: fragment_147.0438 (0.530774793375), fragment_119.04873 (0.0996624215976), loss_153.04161 (0.0836735050778),\n",
      "Topic 261: fragment_166.12294 (0.180705270574), loss_125.06865 (0.0999068326465), fragment_222.08575 (0.080143950487), fragment_294.13965 (0.052737052231), fragment_260.13539 (0.0513042682749),\n",
      "Topic 262: fragment_114.06579 (0.430907383736), fragment_125.04692 (0.0787567027896), fragment_271.11212 (0.0659939159978),\n",
      "Topic 263: fragment_127.03869 (0.756529549546), fragment_103.03861 (0.120408532795),\n",
      "Topic 264: fragment_58.06553 (0.547907224685), loss_141.0786 (0.133324335198), fragment_211.14356 (0.100245252503),\n",
      "Topic 265: loss_44.02598 (0.23462777133), loss_72.02096 (0.193720117511), fragment_111.04425 (0.108122581464), fragment_187.05925 (0.101065336988),\n",
      "Topic 266: fragment_57.07008 (0.841139367115),\n",
      "Topic 267: fragment_97.02839 (0.933617305705),\n",
      "Topic 268: fragment_85.04763 (0.270137800048), loss_174.12507 (0.0802935187131), fragment_127.07545 (0.0702786526433),\n",
      "Topic 269: fragment_180.1012 (0.497182038149), fragment_347.0957 (0.0883948629312), fragment_162.09141 (0.0790065048502),\n",
      "Topic 270: fragment_365.10692 (0.356524232192), fragment_294.16232 (0.0671773448717), fragment_96.04414 (0.0648877942133),\n",
      "Topic 271: fragment_86.09652 (0.935764884572),\n",
      "Topic 272: loss_45.02131 (0.256754479549), loss_99.03165 (0.132802785363), fragment_194.08213 (0.12612177442), loss_71.03687 (0.0680596992712),\n",
      "Topic 273: loss_102.03162 (0.333958702918), fragment_162.11279 (0.142389468606), fragment_228.0318 (0.0626010619524), fragment_98.07102 (0.0587691016051),\n",
      "Topic 274: loss_87.03199 (0.344227770749), loss_17.02709 (0.338861725787),\n",
      "Topic 275:\n",
      "Topic 276: fragment_124.03979 (0.660378693196), loss_176.03168 (0.0522281477922),\n",
      "Topic 277: fragment_112.07587 (0.609668752198), fragment_140.07025 (0.0663175291996),\n",
      "Topic 278: fragment_109.10127 (0.0787187227094), fragment_119.08567 (0.0663054862491), fragment_93.06981 (0.0571486605427), fragment_283.16853 (0.0568334423624),\n",
      "Topic 279: loss_101.04756 (0.27448803516), fragment_186.07564 (0.157316073779), loss_149.03209 (0.0745762152881), fragment_116.03443 (0.0542623039191),\n",
      "Topic 280: fragment_76.03919 (0.179138282034), fragment_220.0809 (0.123753376804), loss_18.00852 (0.110649480833), fragment_238.12006 (0.0901005445139),\n",
      "Topic 281: loss_31.04189 (0.178828650191), fragment_109.05263 (0.163085851621), loss_77.02419 (0.0816282576568), fragment_168.06504 (0.079299516765), fragment_138.05453 (0.0605264330951), loss_48.02102 (0.054327566196),\n",
      "Topic 282: loss_45.96892 (0.210704070988), loss_47.03667 (0.122416520785), fragment_98.09684 (0.0582334519097),\n",
      "Topic 283: fragment_60.08102 (0.237142959546), fragment_309.08884 (0.141450523538), fragment_206.08549 (0.0804020130166), fragment_419.15237 (0.070523140077), fragment_263.08109 (0.0663087592012), fragment_160.13255 (0.0602039572544),\n",
      "Topic 284: loss_17.97373 (0.226959156835), fragment_219.1012 (0.110536652207), fragment_293.13741 (0.0630295205706), fragment_217.08646 (0.0589331458637), loss_198.06312 (0.0554841194603), fragment_177.01865 (0.0519815041111),\n",
      "Topic 285: loss_146.05765 (0.225528995872), loss_106.06276 (0.149210437989), loss_130.06263 (0.0909431012104), loss_132.07868 (0.0870020303514), loss_120.07867 (0.0723110351061), fragment_139.05841 (0.0509205465965),\n",
      "Topic 286: loss_74.03657 (0.634827662957), fragment_100.07535 (0.211705926684),\n",
      "Topic 287: fragment_86.06016 (0.91574324565),\n",
      "Topic 288: loss_74.00025 (0.807371080371),\n",
      "Topic 289: loss_18.01116 (0.384604206163), fragment_98.03153 (0.0985738909421), fragment_198.1138 (0.0706290185702),\n",
      "Topic 290: fragment_96.04414 (0.527548541544), fragment_138.05453 (0.14601378507), fragment_126.05436 (0.0631869184219), loss_66.03185 (0.0603174945731),\n",
      "Topic 291: loss_105.05372 (0.166500605972), fragment_160.13397 (0.139768368031), fragment_296.14969 (0.0649927582107), fragment_96.08074 (0.0575811973198),\n",
      "Topic 292: loss_117.07879 (0.406301450279), fragment_138.05453 (0.252689756651), fragment_348.09775 (0.0596634051429), loss_154.00263 (0.056366529447),\n",
      "Topic 293: fragment_93.05448 (0.210605064418), fragment_393.13831 (0.169945027743), loss_170.07967 (0.12097356163), fragment_75.04398 (0.0873343322335), loss_179.05799 (0.0537121633963), loss_136.0372 (0.0533733938809),\n",
      "Topic 294: fragment_203.05188 (0.478382010022), fragment_128.95328 (0.146633010289),\n",
      "Topic 295: fragment_133.10092 (0.127928829956), fragment_93.06981 (0.120479741827), loss_198.1256 (0.114041712339), fragment_95.0854 (0.0975174969573),\n",
      "Topic 296: fragment_156.06569 (0.279068330157),\n",
      "Topic 297: loss_27.99468 (0.572193679405), fragment_167.08947 (0.0590790034899), loss_58.00531 (0.0503622078825),\n",
      "Topic 298: loss_36.02015 (0.447253369296), fragment_194.08466 (0.110346098814), fragment_158.08222 (0.110320074208), fragment_194.118 (0.072181077284), loss_124.03666 (0.0608739765858),\n",
      "Topic 299: loss_60.03214 (0.535478256193), fragment_61.03976 (0.229658484126),\n"
     ]
    }
   ],
   "source": [
    "ms2lda.print_topic_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ms2lda.plot_lda_fragments(consistency=0.50, sort_by=\"h_index\", interactive=True)\n",
    "# ms2lda.plot_lda_fragments(consistency=0.50, sort_by=\"in_degree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
